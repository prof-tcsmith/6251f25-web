{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2C: Pandas Fundamentals - Data Manipulation and Analysis\n",
    "\n",
    "## Welcome to Pandas!\n",
    "\n",
    "Welcome to your journey into data manipulation with Pandas! Pandas is the cornerstone of data analysis in Python, providing powerful and flexible tools for working with structured data. Named after \"Panel Data\" from econometrics, Pandas has become the de facto standard for data manipulation in Python's data science ecosystem.\n",
    "\n",
    "### Why Pandas?\n",
    "\n",
    "Pandas bridges the gap between Python and production-ready data analysis. It provides:\n",
    "\n",
    "- **Intuitive Data Structures**: DataFrames and Series that make data manipulation natural\n",
    "- **Powerful Tools**: Built-in functions for filtering, grouping, and transforming data\n",
    "- **Integration**: Seamless work with NumPy, Matplotlib, and machine learning libraries\n",
    "- **Performance**: Optimized C implementations for speed\n",
    "- **Flexibility**: Handle missing data, time series, and various file formats\n",
    "\n",
    "### What We'll Cover\n",
    "\n",
    "In this comprehensive notebook, we'll explore:\n",
    "\n",
    "1. **DataFrames and Series** - The fundamental data structures\n",
    "2. **Data Selection and Indexing** - Accessing your data efficiently\n",
    "3. **Data Manipulation** - Transforming and modifying data\n",
    "4. **Aggregation and Statistics** - Summarizing and analyzing data\n",
    "5. **Handling Missing Data** - Dealing with real-world imperfect data\n",
    "6. **Merging and Joining** - Combining data from multiple sources\n",
    "\n",
    "Let's begin our exploration of Pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Pandas Data Structures\n",
    "\n",
    "### Series: The Building Block\n",
    "\n",
    "A Series is a one-dimensional labeled array capable of holding any data type. Think of it as a sophisticated version of a Python list or a single column in a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a Series from a list\n",
    "temperatures = pd.Series([72, 75, 69, 80, 83, 79, 77])\n",
    "print(\"Temperature Series:\")\n",
    "print(temperatures)\n",
    "print(f\"\\nType: {type(temperatures)}\")\n",
    "print(f\"Shape: {temperatures.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series with custom index\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "temp_series = pd.Series([72, 75, 69, 80, 83, 79, 77], index=days)\n",
    "\n",
    "print(\"Temperature by Day:\")\n",
    "print(temp_series)\n",
    "print(f\"\\nWednesday's temperature: {temp_series['Wednesday']}¬∞F\")\n",
    "print(f\"Weekend average: {temp_series[['Saturday', 'Sunday']].mean():.1f}¬∞F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series operations\n",
    "print(\"Temperature Statistics:\")\n",
    "print(f\"Mean: {temp_series.mean():.1f}¬∞F\")\n",
    "print(f\"Max: {temp_series.max()}¬∞F on {temp_series.idxmax()}\")\n",
    "print(f\"Min: {temp_series.min()}¬∞F on {temp_series.idxmin()}\")\n",
    "print(f\"\\nDays above 75¬∞F:\")\n",
    "print(temp_series[temp_series > 75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames: The Workhorse of Pandas\n",
    "\n",
    "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. Think of it as a spreadsheet or SQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from a dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Edward'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'Department': ['Sales', 'IT', 'HR', 'Sales', 'IT'],\n",
    "    'Salary': [50000, 75000, 60000, 55000, 80000],\n",
    "    'Years_Experience': [2, 5, 8, 3, 7]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Employee DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape} (rows, columns)\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame information and statistics\n",
    "print(\"DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nDataFrame Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame from various sources\n",
    "# From lists of lists\n",
    "data_list = [\n",
    "    ['Product A', 100, 25.50],\n",
    "    ['Product B', 150, 30.00],\n",
    "    ['Product C', 75, 22.75]\n",
    "]\n",
    "products_df = pd.DataFrame(data_list, columns=['Product', 'Quantity', 'Price'])\n",
    "print(\"Products DataFrame:\")\n",
    "print(products_df)\n",
    "\n",
    "# From NumPy array\n",
    "np_data = np.random.randn(5, 3)\n",
    "random_df = pd.DataFrame(np_data, columns=['A', 'B', 'C'])\n",
    "print(\"\\nRandom DataFrame:\")\n",
    "print(random_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Practice Exercise 1: Creating DataFrames\n",
    "\n",
    "Create a DataFrame for student grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create a student grades DataFrame\n",
    "# TODO: Create a DataFrame with the following columns:\n",
    "# - Student names: John, Sarah, Mike, Emily, David\n",
    "# - Math scores: 85, 92, 78, 95, 88\n",
    "# - Science scores: 90, 88, 85, 92, 79\n",
    "# - English scores: 78, 94, 82, 89, 91\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# Test your solution (uncomment when ready):\n",
    "# print(\"Student Grades:\")\n",
    "# print(students_df)\n",
    "# print(f\"\\nAverage Math Score: {students_df['Math'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Selection and Indexing\n",
    "\n",
    "### Selecting Columns\n",
    "\n",
    "There are multiple ways to select data from a DataFrame. Let's explore the most common methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single column selection (returns Series)\n",
    "print(\"Names (as Series):\")\n",
    "print(df['Name'])\n",
    "print(f\"Type: {type(df['Name'])}\")\n",
    "\n",
    "# Alternative syntax using dot notation (only for valid Python identifiers)\n",
    "print(\"\\nAges (using dot notation):\")\n",
    "print(df.Age)\n",
    "\n",
    "# Multiple columns (returns DataFrame)\n",
    "print(\"\\nName and Salary:\")\n",
    "print(df[['Name', 'Salary']])\n",
    "print(f\"Type: {type(df[['Name', 'Salary']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Selection with iloc and loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc - Integer location based indexing\n",
    "print(\"First row (iloc[0]):\")\n",
    "print(df.iloc[0])\n",
    "\n",
    "print(\"\\nFirst three rows (iloc[0:3]):\")\n",
    "print(df.iloc[0:3])\n",
    "\n",
    "print(\"\\nSpecific rows and columns (iloc[1:4, 1:3]):\")\n",
    "print(df.iloc[1:4, 1:3])  # Rows 1-3, columns 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting custom index for loc demonstration\n",
    "df_indexed = df.set_index('Name')\n",
    "print(\"DataFrame with Name as index:\")\n",
    "print(df_indexed)\n",
    "\n",
    "# loc - Label based indexing\n",
    "print(\"\\nAlice's data (loc['Alice']):\")\n",
    "print(df_indexed.loc['Alice'])\n",
    "\n",
    "print(\"\\nMultiple employees:\")\n",
    "print(df_indexed.loc[['Bob', 'Diana']])\n",
    "\n",
    "print(\"\\nSpecific cells:\")\n",
    "print(f\"Bob's Salary: ${df_indexed.loc['Bob', 'Salary']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing (Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple filtering\n",
    "print(\"Employees with salary > $60,000:\")\n",
    "high_earners = df[df['Salary'] > 60000]\n",
    "print(high_earners)\n",
    "\n",
    "# Multiple conditions\n",
    "print(\"\\nIT employees with 5+ years experience:\")\n",
    "experienced_it = df[(df['Department'] == 'IT') & (df['Years_Experience'] >= 5)]\n",
    "print(experienced_it)\n",
    "\n",
    "# Using isin() for multiple values\n",
    "print(\"\\nEmployees in Sales or HR:\")\n",
    "sales_hr = df[df['Department'].isin(['Sales', 'HR'])]\n",
    "print(sales_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query method - more readable for complex conditions\n",
    "print(\"Using query method:\")\n",
    "result = df.query('Salary > 60000 and Years_Experience < 8')\n",
    "print(result)\n",
    "\n",
    "# Query with variables\n",
    "min_salary = 55000\n",
    "result2 = df.query('Salary >= @min_salary')\n",
    "print(f\"\\nEmployees earning >= ${min_salary:,}:\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Practice Exercise 2: Data Selection\n",
    "\n",
    "Practice selecting and filtering data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Data Selection and Filtering\n",
    "# Given the employee DataFrame (df), find:\n",
    "# 1. All employees aged 30 or older\n",
    "# 2. Names and salaries of IT department employees\n",
    "# 3. The employee with the highest salary\n",
    "# 4. Average salary by department\n",
    "\n",
    "# Your code here:\n",
    "# TODO: Filter employees aged 30+\n",
    "# TODO: Select IT employees' names and salaries\n",
    "# TODO: Find highest paid employee\n",
    "# TODO: Calculate average salary by department\n",
    "\n",
    "\n",
    "# Test your solution (uncomment when ready):\n",
    "# print(\"Employees aged 30+:\")\n",
    "# print(older_employees)\n",
    "# print(\"\\nIT Department (Name & Salary):\")\n",
    "# print(it_salaries)\n",
    "# print(f\"\\nHighest paid: {highest_paid}\")\n",
    "# print(\"\\nAverage salary by department:\")\n",
    "# print(dept_avg_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Manipulation\n",
    "\n",
    "### Adding and Modifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to work with\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Adding a new column\n",
    "df_copy['Bonus'] = df_copy['Salary'] * 0.1\n",
    "print(\"DataFrame with Bonus column:\")\n",
    "print(df_copy)\n",
    "\n",
    "# Conditional column creation\n",
    "df_copy['Level'] = df_copy['Years_Experience'].apply(\n",
    "    lambda x: 'Senior' if x >= 5 else 'Junior'\n",
    ")\n",
    "print(\"\\nWith experience level:\")\n",
    "print(df_copy[['Name', 'Years_Experience', 'Level']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using np.where for conditional values\n",
    "df_copy['Performance'] = np.where(\n",
    "    df_copy['Salary'] > 65000, \n",
    "    'Excellent', \n",
    "    np.where(df_copy['Salary'] > 55000, 'Good', 'Average')\n",
    ")\n",
    "print(\"Performance ratings:\")\n",
    "print(df_copy[['Name', 'Salary', 'Performance']])\n",
    "\n",
    "# Using pd.cut for binning continuous values\n",
    "df_copy['Age_Group'] = pd.cut(\n",
    "    df_copy['Age'], \n",
    "    bins=[0, 30, 35, 100], \n",
    "    labels=['Young', 'Middle', 'Senior']\n",
    ")\n",
    "print(\"\\nAge groups:\")\n",
    "print(df_copy[['Name', 'Age', 'Age_Group']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by single column\n",
    "sorted_by_salary = df.sort_values('Salary', ascending=False)\n",
    "print(\"Sorted by Salary (highest first):\")\n",
    "print(sorted_by_salary)\n",
    "\n",
    "# Sort by multiple columns\n",
    "sorted_multi = df.sort_values(['Department', 'Salary'], ascending=[True, False])\n",
    "print(\"\\nSorted by Department (A-Z), then Salary (high to low):\")\n",
    "print(sorted_multi)\n",
    "\n",
    "# Sort by index\n",
    "df_copy.sort_index(inplace=True)\n",
    "print(\"\\nSorted by index:\")\n",
    "print(df_copy[['Name', 'Department']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping and Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "df_reduced = df_copy.drop(columns=['Bonus', 'Performance'])\n",
    "print(\"After dropping columns:\")\n",
    "print(df_reduced.columns.tolist())\n",
    "\n",
    "# Dropping rows\n",
    "df_filtered = df_copy.drop(index=[0, 2])  # Drop rows at index 0 and 2\n",
    "print(\"\\nAfter dropping rows 0 and 2:\")\n",
    "print(df_filtered[['Name', 'Department']])\n",
    "\n",
    "# Renaming columns\n",
    "df_renamed = df.rename(columns={\n",
    "    'Years_Experience': 'Experience',\n",
    "    'Department': 'Dept'\n",
    "})\n",
    "print(\"\\nRenamed columns:\")\n",
    "print(df_renamed.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String methods on Series\n",
    "df_str = df.copy()\n",
    "\n",
    "# Convert to uppercase\n",
    "df_str['Name_Upper'] = df_str['Name'].str.upper()\n",
    "print(\"Uppercase names:\")\n",
    "print(df_str[['Name', 'Name_Upper']])\n",
    "\n",
    "# Extract information\n",
    "df_str['Name_Length'] = df_str['Name'].str.len()\n",
    "df_str['First_Letter'] = df_str['Name'].str[0]\n",
    "print(\"\\nString analysis:\")\n",
    "print(df_str[['Name', 'Name_Length', 'First_Letter']])\n",
    "\n",
    "# Contains method\n",
    "print(\"\\nNames containing 'a' (case insensitive):\")\n",
    "contains_a = df_str[df_str['Name'].str.contains('a', case=False)]\n",
    "print(contains_a['Name'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Practice Exercise 3: Data Manipulation\n",
    "\n",
    "Transform and enhance the employee dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Data Manipulation\n",
    "# Using the employee DataFrame:\n",
    "# 1. Add a 'Total_Compensation' column (Salary + 15% bonus)\n",
    "# 2. Create a 'Seniority' column: 'Entry' (<3 years), 'Mid' (3-6), 'Senior' (>6)\n",
    "# 3. Add email addresses: first_name.lastname@company.com (lowercase)\n",
    "# 4. Sort by Total_Compensation (highest first)\n",
    "\n",
    "df_exercise = df.copy()\n",
    "\n",
    "# Your code here:\n",
    "# TODO: Add Total_Compensation\n",
    "# TODO: Create Seniority levels\n",
    "# TODO: Generate email addresses\n",
    "# TODO: Sort by compensation\n",
    "\n",
    "\n",
    "# Test your solution (uncomment when ready):\n",
    "# print(\"Enhanced Employee Data:\")\n",
    "# print(df_exercise[['Name', 'Email', 'Seniority', 'Total_Compensation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Aggregation and Statistics\n",
    "\n",
    "### GroupBy Operations\n",
    "\n",
    "GroupBy is one of the most powerful features in Pandas, implementing the split-apply-combine strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic groupby\n",
    "dept_groups = df.groupby('Department')\n",
    "\n",
    "# Aggregate with single function\n",
    "print(\"Average salary by department:\")\n",
    "print(dept_groups['Salary'].mean())\n",
    "\n",
    "# Multiple aggregations\n",
    "print(\"\\nDepartment statistics:\")\n",
    "dept_stats = dept_groups['Salary'].agg(['mean', 'min', 'max', 'count'])\n",
    "print(dept_stats)\n",
    "\n",
    "# Different aggregations for different columns\n",
    "print(\"\\nComprehensive department summary:\")\n",
    "summary = dept_groups.agg({\n",
    "    'Salary': ['mean', 'max'],\n",
    "    'Age': 'mean',\n",
    "    'Years_Experience': 'sum'\n",
    "}).round(2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation functions\n",
    "def salary_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "print(\"Salary range by department:\")\n",
    "print(dept_groups['Salary'].agg(salary_range))\n",
    "\n",
    "# Multiple groupby keys\n",
    "df_copy = df.copy()\n",
    "df_copy['Level'] = df_copy['Years_Experience'].apply(\n",
    "    lambda x: 'Senior' if x >= 5 else 'Junior'\n",
    ")\n",
    "\n",
    "multi_group = df_copy.groupby(['Department', 'Level'])['Salary'].mean()\n",
    "print(\"\\nAverage salary by department and level:\")\n",
    "print(multi_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables\n",
    "\n",
    "Pivot tables provide a flexible way to create spreadsheet-style summary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=20),\n",
    "    'Product': np.random.choice(['A', 'B', 'C'], 20),\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], 20),\n",
    "    'Sales': np.random.randint(100, 1000, 20),\n",
    "    'Units': np.random.randint(10, 50, 20)\n",
    "})\n",
    "\n",
    "print(\"Sales Data Sample:\")\n",
    "print(sales_data.head(10))\n",
    "\n",
    "# Create pivot table\n",
    "pivot = pd.pivot_table(\n",
    "    sales_data,\n",
    "    values='Sales',\n",
    "    index='Product',\n",
    "    columns='Region',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\nPivot Table - Sales by Product and Region:\")\n",
    "print(pivot)\n",
    "\n",
    "# Multiple aggregations in pivot table\n",
    "pivot_multi = pd.pivot_table(\n",
    "    sales_data,\n",
    "    values=['Sales', 'Units'],\n",
    "    index='Product',\n",
    "    aggfunc={'Sales': 'sum', 'Units': 'mean'}\n",
    ")\n",
    "\n",
    "print(\"\\nPivot with multiple metrics:\")\n",
    "print(pivot_multi.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"Employee DataFrame Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number])\n",
    "correlation = numeric_cols.corr()\n",
    "print(correlation.round(3))\n",
    "\n",
    "# Specific statistics\n",
    "print(\"\\nDetailed Statistics:\")\n",
    "print(f\"Salary variance: {df['Salary'].var():,.2f}\")\n",
    "print(f\"Salary std dev: {df['Salary'].std():,.2f}\")\n",
    "print(f\"Age median: {df['Age'].median()}\")\n",
    "print(f\"25th percentile salary: ${df['Salary'].quantile(0.25):,.2f}\")\n",
    "print(f\"75th percentile salary: ${df['Salary'].quantile(0.75):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Practice Exercise 4: Aggregation\n",
    "\n",
    "Perform complex aggregations on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Advanced Aggregation\n",
    "# Create a summary report that shows:\n",
    "# 1. Number of employees per department\n",
    "# 2. Average years of experience per department\n",
    "# 3. Salary statistics (min, max, mean, median) per department\n",
    "# 4. Which department has the highest average salary per year of experience\n",
    "\n",
    "# Your code here:\n",
    "# TODO: Count employees per department\n",
    "# TODO: Average experience per department\n",
    "# TODO: Salary statistics per department\n",
    "# TODO: Calculate salary per year of experience\n",
    "\n",
    "\n",
    "# Test your solution (uncomment when ready):\n",
    "# print(\"Department Summary Report:\")\n",
    "# print(dept_summary)\n",
    "# print(f\"\\nBest salary per experience: {best_dept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Handling Missing Data\n",
    "\n",
    "### Detecting Missing Data\n",
    "\n",
    "Real-world data often contains missing values. Pandas provides tools to handle them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, np.nan, 5],\n",
    "    'C': [1, 2, 3, 4, 5],\n",
    "    'D': [np.nan, np.nan, np.nan, 4, 5]\n",
    "})\n",
    "\n",
    "print(\"Data with missing values:\")\n",
    "print(missing_data)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_data.isnull().sum())\n",
    "\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((missing_data.isnull().sum() / len(missing_data) * 100).round(1))\n",
    "\n",
    "# Check which rows have any missing values\n",
    "print(\"\\nRows with any missing values:\")\n",
    "print(missing_data[missing_data.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "print(\"Drop rows with any NaN:\")\n",
    "print(missing_data.dropna())\n",
    "\n",
    "print(\"\\nDrop columns with any NaN:\")\n",
    "print(missing_data.dropna(axis=1))\n",
    "\n",
    "print(\"\\nDrop rows where all values are NaN:\")\n",
    "print(missing_data.dropna(how='all'))\n",
    "\n",
    "# Fill missing values\n",
    "print(\"\\nFill with constant value (0):\")\n",
    "print(missing_data.fillna(0))\n",
    "\n",
    "print(\"\\nFill with column mean:\")\n",
    "filled_mean = missing_data.fillna(missing_data.mean())\n",
    "print(filled_mean.round(2))\n",
    "\n",
    "print(\"\\nForward fill (use previous value):\")\n",
    "print(missing_data.fillna(method='ffill'))\n",
    "\n",
    "print(\"\\nBackward fill (use next value):\")\n",
    "print(missing_data.fillna(method='bfill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation for missing values\n",
    "time_series = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'value': [1, np.nan, np.nan, 4, 5, np.nan, 7, 8, np.nan, 10]\n",
    "})\n",
    "\n",
    "print(\"Time series with gaps:\")\n",
    "print(time_series)\n",
    "\n",
    "print(\"\\nLinear interpolation:\")\n",
    "time_series['interpolated'] = time_series['value'].interpolate(method='linear')\n",
    "print(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Merging and Joining DataFrames\n",
    "\n",
    "### Concatenation\n",
    "\n",
    "Combining DataFrames vertically or horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'A': [7, 8, 9],\n",
    "    'B': [10, 11, 12]\n",
    "})\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'C': [13, 14, 15],\n",
    "    'D': [16, 17, 18]\n",
    "})\n",
    "\n",
    "# Vertical concatenation (stack rows)\n",
    "vertical_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Vertical concatenation:\")\n",
    "print(vertical_concat)\n",
    "\n",
    "# Horizontal concatenation (add columns)\n",
    "horizontal_concat = pd.concat([df1, df3], axis=1)\n",
    "print(\"\\nHorizontal concatenation:\")\n",
    "print(horizontal_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging DataFrames\n",
    "\n",
    "Similar to SQL joins, merge combines DataFrames based on common columns or indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create related DataFrames\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Edward'],\n",
    "    'dept_id': [101, 102, 101, 103, 102]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'dept_id': [101, 102, 103, 104],\n",
    "    'dept_name': ['Sales', 'IT', 'HR', 'Finance'],\n",
    "    'location': ['NYC', 'SF', 'LA', 'Chicago']\n",
    "})\n",
    "\n",
    "salaries = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 5, 6],  # Note: emp_id 4 missing, 6 is extra\n",
    "    'salary': [50000, 75000, 60000, 80000, 45000]\n",
    "})\n",
    "\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nDepartments:\")\n",
    "print(departments)\n",
    "print(\"\\nSalaries:\")\n",
    "print(salaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (only matching records)\n",
    "inner_merge = pd.merge(employees, departments, on='dept_id')\n",
    "print(\"Inner join - Employees with Departments:\")\n",
    "print(inner_merge)\n",
    "\n",
    "# Left join (all from left, matching from right)\n",
    "left_merge = pd.merge(employees, salaries, on='emp_id', how='left')\n",
    "print(\"\\nLeft join - All employees with salaries (if available):\")\n",
    "print(left_merge)\n",
    "\n",
    "# Outer join (all from both)\n",
    "outer_merge = pd.merge(employees, salaries, on='emp_id', how='outer')\n",
    "print(\"\\nOuter join - All employees and all salaries:\")\n",
    "print(outer_merge)\n",
    "\n",
    "# Multiple joins\n",
    "full_data = pd.merge(employees, departments, on='dept_id')\n",
    "full_data = pd.merge(full_data, salaries, on='emp_id', how='left')\n",
    "print(\"\\nComplete employee information:\")\n",
    "print(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Practice Exercise 5: Merging Data\n",
    "\n",
    "Combine multiple datasets to create a comprehensive view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Data Integration\n",
    "# Given three DataFrames:\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3, 4, 5],\n",
    "    'customer_id': [101, 102, 101, 103, 102],\n",
    "    'product_id': [1, 2, 1, 3, 2],\n",
    "    'quantity': [2, 1, 3, 1, 2]\n",
    "})\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103],\n",
    "    'customer_name': ['John', 'Sarah', 'Mike'],\n",
    "    'city': ['NYC', 'LA', 'Chicago']\n",
    "})\n",
    "\n",
    "products = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard'],\n",
    "    'price': [1000, 50, 80]\n",
    "})\n",
    "\n",
    "# TODO: Merge all three DataFrames to create a complete order report\n",
    "# TODO: Calculate total price for each order (quantity * price)\n",
    "# TODO: Find total sales per customer\n",
    "# TODO: Find most popular product\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# Test your solution (uncomment when ready):\n",
    "# print(\"Complete Order Report:\")\n",
    "# print(complete_orders)\n",
    "# print(f\"\\nTotal sales per customer:\")\n",
    "# print(customer_sales)\n",
    "# print(f\"\\nMost popular product: {popular_product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Working with Dates and Times\n",
    "\n",
    "### DateTime Operations\n",
    "\n",
    "Pandas has extensive support for working with dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating date ranges\n",
    "dates = pd.date_range('2024-01-01', periods=10, freq='D')\n",
    "print(\"Daily dates:\")\n",
    "print(dates)\n",
    "\n",
    "# Different frequencies\n",
    "business_days = pd.date_range('2024-01-01', periods=10, freq='B')  # Business days\n",
    "print(\"\\nBusiness days only:\")\n",
    "print(business_days)\n",
    "\n",
    "# Create time series DataFrame\n",
    "ts_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'value': np.random.randn(10).cumsum() + 100\n",
    "})\n",
    "ts_df.set_index('date', inplace=True)\n",
    "print(\"\\nTime series DataFrame:\")\n",
    "print(ts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing dates from strings\n",
    "date_strings = ['2024-01-15', '2024-02-20', '2024-03-10']\n",
    "parsed_dates = pd.to_datetime(date_strings)\n",
    "print(\"Parsed dates:\")\n",
    "print(parsed_dates)\n",
    "\n",
    "# Extract date components\n",
    "date_df = pd.DataFrame({'date': parsed_dates})\n",
    "date_df['year'] = date_df['date'].dt.year\n",
    "date_df['month'] = date_df['date'].dt.month\n",
    "date_df['day'] = date_df['date'].dt.day\n",
    "date_df['weekday'] = date_df['date'].dt.day_name()\n",
    "print(\"\\nDate components:\")\n",
    "print(date_df)\n",
    "\n",
    "# Date arithmetic\n",
    "date_df['plus_30_days'] = date_df['date'] + pd.Timedelta(days=30)\n",
    "date_df['plus_1_month'] = date_df['date'] + pd.DateOffset(months=1)\n",
    "print(\"\\nDate arithmetic:\")\n",
    "print(date_df[['date', 'plus_30_days', 'plus_1_month']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Reading and Writing Data\n",
    "\n",
    "### File I/O Operations\n",
    "\n",
    "Pandas can read and write data in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data to save\n",
    "sample_data = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Score': [85.5, 92.3, 78.9]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "sample_data.to_csv('sample.csv', index=False)\n",
    "print(\"Data saved to CSV\")\n",
    "\n",
    "# Read from CSV\n",
    "read_data = pd.read_csv('sample.csv')\n",
    "print(\"\\nData read from CSV:\")\n",
    "print(read_data)\n",
    "\n",
    "# Save to Excel (requires openpyxl)\n",
    "try:\n",
    "    sample_data.to_excel('sample.xlsx', index=False, sheet_name='Data')\n",
    "    print(\"\\nData saved to Excel\")\n",
    "except ImportError:\n",
    "    print(\"\\nExcel writing requires 'openpyxl' package\")\n",
    "\n",
    "# Save to JSON\n",
    "sample_data.to_json('sample.json', orient='records', indent=2)\n",
    "print(\"\\nData saved to JSON\")\n",
    "\n",
    "# Clean up files\n",
    "import os\n",
    "for file in ['sample.csv', 'sample.xlsx', 'sample.json']:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "print(\"\\nTemporary files cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "Congratulations! You've mastered the fundamentals of Pandas:\n",
    "\n",
    "1. **Data Structures**: Working with Series and DataFrames\n",
    "2. **Selection and Indexing**: Accessing data using loc, iloc, and boolean indexing\n",
    "3. **Data Manipulation**: Adding columns, sorting, and transforming data\n",
    "4. **Aggregation**: GroupBy operations and pivot tables\n",
    "5. **Missing Data**: Detecting and handling NaN values\n",
    "6. **Merging**: Combining multiple DataFrames\n",
    "7. **DateTime**: Working with time series data\n",
    "8. **File I/O**: Reading and writing various file formats\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **DataFrames** are the primary data structure for tabular data\n",
    "- **Method chaining** allows for elegant data transformations\n",
    "- **GroupBy** operations enable powerful aggregations\n",
    "- **Merging** DataFrames is similar to SQL joins\n",
    "- **Missing data** handling is crucial for real-world datasets\n",
    "\n",
    "### Practice Projects\n",
    "\n",
    "To solidify your understanding, try these projects:\n",
    "\n",
    "1. **Sales Analysis**: Load sales data and create a dashboard with top products, regional performance, and trends\n",
    "2. **Student Performance**: Analyze student grades, identify patterns, and create summary reports\n",
    "3. **Stock Market Data**: Download stock prices, calculate returns, and perform technical analysis\n",
    "4. **Customer Segmentation**: Group customers based on purchasing behavior\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebook (Week 2D - Matplotlib), we'll explore:\n",
    "\n",
    "- Creating various plot types\n",
    "- Customizing visualizations\n",
    "- Statistical plots\n",
    "- Combining Pandas with Matplotlib for data visualization\n",
    "\n",
    "### Final Challenge\n",
    "\n",
    "Create a complete data analysis pipeline:\n",
    "1. Load a dataset (create your own or use sample data)\n",
    "2. Clean and prepare the data\n",
    "3. Perform exploratory data analysis\n",
    "4. Create summary statistics and visualizations\n",
    "5. Export results to multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Challenge: Complete Data Analysis Pipeline\n",
    "# Your implementation here:\n",
    "\n",
    "def analyze_dataset(data):\n",
    "    \"\"\"\n",
    "    Perform comprehensive analysis on a dataset.\n",
    "    \n",
    "    Steps:\n",
    "    1. Display basic information\n",
    "    2. Handle missing values\n",
    "    3. Generate summary statistics\n",
    "    4. Perform groupby analysis\n",
    "    5. Create derived features\n",
    "    6. Export results\n",
    "    \"\"\"\n",
    "    # TODO: Implement your analysis pipeline\n",
    "    pass\n",
    "\n",
    "# Create sample dataset and analyze\n",
    "# Uncomment to test:\n",
    "# sample_dataset = create_sample_data()\n",
    "# analyze_dataset(sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resources for Further Learning\n",
    "\n",
    "- **Official Pandas Documentation**: https://pandas.pydata.org/docs/\n",
    "- **Pandas User Guide**: https://pandas.pydata.org/docs/user_guide/index.html\n",
    "- **10 Minutes to Pandas**: https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "- **Pandas Cookbook**: https://pandas.pydata.org/docs/user_guide/cookbook.html\n",
    "- **Real Python Pandas Tutorials**: https://realpython.com/learning-paths/pandas-data-science/\n",
    "\n",
    "Remember: Pandas is best learned through practice. Work with real datasets, experiment with different methods, and don't be afraid to explore the documentation!\n",
    "\n",
    "Happy Data Wrangling! üêºüìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}