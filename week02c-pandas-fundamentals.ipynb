{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2c: Pandas Fundamentals\n",
    "## ISM 6251: Introduction to Machine Learning\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Create and manipulate DataFrames and Series\n",
    "2. Load and save data from various formats\n",
    "3. Select and filter data\n",
    "4. Handle missing data\n",
    "5. Perform basic data aggregation\n",
    "6. Merge and join DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Pandas\n",
    "\n",
    "Pandas is a powerful data manipulation library that provides:\n",
    "- DataFrame: 2D labeled data structure\n",
    "- Series: 1D labeled array\n",
    "- Tools for reading/writing data\n",
    "- Data alignment and missing data handling\n",
    "- Grouping and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Check Pandas version\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pandas Series\n",
    "\n",
    "A Series is a one-dimensional labeled array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series from a list\n",
    "s1 = pd.Series([1, 3, 5, 7, 9])\n",
    "print(\"Series from list:\")\n",
    "print(s1)\n",
    "\n",
    "# Series with custom index\n",
    "s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])\n",
    "print(\"\\nSeries with custom index:\")\n",
    "print(s2)\n",
    "\n",
    "# Series from dictionary\n",
    "s3 = pd.Series({'apple': 3, 'banana': 5, 'orange': 2})\n",
    "print(\"\\nSeries from dictionary:\")\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic operations\n",
    "s = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "print(f\"Original Series:\\n{s}\")\n",
    "print(f\"\\nAccess by index: s['c'] = {s['c']}\")\n",
    "print(f\"Access by position: s[2] = {s.iloc[2]}\")\n",
    "print(f\"\\nSlicing: s['b':'d']\\n{s['b':'d']}\")\n",
    "print(f\"\\nArithmetic: s * 2\\n{s * 2}\")\n",
    "print(f\"\\nBoolean indexing: s[s > 2]\\n{s[s > 2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pandas DataFrame\n",
    "\n",
    "A DataFrame is a 2D labeled data structure with columns of potentially different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame from dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Age': [25, 30, 35, 28],\n",
    "    'City': ['New York', 'Paris', 'London', 'Tokyo'],\n",
    "    'Salary': [70000, 80000, 75000, 90000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame from Various Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From NumPy array\n",
    "np_data = np.random.randn(4, 3)\n",
    "df_numpy = pd.DataFrame(np_data, columns=['A', 'B', 'C'])\n",
    "print(\"DataFrame from NumPy array:\")\n",
    "print(df_numpy)\n",
    "\n",
    "# From list of dictionaries\n",
    "list_data = [\n",
    "    {'Product': 'Laptop', 'Price': 1200, 'Quantity': 5},\n",
    "    {'Product': 'Mouse', 'Price': 25, 'Quantity': 50},\n",
    "    {'Product': 'Keyboard', 'Price': 75, 'Quantity': 20}\n",
    "]\n",
    "df_list = pd.DataFrame(list_data)\n",
    "print(\"\\nDataFrame from list of dictionaries:\")\n",
    "print(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DataFrame Attributes and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50],\n",
    "    'C': ['a', 'b', 'c', 'd', 'e'],\n",
    "    'D': [1.1, 2.2, 3.3, 4.4, 5.5]\n",
    "})\n",
    "\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Index: {df.index.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nInfo:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head and tail\n",
    "print(\"First 3 rows:\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\nLast 2 rows:\")\n",
    "print(df.tail(2))\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Include all columns in describe\n",
    "print(\"\\nSummary (all columns):\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Selection and Indexing\n",
    "\n",
    "### Column Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 33],\n",
    "    'Department': ['Sales', 'IT', 'HR', 'Sales', 'IT'],\n",
    "    'Salary': [50000, 70000, 60000, 55000, 75000]\n",
    "})\n",
    "\n",
    "# Single column (returns Series)\n",
    "print(\"Single column (Age):\")\n",
    "print(df['Age'])\n",
    "\n",
    "# Multiple columns (returns DataFrame)\n",
    "print(\"\\nMultiple columns:\")\n",
    "print(df[['Name', 'Salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using iloc (integer location)\n",
    "print(\"Row at index 2 (using iloc):\")\n",
    "print(df.iloc[2])\n",
    "\n",
    "print(\"\\nRows 1 to 3:\")\n",
    "print(df.iloc[1:4])\n",
    "\n",
    "# Using loc (label location)\n",
    "df_indexed = df.set_index('Name')\n",
    "print(\"\\nDataFrame with Name as index:\")\n",
    "print(df_indexed)\n",
    "\n",
    "print(\"\\nRow for 'Bob':\")\n",
    "print(df_indexed.loc['Bob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing\n",
    "print(\"Employees with Age > 30:\")\n",
    "print(df[df['Age'] > 30])\n",
    "\n",
    "print(\"\\nIT Department employees:\")\n",
    "print(df[df['Department'] == 'IT'])\n",
    "\n",
    "# Multiple conditions\n",
    "print(\"\\nSales employees with Salary > 50000:\")\n",
    "print(df[(df['Department'] == 'Sales') & (df['Salary'] > 50000)])\n",
    "\n",
    "# Using query method\n",
    "print(\"\\nUsing query method:\")\n",
    "print(df.query('Age > 30 and Salary < 70000'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adding and Modifying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to work with\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Add new column\n",
    "df_copy['Bonus'] = df_copy['Salary'] * 0.1\n",
    "print(\"After adding Bonus column:\")\n",
    "print(df_copy)\n",
    "\n",
    "# Modify existing column\n",
    "df_copy['Salary'] = df_copy['Salary'] * 1.05\n",
    "print(\"\\nAfter 5% salary increase:\")\n",
    "print(df_copy)\n",
    "\n",
    "# Add column with conditions\n",
    "df_copy['Level'] = pd.cut(df_copy['Age'], \n",
    "                          bins=[0, 30, 35, 100], \n",
    "                          labels=['Junior', 'Mid', 'Senior'])\n",
    "print(\"\\nWith Level column:\")\n",
    "print(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with missing values\n",
    "df_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [10, np.nan, 30, np.nan, 50],\n",
    "    'C': ['x', 'y', 'z', np.nan, 'w']\n",
    "})\n",
    "\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df_missing)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "# Drop rows with any missing values\n",
    "print(\"\\nDrop rows with any NaN:\")\n",
    "print(df_missing.dropna())\n",
    "\n",
    "# Fill missing values\n",
    "print(\"\\nFill NaN with 0:\")\n",
    "print(df_missing.fillna(0))\n",
    "\n",
    "# Forward fill\n",
    "print(\"\\nForward fill:\")\n",
    "print(df_missing.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sorting and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by values\n",
    "print(\"Sort by Age:\")\n",
    "print(df.sort_values('Age'))\n",
    "\n",
    "print(\"\\nSort by Salary (descending):\")\n",
    "print(df.sort_values('Salary', ascending=False))\n",
    "\n",
    "# Sort by multiple columns\n",
    "print(\"\\nSort by Department, then Salary:\")\n",
    "print(df.sort_values(['Department', 'Salary'], ascending=[True, False]))\n",
    "\n",
    "# Ranking\n",
    "df_rank = df.copy()\n",
    "df_rank['Salary_Rank'] = df_rank['Salary'].rank(ascending=False)\n",
    "print(\"\\nWith Salary Rank:\")\n",
    "print(df_rank[['Name', 'Salary', 'Salary_Rank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=12),\n",
    "    'Product': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'A', 'B', 'C', 'A', 'B'],\n",
    "    'Region': ['East', 'West', 'East', 'East', 'West', 'West', 'East', 'West', 'East', 'West', 'East', 'West'],\n",
    "    'Sales': [100, 150, 120, 90, 180, 110, 95, 130, 160, 100, 140, 170],\n",
    "    'Quantity': [10, 15, 12, 9, 18, 11, 10, 13, 16, 10, 14, 17]\n",
    "})\n",
    "\n",
    "print(\"Sales Data:\")\n",
    "print(sales_data)\n",
    "\n",
    "# Group by single column\n",
    "print(\"\\nGroup by Product:\")\n",
    "print(sales_data.groupby('Product')['Sales'].sum())\n",
    "\n",
    "# Group by multiple columns\n",
    "print(\"\\nGroup by Product and Region:\")\n",
    "print(sales_data.groupby(['Product', 'Region'])['Sales'].mean())\n",
    "\n",
    "# Multiple aggregations\n",
    "print(\"\\nMultiple aggregations:\")\n",
    "print(sales_data.groupby('Product').agg({\n",
    "    'Sales': ['sum', 'mean', 'max'],\n",
    "    'Quantity': ['sum', 'mean']\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Merging and Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "employees = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'DepartmentID': [10, 20, 10, 30]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'DepartmentID': [10, 20, 30],\n",
    "    'DepartmentName': ['Sales', 'IT', 'HR']\n",
    "})\n",
    "\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nDepartments:\")\n",
    "print(departments)\n",
    "\n",
    "# Inner join\n",
    "print(\"\\nInner Join:\")\n",
    "merged_inner = pd.merge(employees, departments, on='DepartmentID')\n",
    "print(merged_inner)\n",
    "\n",
    "# Left join\n",
    "employees_extra = employees.copy()\n",
    "employees_extra.loc[4] = [5, 'Eve', 40]\n",
    "print(\"\\nLeft Join (with employee in non-existent department):\")\n",
    "merged_left = pd.merge(employees_extra, departments, on='DepartmentID', how='left')\n",
    "print(merged_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "df3 = pd.DataFrame({'C': [9, 10], 'D': [11, 12]})\n",
    "\n",
    "# Vertical concatenation\n",
    "print(\"Vertical concatenation:\")\n",
    "concat_vertical = pd.concat([df1, df2], ignore_index=True)\n",
    "print(concat_vertical)\n",
    "\n",
    "# Horizontal concatenation\n",
    "print(\"\\nHorizontal concatenation:\")\n",
    "concat_horizontal = pd.concat([df1, df3], axis=1)\n",
    "print(concat_horizontal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with string data\n",
    "df_str = pd.DataFrame({\n",
    "    'Name': ['  Alice Smith  ', 'bob jones', 'CHARLIE BROWN', 'diana_ross'],\n",
    "    'Email': ['alice@email.com', 'BOB@GMAIL.COM', 'charlie@yahoo.com', 'diana@hotmail.com']\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df_str)\n",
    "\n",
    "# String methods\n",
    "df_str['Name_Clean'] = df_str['Name'].str.strip().str.title()\n",
    "df_str['Email_Lower'] = df_str['Email'].str.lower()\n",
    "df_str['Domain'] = df_str['Email'].str.split('@').str[1]\n",
    "\n",
    "print(\"\\nAfter string operations:\")\n",
    "print(df_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Date and Time Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with dates\n",
    "df_dates = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=10, freq='D'),\n",
    "    'Sales': np.random.randint(100, 500, 10)\n",
    "})\n",
    "\n",
    "print(\"DataFrame with dates:\")\n",
    "print(df_dates)\n",
    "\n",
    "# Extract date components\n",
    "df_dates['Year'] = df_dates['Date'].dt.year\n",
    "df_dates['Month'] = df_dates['Date'].dt.month\n",
    "df_dates['Day'] = df_dates['Date'].dt.day\n",
    "df_dates['DayOfWeek'] = df_dates['Date'].dt.day_name()\n",
    "\n",
    "print(\"\\nWith date components:\")\n",
    "print(df_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data\n",
    "sales = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=20),\n",
    "    'Product': np.random.choice(['A', 'B', 'C'], 20),\n",
    "    'Region': np.random.choice(['East', 'West'], 20),\n",
    "    'Sales': np.random.randint(50, 200, 20)\n",
    "})\n",
    "\n",
    "print(\"Sales data (first 10 rows):\")\n",
    "print(sales.head(10))\n",
    "\n",
    "# Create pivot table\n",
    "pivot = sales.pivot_table(\n",
    "    values='Sales',\n",
    "    index='Product',\n",
    "    columns='Region',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\nPivot table (Sum of Sales):\")\n",
    "print(pivot)\n",
    "\n",
    "# Multiple aggregations\n",
    "pivot_multi = sales.pivot_table(\n",
    "    values='Sales',\n",
    "    index='Product',\n",
    "    columns='Region',\n",
    "    aggfunc=['sum', 'mean', 'count'],\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\nPivot table with multiple aggregations:\")\n",
    "print(pivot_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Reading and Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "sample_df = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Score': [85, 92, 78, 95, 88]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "sample_df.to_csv('sample_data.csv', index=False)\n",
    "print(\"Data saved to CSV\")\n",
    "\n",
    "# Read from CSV\n",
    "df_from_csv = pd.read_csv('sample_data.csv')\n",
    "print(\"\\nData read from CSV:\")\n",
    "print(df_from_csv)\n",
    "\n",
    "# Save to Excel (requires openpyxl)\n",
    "try:\n",
    "    sample_df.to_excel('sample_data.xlsx', index=False, sheet_name='Scores')\n",
    "    print(\"\\nData saved to Excel\")\n",
    "except:\n",
    "    print(\"\\nExcel writing requires 'openpyxl' package\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "if os.path.exists('sample_data.csv'):\n",
    "    os.remove('sample_data.csv')\n",
    "if os.path.exists('sample_data.xlsx'):\n",
    "    os.remove('sample_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Practical Examples\n",
    "\n",
    "### Example 1: Student Grade Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create student grades data\n",
    "np.random.seed(42)\n",
    "students = ['Student_' + str(i) for i in range(1, 21)]\n",
    "grades = pd.DataFrame({\n",
    "    'Student': students,\n",
    "    'Math': np.random.randint(60, 100, 20),\n",
    "    'Science': np.random.randint(60, 100, 20),\n",
    "    'English': np.random.randint(60, 100, 20),\n",
    "    'History': np.random.randint(60, 100, 20)\n",
    "})\n",
    "\n",
    "print(\"Student Grades:\")\n",
    "print(grades.head(10))\n",
    "\n",
    "# Calculate average grade per student\n",
    "grades['Average'] = grades[['Math', 'Science', 'English', 'History']].mean(axis=1)\n",
    "\n",
    "# Assign letter grades\n",
    "def assign_letter_grade(score):\n",
    "    if score >= 90: return 'A'\n",
    "    elif score >= 80: return 'B'\n",
    "    elif score >= 70: return 'C'\n",
    "    elif score >= 60: return 'D'\n",
    "    else: return 'F'\n",
    "\n",
    "grades['Letter_Grade'] = grades['Average'].apply(assign_letter_grade)\n",
    "\n",
    "# Top 5 students\n",
    "print(\"\\nTop 5 Students:\")\n",
    "print(grades.nlargest(5, 'Average')[['Student', 'Average', 'Letter_Grade']])\n",
    "\n",
    "# Grade distribution\n",
    "print(\"\\nGrade Distribution:\")\n",
    "print(grades['Letter_Grade'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Sales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sales data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=100)\n",
    "products = ['Product_A', 'Product_B', 'Product_C']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': np.random.choice(dates, 200),\n",
    "    'Product': np.random.choice(products, 200),\n",
    "    'Region': np.random.choice(regions, 200),\n",
    "    'Units': np.random.randint(1, 50, 200),\n",
    "    'Price': np.random.uniform(10, 100, 200)\n",
    "})\n",
    "\n",
    "sales_data['Revenue'] = sales_data['Units'] * sales_data['Price']\n",
    "\n",
    "print(\"Sales Data Sample:\")\n",
    "print(sales_data.head(10))\n",
    "\n",
    "# Analysis by Product\n",
    "product_summary = sales_data.groupby('Product').agg({\n",
    "    'Units': 'sum',\n",
    "    'Revenue': ['sum', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nProduct Summary:\")\n",
    "print(product_summary)\n",
    "\n",
    "# Analysis by Region\n",
    "region_summary = sales_data.groupby('Region')['Revenue'].agg(['sum', 'mean', 'count']).round(2)\n",
    "print(\"\\nRegion Summary:\")\n",
    "print(region_summary)\n",
    "\n",
    "# Best performing product-region combination\n",
    "best_combo = sales_data.groupby(['Product', 'Region'])['Revenue'].sum().nlargest(5)\n",
    "print(\"\\nTop 5 Product-Region Combinations:\")\n",
    "print(best_combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Series and DataFrames**: Core Pandas data structures\n",
    "2. **Data Selection**: Using loc, iloc, and boolean indexing\n",
    "3. **Data Modification**: Adding columns and modifying values\n",
    "4. **Missing Data**: Detection and handling strategies\n",
    "5. **Sorting and Ranking**: Organizing data\n",
    "6. **Grouping and Aggregation**: Summarizing data by categories\n",
    "7. **Merging and Joining**: Combining multiple DataFrames\n",
    "8. **String Operations**: Text data manipulation\n",
    "9. **Date/Time Operations**: Working with temporal data\n",
    "10. **Pivot Tables**: Reshaping data for analysis\n",
    "11. **I/O Operations**: Reading and writing data files\n",
    "\n",
    "Pandas is essential for:\n",
    "- Data cleaning and preparation\n",
    "- Exploratory data analysis\n",
    "- Feature engineering for machine learning\n",
    "- Data transformation and aggregation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}