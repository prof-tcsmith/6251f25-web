{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISM6251 - Week 3 Assignment\n",
    "# Customer Data Pipeline: From Chaos to Clarity\n",
    "\n",
    "**Student Name:** [Enter your name here]\n",
    "\n",
    "**Student ID:** [Enter your student ID here]\n",
    "\n",
    "**Date:** [Enter submission date]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Context\n",
    "\n",
    "You are a Data Analyst at **CloudTech Solutions**, a B2B SaaS company. The customer database has quality issues from multiple data sources and manual processes. Your task is to:\n",
    "\n",
    "1. Clean and prepare customer data for churn analysis\n",
    "2. Create meaningful customer segments\n",
    "3. Build a reusable data preparation pipeline\n",
    "4. Document data quality issues and recommendations\n",
    "\n",
    "### Assignment Overview\n",
    "- **Part 1:** Data Quality Assessment (15 points)\n",
    "- **Part 2:** Missing Data Strategy (20 points)\n",
    "- **Part 3:** Data Filtering & Outlier Management (20 points)\n",
    "- **Part 4:** Grouping & Feature Engineering (20 points)\n",
    "- **Part 5:** Data Quality Report (15 points)\n",
    "- **Total: 90 points**\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Quality Assessment (15 points)\n",
    "\n",
    "### Task 1.1: Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TODO: IMPORTANT - Replace with your actual student ID\n",
    "STUDENT_ID = 'UXXX'  # <-- CHANGE THIS TO YOUR STUDENT ID\n",
    "\n",
    "# Create filenames for your data\n",
    "raw_data_filename = f\"{STUDENT_ID}-week03-raw.csv\"\n",
    "clean_data_filename = f\"{STUDENT_ID}-week03-clean.csv\"\n",
    "\n",
    "print(f\"Raw data will be saved as: {raw_data_filename}\")\n",
    "print(f\"Clean data will be saved as: {clean_data_filename}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Generate Messy Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic messy customer data (DO NOT MODIFY THIS CELL)\n",
    "def generate_messy_customer_data():\n",
    "    \"\"\"\n",
    "    Generate a realistic messy customer dataset with various data quality issues\n",
    "    \"\"\"\n",
    "    n_customers = 5000\n",
    "    \n",
    "    # Industries with typos and variations\n",
    "    industries = ['Technology', 'Finance', 'Healthcare', 'Retail', 'Manufacturing', \n",
    "                 'Education', 'Real Estate', 'Consulting']\n",
    "    industry_variations = {\n",
    "        'Technology': ['Technology', 'Tech', 'technology', 'IT', 'Information Technology'],\n",
    "        'Finance': ['Finance', 'Financial', 'Banking', 'finance', 'FinTech'],\n",
    "        'Healthcare': ['Healthcare', 'Health', 'Medical', 'healthcare', 'Pharma']\n",
    "    }\n",
    "    \n",
    "    # Contract types with inconsistencies\n",
    "    contract_types = ['Monthly', 'Annual', 'Enterprise']\n",
    "    contract_variations = {\n",
    "        'Monthly': ['Monthly', 'monthly', 'Month-to-Month', 'MTM', '1-month'],\n",
    "        'Annual': ['Annual', 'Yearly', 'annual', '12-month', '1-year'],\n",
    "        'Enterprise': ['Enterprise', 'enterprise', 'Ent', 'Custom', 'ENTERPRISE']\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    used_ids = set()\n",
    "    \n",
    "    for i in range(n_customers):\n",
    "        # Create customer ID with some duplicates\n",
    "        if i < 4950:\n",
    "            customer_id = f\"CUST{i+1:04d}\"\n",
    "        else:\n",
    "            # Create 50 duplicate IDs\n",
    "            customer_id = f\"CUST{np.random.randint(1, 4950):04d}\"\n",
    "        \n",
    "        # Company name (some missing)\n",
    "        if np.random.random() < 0.02:\n",
    "            company_name = np.nan\n",
    "        else:\n",
    "            company_name = f\"Company_{np.random.randint(1, 3000)}\"\n",
    "        \n",
    "        # Industry with variations and missing values\n",
    "        if np.random.random() < 0.08:\n",
    "            industry = np.nan\n",
    "        else:\n",
    "            base_industry = np.random.choice(industries)\n",
    "            if base_industry in industry_variations and np.random.random() < 0.3:\n",
    "                industry = np.random.choice(industry_variations[base_industry])\n",
    "            else:\n",
    "                industry = base_industry\n",
    "        \n",
    "        # Employee count with outliers and errors\n",
    "        if np.random.random() < 0.05:\n",
    "            employee_count = np.nan\n",
    "        elif np.random.random() < 0.02:\n",
    "            employee_count = np.random.choice([-10, -5, 0, 999999])  # Invalid values\n",
    "        elif np.random.random() < 0.05:\n",
    "            employee_count = np.random.randint(5000, 50000)  # Outliers\n",
    "        else:\n",
    "            employee_count = np.random.lognormal(4, 1.5)\n",
    "            employee_count = int(max(1, employee_count))\n",
    "        \n",
    "        # Annual revenue (correlated with employees, missing for some)\n",
    "        if np.random.random() < 0.175:  # 17.5% missing\n",
    "            annual_revenue = np.nan\n",
    "        elif employee_count and employee_count > 0:\n",
    "            base_revenue = employee_count * np.random.uniform(50000, 200000)\n",
    "            noise = np.random.normal(0, base_revenue * 0.2)\n",
    "            annual_revenue = max(0, base_revenue + noise)\n",
    "            if np.random.random() < 0.02:  # Some extreme outliers\n",
    "                annual_revenue *= np.random.choice([0.01, 100])\n",
    "        else:\n",
    "            annual_revenue = np.nan\n",
    "        \n",
    "        # Signup date\n",
    "        days_ago = np.random.randint(30, 1095)  # 1 month to 3 years\n",
    "        signup_date = datetime.now() - timedelta(days=days_ago)\n",
    "        \n",
    "        # Last login date (missing indicates inactive)\n",
    "        if np.random.random() < 0.125:  # 12.5% haven't logged in\n",
    "            last_login_date = np.nan\n",
    "        else:\n",
    "            # Active users login recently, inactive users have old logins\n",
    "            if np.random.random() < 0.7:  # 70% active\n",
    "                days_since_login = np.random.randint(0, 30)\n",
    "            else:  # 30% at risk\n",
    "                days_since_login = np.random.randint(31, 180)\n",
    "            last_login_date = datetime.now() - timedelta(days=days_since_login)\n",
    "        \n",
    "        # Monthly spend (with some negative values as errors)\n",
    "        if np.random.random() < 0.06:\n",
    "            monthly_spend = np.nan\n",
    "        elif np.random.random() < 0.01:\n",
    "            monthly_spend = np.random.uniform(-500, -10)  # Error: negative values\n",
    "        else:\n",
    "            if employee_count and employee_count > 0:\n",
    "                base_spend = employee_count * np.random.uniform(10, 50)\n",
    "                monthly_spend = max(0, base_spend + np.random.normal(0, base_spend * 0.3))\n",
    "            else:\n",
    "                monthly_spend = np.random.uniform(100, 5000)\n",
    "        \n",
    "        # Support tickets (missing for new or inactive customers)\n",
    "        if np.random.random() < 0.10:  # 10% missing\n",
    "            support_tickets = np.nan\n",
    "        elif pd.isna(last_login_date):  # No tickets if never logged in\n",
    "            support_tickets = 0\n",
    "        else:\n",
    "            # More tickets for problematic accounts\n",
    "            support_tickets = np.random.poisson(2) if np.random.random() < 0.8 else np.random.poisson(10)\n",
    "        \n",
    "        # Features used (out of 20 total features)\n",
    "        if np.random.random() < 0.04:\n",
    "            features_used = np.nan\n",
    "        elif np.random.random() < 0.01:\n",
    "            features_used = np.random.choice([-1, 25, 30])  # Invalid: out of range\n",
    "        else:\n",
    "            features_used = np.random.binomial(20, 0.6)\n",
    "        \n",
    "        # Satisfaction score (1-10, with invalid values)\n",
    "        if np.random.random() < 0.25:  # 25% missing\n",
    "            satisfaction_score = np.nan\n",
    "        elif np.random.random() < 0.01:\n",
    "            satisfaction_score = np.random.choice([0, 11, 15, -5])  # Invalid scores\n",
    "        else:\n",
    "            # Correlated with support tickets\n",
    "            if support_tickets and support_tickets > 5:\n",
    "                satisfaction_score = np.random.uniform(3, 7)\n",
    "            else:\n",
    "                satisfaction_score = np.random.uniform(6, 10)\n",
    "        \n",
    "        # Contract type with variations\n",
    "        if np.random.random() < 0.07:\n",
    "            contract_type = np.nan\n",
    "        else:\n",
    "            base_contract = np.random.choice(contract_types)\n",
    "            if base_contract in contract_variations and np.random.random() < 0.4:\n",
    "                contract_type = np.random.choice(contract_variations[base_contract])\n",
    "            else:\n",
    "                contract_type = base_contract\n",
    "        \n",
    "        # Payment method\n",
    "        if np.random.random() < 0.05:\n",
    "            payment_method = np.nan\n",
    "        else:\n",
    "            payment_method = np.random.choice(['Credit Card', 'ACH', 'Wire Transfer', 'Invoice'])\n",
    "        \n",
    "        data.append({\n",
    "            'customer_id': customer_id,\n",
    "            'company_name': company_name,\n",
    "            'industry': industry,\n",
    "            'employee_count': employee_count,\n",
    "            'annual_revenue': annual_revenue,\n",
    "            'signup_date': signup_date,\n",
    "            'last_login_date': last_login_date,\n",
    "            'monthly_spend': monthly_spend,\n",
    "            'support_tickets': support_tickets,\n",
    "            'features_used': features_used,\n",
    "            'satisfaction_score': satisfaction_score,\n",
    "            'contract_type': contract_type,\n",
    "            'payment_method': payment_method\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate and save the messy data\n",
    "df_raw = generate_messy_customer_data()\n",
    "df_raw.to_csv(raw_data_filename, index=False)\n",
    "print(f\"Messy customer data generated and saved to: {raw_data_filename}\")\n",
    "print(f\"Total records: {len(df_raw)}\")\n",
    "print(f\"Total columns: {len(df_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the raw data\n",
    "df = pd.read_csv(raw_data_filename, parse_dates=['signup_date', 'last_login_date'])\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Numeric Columns Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percent': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(missing_summary.to_string(index=False))\n",
    "print(f\"\\nColumns with missing values: {len(missing_summary)}/{len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for duplicate customer IDs\n",
    "duplicate_ids = df[df.duplicated(subset=['customer_id'], keep=False)]\n",
    "print(f\"Duplicate customer IDs found: {len(duplicate_ids)} records\")\n",
    "print(f\"Unique duplicate IDs: {duplicate_ids['customer_id'].nunique()}\")\n",
    "\n",
    "if len(duplicate_ids) > 0:\n",
    "    print(\"\\nSample of duplicate records:\")\n",
    "    print(duplicate_ids.head(10)[['customer_id', 'company_name', 'signup_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify data quality issues\n",
    "print(\"Data Quality Issues Identified:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for negative values where they shouldn't exist\n",
    "negative_employees = df[df['employee_count'] < 0]['employee_count'].count() if 'employee_count' in df.columns else 0\n",
    "negative_spend = df[df['monthly_spend'] < 0]['monthly_spend'].count() if 'monthly_spend' in df.columns else 0\n",
    "\n",
    "print(f\"1. Negative employee counts: {negative_employees}\")\n",
    "print(f\"2. Negative monthly spend: {negative_spend}\")\n",
    "\n",
    "# Check satisfaction score range\n",
    "invalid_satisfaction = df[(df['satisfaction_score'] < 1) | (df['satisfaction_score'] > 10)]['satisfaction_score'].count()\n",
    "print(f\"3. Invalid satisfaction scores (not 1-10): {invalid_satisfaction}\")\n",
    "\n",
    "# Check features used range\n",
    "invalid_features = df[(df['features_used'] < 0) | (df['features_used'] > 20)]['features_used'].count()\n",
    "print(f\"4. Invalid features used (not 0-20): {invalid_features}\")\n",
    "\n",
    "# Check for inconsistent categorical values\n",
    "print(f\"\\n5. Industry value variations: {df['industry'].nunique()} unique values\")\n",
    "print(f\"6. Contract type variations: {df['contract_type'].nunique()} unique values\")\n",
    "\n",
    "# TODO: Add your own quality checks here\n",
    "# For example: Check for outliers, check date consistency, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Quality Observations:** \n",
    "\n",
    "[TODO: Write 2-3 sentences about the main data quality issues you've identified]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Missing Data Strategy (20 points)\n",
    "\n",
    "### Task 2.1: Analyze Missing Data Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize missing data patterns\n",
    "# Create a heatmap showing missing values\n",
    "plt.figure(figsize=(12, 8))\n",
    "missing_matrix = df.isnull().astype(int)\n",
    "sns.heatmap(missing_matrix, cmap='RdYlBu', cbar_kws={'label': 'Missing (1) vs Present (0)'}, yticklabels=False)\n",
    "plt.title('Missing Data Pattern Visualization')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Customers')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze correlations between missing values\n",
    "missing_corr = missing_matrix.corr()\n",
    "print(\"\\nCorrelation between missing values:\")\n",
    "print(\"(High correlation suggests data is not missing completely at random)\")\n",
    "print(missing_corr.loc[missing_corr.abs().gt(0.3) & (missing_corr != 1)].dropna(how='all', axis=0).dropna(how='all', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Decide Drop vs Impute Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make decisions based on missing percentage and business importance\n",
    "def classify_missing_strategy(column, missing_pct, df):\n",
    "    \"\"\"\n",
    "    Classify the strategy for handling missing data based on the guidelines from lecture\n",
    "    \"\"\"\n",
    "    if missing_pct > 50:\n",
    "        return \"DROP_COLUMN\"\n",
    "    elif missing_pct < 5:\n",
    "        return \"DROP_ROWS\"\n",
    "    elif 10 <= missing_pct <= 50:\n",
    "        # Important columns to impute\n",
    "        important_cols = ['annual_revenue', 'monthly_spend', 'satisfaction_score', 'support_tickets']\n",
    "        if column in important_cols:\n",
    "            return \"IMPUTE\"\n",
    "        else:\n",
    "            return \"IMPUTE\"  # Default to impute for moderate missing\n",
    "    else:  # 5-10% gray area\n",
    "        return \"IMPUTE\"  # Conservative approach\n",
    "\n",
    "# Create strategy table\n",
    "strategy_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Percent': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "\n",
    "strategy_df['Strategy'] = strategy_df.apply(\n",
    "    lambda row: classify_missing_strategy(row['Column'], row['Missing_Percent'], df), axis=1\n",
    ")\n",
    "\n",
    "print(\"Missing Data Strategy:\")\n",
    "print(\"=\"*60)\n",
    "for strategy in ['DROP_COLUMN', 'DROP_ROWS', 'IMPUTE']:\n",
    "    cols = strategy_df[strategy_df['Strategy'] == strategy]['Column'].tolist()\n",
    "    if cols:\n",
    "        print(f\"\\n{strategy}:\")\n",
    "        for col in cols:\n",
    "            pct = strategy_df[strategy_df['Column'] == col]['Missing_Percent'].values[0]\n",
    "            print(f\"  - {col}: {pct:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Implement Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your imputation strategy\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Step 1: Handle duplicates first\n",
    "print(\"Handling duplicate customer IDs...\")\n",
    "# Keep the most recent record for each customer_id\n",
    "df_clean = df_clean.sort_values('signup_date').drop_duplicates(subset=['customer_id'], keep='last')\n",
    "print(f\"Removed {len(df) - len(df_clean)} duplicate records\")\n",
    "\n",
    "# Step 2: Drop columns with >50% missing (if any)\n",
    "cols_to_drop = strategy_df[strategy_df['Strategy'] == 'DROP_COLUMN']['Column'].tolist()\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nDropping columns with >50% missing: {cols_to_drop}\")\n",
    "    df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "\n",
    "# Step 3: Impute numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"\\nImputing numeric columns...\")\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        # Check skewness to decide between mean and median\n",
    "        if df_clean[col].skew() > 1:\n",
    "            # Use median for skewed data\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            df_clean[col] = imputer.fit_transform(df_clean[[col]]).ravel()\n",
    "            print(f\"  - {col}: Imputed with median (skewed distribution)\")\n",
    "        else:\n",
    "            # Use mean for normal-ish data\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            df_clean[col] = imputer.fit_transform(df_clean[[col]]).ravel()\n",
    "            print(f\"  - {col}: Imputed with mean\")\n",
    "\n",
    "# Step 4: Impute categorical columns\n",
    "print(\"\\nImputing categorical columns...\")\n",
    "for col in categorical_cols:\n",
    "    if col != 'customer_id' and df_clean[col].isnull().sum() > 0:\n",
    "        # Use mode for categorical\n",
    "        imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df_clean[col] = imputer.fit_transform(df_clean[[col]]).ravel()\n",
    "        print(f\"  - {col}: Imputed with mode (most frequent value)\")\n",
    "\n",
    "# Verify no missing values remain in critical columns\n",
    "remaining_missing = df_clean.isnull().sum()\n",
    "if remaining_missing.sum() > 0:\n",
    "    print(\"\\nRemaining missing values:\")\n",
    "    print(remaining_missing[remaining_missing > 0])\n",
    "else:\n",
    "    print(\"\\nAll missing values handled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Data Filtering & Outlier Management (20 points)\n",
    "\n",
    "### Task 3.1: Fix Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix invalid values based on business rules\n",
    "print(\"Fixing invalid values...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fix negative employee counts\n",
    "invalid_employees = df_clean['employee_count'] <= 0\n",
    "if invalid_employees.sum() > 0:\n",
    "    df_clean.loc[invalid_employees, 'employee_count'] = df_clean['employee_count'].median()\n",
    "    print(f\"Fixed {invalid_employees.sum()} invalid employee counts (replaced with median)\")\n",
    "\n",
    "# Fix negative monthly spend\n",
    "invalid_spend = df_clean['monthly_spend'] < 0\n",
    "if invalid_spend.sum() > 0:\n",
    "    df_clean.loc[invalid_spend, 'monthly_spend'] = df_clean['monthly_spend'].median()\n",
    "    print(f\"Fixed {invalid_spend.sum()} negative monthly spend values\")\n",
    "\n",
    "# Fix satisfaction scores outside 1-10 range\n",
    "invalid_satisfaction = (df_clean['satisfaction_score'] < 1) | (df_clean['satisfaction_score'] > 10)\n",
    "if invalid_satisfaction.sum() > 0:\n",
    "    # Clip to valid range\n",
    "    df_clean['satisfaction_score'] = df_clean['satisfaction_score'].clip(1, 10)\n",
    "    print(f\"Fixed {invalid_satisfaction.sum()} invalid satisfaction scores (clipped to 1-10)\")\n",
    "\n",
    "# Fix features_used outside 0-20 range\n",
    "invalid_features = (df_clean['features_used'] < 0) | (df_clean['features_used'] > 20)\n",
    "if invalid_features.sum() > 0:\n",
    "    df_clean['features_used'] = df_clean['features_used'].clip(0, 20)\n",
    "    print(f\"Fixed {invalid_features.sum()} invalid features_used values (clipped to 0-20)\")\n",
    "\n",
    "# Standardize categorical values\n",
    "print(\"\\nStandardizing categorical values...\")\n",
    "\n",
    "# Standardize industry names\n",
    "industry_mapping = {\n",
    "    'Tech': 'Technology', 'technology': 'Technology', 'IT': 'Technology', \n",
    "    'Information Technology': 'Technology',\n",
    "    'Financial': 'Finance', 'Banking': 'Finance', 'finance': 'Finance', 'FinTech': 'Finance',\n",
    "    'Health': 'Healthcare', 'Medical': 'Healthcare', 'healthcare': 'Healthcare', 'Pharma': 'Healthcare'\n",
    "}\n",
    "df_clean['industry'] = df_clean['industry'].replace(industry_mapping)\n",
    "print(f\"  - Industry: Reduced from {df['industry'].nunique()} to {df_clean['industry'].nunique()} categories\")\n",
    "\n",
    "# Standardize contract types\n",
    "contract_mapping = {\n",
    "    'monthly': 'Monthly', 'Month-to-Month': 'Monthly', 'MTM': 'Monthly', '1-month': 'Monthly',\n",
    "    'Yearly': 'Annual', 'annual': 'Annual', '12-month': 'Annual', '1-year': 'Annual',\n",
    "    'enterprise': 'Enterprise', 'Ent': 'Enterprise', 'Custom': 'Enterprise', 'ENTERPRISE': 'Enterprise'\n",
    "}\n",
    "df_clean['contract_type'] = df_clean['contract_type'].replace(contract_mapping)\n",
    "print(f\"  - Contract Type: Reduced from {df['contract_type'].nunique()} to {df_clean['contract_type'].nunique()} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Detect and Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Detect outliers using IQR method\n",
    "def detect_outliers_iqr(df, column, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check for outliers in key numeric columns\n",
    "outlier_columns = ['employee_count', 'annual_revenue', 'monthly_spend', 'support_tickets']\n",
    "\n",
    "print(\"Outlier Detection Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "outlier_summary = {}\n",
    "for col in outlier_columns:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_clean, col, multiplier=3)  # Using 3*IQR for conservative approach\n",
    "    outlier_summary[col] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': (len(outliers) / len(df_clean)) * 100,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  - Outliers: {len(outliers)} ({(len(outliers)/len(df_clean))*100:.1f}%)\")\n",
    "    print(f\"  - Bounds: [{lower:.0f}, {upper:.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(outlier_columns):\n",
    "    axes[idx].boxplot(df_clean[col].dropna())\n",
    "    axes[idx].set_title(f'{col}\\n({outlier_summary[col][\"count\"]} outliers)')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Outlier Detection - Box Plots', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Handle outliers based on business logic\n",
    "print(\"Handling outliers based on business logic...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# For this assignment, we'll cap extreme outliers rather than remove them\n",
    "# This preserves the data while reducing their impact\n",
    "\n",
    "for col in outlier_columns:\n",
    "    # Use 99th percentile as cap for upper outliers\n",
    "    upper_cap = df_clean[col].quantile(0.99)\n",
    "    lower_cap = df_clean[col].quantile(0.01)\n",
    "    \n",
    "    original_outliers = ((df_clean[col] > upper_cap) | (df_clean[col] < lower_cap)).sum()\n",
    "    \n",
    "    # Cap the values\n",
    "    df_clean[col] = df_clean[col].clip(lower_cap, upper_cap)\n",
    "    \n",
    "    print(f\"{col}: Capped {original_outliers} extreme values to 1st-99th percentile range\")\n",
    "\n",
    "print(\"\\nOutlier handling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Grouping & Feature Engineering (20 points)\n",
    "\n",
    "### Task 4.1: Create Customer Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create meaningful customer segments based on multiple criteria\n",
    "\n",
    "# First, create some useful features\n",
    "df_clean['days_since_login'] = (pd.Timestamp.now() - df_clean['last_login_date']).dt.days\n",
    "df_clean['account_age_days'] = (pd.Timestamp.now() - df_clean['signup_date']).dt.days\n",
    "df_clean['revenue_per_employee'] = df_clean['annual_revenue'] / df_clean['employee_count']\n",
    "df_clean['avg_ticket_per_month'] = df_clean['support_tickets'] / (df_clean['account_age_days'] / 30)\n",
    "\n",
    "# Define customer segments based on value and engagement\n",
    "def segment_customer(row):\n",
    "    \"\"\"Segment customers based on value and engagement\"\"\"\n",
    "    \n",
    "    # High value threshold (top 30%)\n",
    "    high_value_threshold = df_clean['monthly_spend'].quantile(0.70)\n",
    "    \n",
    "    # Engagement based on login recency\n",
    "    if pd.isna(row['days_since_login']) or row['days_since_login'] > 90:\n",
    "        engagement = 'Inactive'\n",
    "    elif row['days_since_login'] <= 30:\n",
    "        engagement = 'Active'\n",
    "    else:\n",
    "        engagement = 'At-Risk'\n",
    "    \n",
    "    # Value based on monthly spend\n",
    "    if row['monthly_spend'] >= high_value_threshold:\n",
    "        value = 'High-Value'\n",
    "    elif row['monthly_spend'] >= df_clean['monthly_spend'].quantile(0.30):\n",
    "        value = 'Standard'\n",
    "    else:\n",
    "        value = 'Low-Value'\n",
    "    \n",
    "    # Combine for final segment\n",
    "    if value == 'High-Value' and engagement == 'Active':\n",
    "        return 'Champions'\n",
    "    elif value == 'High-Value' and engagement == 'At-Risk':\n",
    "        return 'At-Risk High-Value'\n",
    "    elif value == 'High-Value' and engagement == 'Inactive':\n",
    "        return 'Lost High-Value'\n",
    "    elif value == 'Standard' and engagement == 'Active':\n",
    "        return 'Loyal Customers'\n",
    "    elif value == 'Standard' and engagement == 'At-Risk':\n",
    "        return 'Need Attention'\n",
    "    elif value == 'Low-Value' and engagement == 'Active':\n",
    "        return 'Promising'\n",
    "    else:\n",
    "        return 'Low Priority'\n",
    "\n",
    "# Apply segmentation\n",
    "df_clean['customer_segment'] = df_clean.apply(segment_customer, axis=1)\n",
    "\n",
    "# Display segment distribution\n",
    "segment_distribution = df_clean['customer_segment'].value_counts()\n",
    "print(\"Customer Segment Distribution:\")\n",
    "print(\"=\"*50)\n",
    "for segment, count in segment_distribution.items():\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"{segment}: {count} customers ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Analyze Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create segment profiles using groupby operations\n",
    "segment_profiles = df_clean.groupby('customer_segment').agg({\n",
    "    'customer_id': 'count',\n",
    "    'monthly_spend': 'mean',\n",
    "    'annual_revenue': 'mean',\n",
    "    'employee_count': 'mean',\n",
    "    'satisfaction_score': 'mean',\n",
    "    'support_tickets': 'mean',\n",
    "    'features_used': 'mean',\n",
    "    'days_since_login': 'mean',\n",
    "    'account_age_days': 'mean'\n",
    "}).round(1)\n",
    "\n",
    "segment_profiles.rename(columns={'customer_id': 'count'}, inplace=True)\n",
    "\n",
    "print(\"Segment Profiles:\")\n",
    "print(\"=\"*80)\n",
    "print(segment_profiles.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Answer key business questions using groupby\n",
    "print(\"Key Business Insights:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Question 1: Which industry has the highest average monthly spend?\n",
    "industry_spend = df_clean.groupby('industry')['monthly_spend'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "print(\"\\n1. Top Industries by Average Monthly Spend:\")\n",
    "print(industry_spend.head())\n",
    "\n",
    "# Question 2: What's the relationship between contract type and satisfaction?\n",
    "contract_satisfaction = df_clean.groupby('contract_type').agg({\n",
    "    'satisfaction_score': 'mean',\n",
    "    'customer_id': 'count',\n",
    "    'monthly_spend': 'mean'\n",
    "}).round(2)\n",
    "print(\"\\n2. Contract Type Analysis:\")\n",
    "print(contract_satisfaction)\n",
    "\n",
    "# Question 3: Which payment method is most popular among high-value customers?\n",
    "high_value_customers = df_clean[df_clean['customer_segment'].str.contains('High-Value')]\n",
    "payment_preference = high_value_customers['payment_method'].value_counts()\n",
    "print(\"\\n3. Payment Method Preference (High-Value Customers):\")\n",
    "print(payment_preference)\n",
    "\n",
    "# Question 4: Feature usage by segment\n",
    "feature_usage = df_clean.groupby('customer_segment')['features_used'].mean().sort_values(ascending=False)\n",
    "print(\"\\n4. Average Features Used by Segment:\")\n",
    "print(feature_usage.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.3: Create Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Engineer additional features that might be useful for churn prediction\n",
    "\n",
    "# Engagement score (0-100)\n",
    "df_clean['engagement_score'] = (\n",
    "    (df_clean['features_used'] / 20) * 40 +  # 40 points for feature usage\n",
    "    ((30 - df_clean['days_since_login'].clip(0, 30)) / 30) * 30 +  # 30 points for recency\n",
    "    (df_clean['satisfaction_score'] / 10) * 30  # 30 points for satisfaction\n",
    ")\n",
    "\n",
    "# Support intensity (tickets per month)\n",
    "df_clean['support_intensity'] = df_clean['support_tickets'] / (df_clean['account_age_days'] / 30)\n",
    "\n",
    "# Value tier\n",
    "df_clean['value_tier'] = pd.qcut(df_clean['monthly_spend'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Churn risk flag (simplified)\n",
    "df_clean['churn_risk'] = (\n",
    "    (df_clean['days_since_login'] > 60) | \n",
    "    (df_clean['satisfaction_score'] < 5) |\n",
    "    (df_clean['support_intensity'] > 3)\n",
    ").astype(int)\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. engagement_score: Combined metric of user engagement (0-100)\")\n",
    "print(\"2. support_intensity: Support tickets per month\")\n",
    "print(\"3. value_tier: Customer value category (Low/Medium/High)\")\n",
    "print(\"4. churn_risk: Binary flag for potential churn risk\")\n",
    "\n",
    "print(\"\\nChurn Risk Summary:\")\n",
    "churn_summary = df_clean['churn_risk'].value_counts()\n",
    "print(f\"At Risk: {churn_summary.get(1, 0)} customers ({churn_summary.get(1, 0)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Stable: {churn_summary.get(0, 0)} customers ({churn_summary.get(0, 0)/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Data Quality Report (15 points)\n",
    "\n",
    "### Task 5.1: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comprehensive data quality report\n",
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY REPORT - CloudTech Solutions Customer Data\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Report Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Analyst: {STUDENT_ID}\")\n",
    "print()\n",
    "\n",
    "print(\"1. DATA OVERVIEW\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Original Records: {len(df)}\")\n",
    "print(f\"Clean Records: {len(df_clean)}\")\n",
    "print(f\"Records Removed: {len(df) - len(df_clean)} ({(len(df) - len(df_clean))/len(df)*100:.1f}%)\")\n",
    "print(f\"Features: {len(df_clean.columns)} (added {len(df_clean.columns) - len(df.columns)} engineered features)\")\n",
    "print()\n",
    "\n",
    "print(\"2. DATA QUALITY ISSUES ADDRESSED\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"✓ Duplicate customer IDs: {len(df[df.duplicated(subset=['customer_id'], keep=False)])} → 0\")\n",
    "print(f\"✓ Missing values: {df.isnull().sum().sum()} → {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"✓ Invalid employee counts: {(df['employee_count'] <= 0).sum()} → 0\")\n",
    "print(f\"✓ Invalid satisfaction scores: {((df['satisfaction_score'] < 1) | (df['satisfaction_score'] > 10)).sum()} → 0\")\n",
    "print(f\"✓ Industry variations: {df['industry'].nunique()} → {df_clean['industry'].nunique()}\")\n",
    "print(f\"✓ Contract type variations: {df['contract_type'].nunique()} → {df_clean['contract_type'].nunique()}\")\n",
    "print()\n",
    "\n",
    "print(\"3. IMPUTATION SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "imputed_cols = ['annual_revenue', 'satisfaction_score', 'support_tickets', 'monthly_spend']\n",
    "for col in imputed_cols:\n",
    "    original_missing = df[col].isnull().sum()\n",
    "    if original_missing > 0:\n",
    "        print(f\"{col}: {original_missing} values imputed ({original_missing/len(df)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"4. CUSTOMER SEGMENTATION\")\n",
    "print(\"-\" * 40)\n",
    "for segment in df_clean['customer_segment'].value_counts().head(5).index:\n",
    "    count = (df_clean['customer_segment'] == segment).sum()\n",
    "    avg_spend = df_clean[df_clean['customer_segment'] == segment]['monthly_spend'].mean()\n",
    "    print(f\"{segment}: {count} customers (${avg_spend:,.0f} avg monthly)\")\n",
    "print()\n",
    "\n",
    "print(\"5. KEY METRICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Average Monthly Spend: ${df_clean['monthly_spend'].mean():,.2f}\")\n",
    "print(f\"Average Satisfaction Score: {df_clean['satisfaction_score'].mean():.2f}/10\")\n",
    "print(f\"Customers at Churn Risk: {df_clean['churn_risk'].sum()} ({df_clean['churn_risk'].mean()*100:.1f}%)\")\n",
    "print(f\"Average Engagement Score: {df_clean['engagement_score'].mean():.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Save Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the cleaned dataset\n",
    "df_clean.to_csv(clean_data_filename, index=False)\n",
    "print(f\"Clean data saved to: {clean_data_filename}\")\n",
    "print(f\"File contains {len(df_clean)} rows and {len(df_clean.columns)} columns\")\n",
    "print(\"\\nDataset is now ready for machine learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3: Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Recommendations\n",
    "\n",
    "Based on my analysis of CloudTech Solutions' customer data, I recommend the following:\n",
    "\n",
    "### Immediate Actions:\n",
    "\n",
    "1. **Data Collection Improvements:**\n",
    "   - [TODO: Add your recommendation about preventing duplicate customer IDs]\n",
    "   - [TODO: Add your recommendation about reducing missing satisfaction scores]\n",
    "\n",
    "2. **High-Priority Customer Segments:**\n",
    "   - [TODO: Which segment needs immediate attention and why?]\n",
    "   - [TODO: What action should be taken for at-risk high-value customers?]\n",
    "\n",
    "3. **Data Quality Monitoring:**\n",
    "   - [TODO: What metric should be monitored regularly?]\n",
    "   - [TODO: What threshold would indicate data quality problems?]\n",
    "\n",
    "### Long-term Improvements:\n",
    "\n",
    "1. **Systematic Data Governance:**\n",
    "   - [TODO: One suggestion for improving data entry processes]\n",
    "   \n",
    "2. **Feature Engineering for Churn Model:**\n",
    "   - [TODO: What additional data would improve churn prediction?]\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "[TODO: Write 2-3 sentences summarizing the most important findings from your analysis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BONUS: Advanced Analysis (Optional, +10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task 1: Advanced Imputation (+3 points)\n",
    "Implement KNN or Iterative imputation for better handling of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Implement advanced imputation\n",
    "# TODO: Use KNNImputer or IterativeImputer from sklearn\n",
    "# Compare results with simple imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task 2: Data Quality Visualization (+3 points)\n",
    "Create visualizations showing data quality improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Create before/after visualizations\n",
    "# TODO: Show the impact of your data cleaning process visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task 3: Reusable Pipeline (+4 points)\n",
    "Create a reusable function that encapsulates your entire data cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Create a reusable data preparation pipeline\n",
    "def prepare_customer_data(df_raw):\n",
    "    \"\"\"\n",
    "    Complete data preparation pipeline for customer data\n",
    "    \n",
    "    Parameters:\n",
    "    df_raw: Raw customer dataframe\n",
    "    \n",
    "    Returns:\n",
    "    df_clean: Cleaned and feature-engineered dataframe\n",
    "    report: Dictionary containing quality metrics\n",
    "    \"\"\"\n",
    "    # TODO: Implement your complete pipeline here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting:\n",
    "- [ ] Student name and ID filled in\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] Data quality issues identified and documented\n",
    "- [ ] Missing data strategy implemented\n",
    "- [ ] Outliers handled appropriately\n",
    "- [ ] Customer segments created\n",
    "- [ ] Data quality report completed\n",
    "- [ ] Recommendations provided\n",
    "- [ ] Clean CSV file saved with correct name\n",
    "\n",
    "**Files to submit:**\n",
    "1. This notebook (.ipynb)\n",
    "2. HTML export of notebook\n",
    "3. Your cleaned CSV data file\n",
    "\n",
    "**Great job completing your data preparation pipeline!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}