{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3A: Data Quality Assessment and Missing Data Strategies\n",
    "\n",
    "## ISM6251: Machine Learning for Business Applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to Week 3! This week, we dive deep into one of the most critical aspects of machine learning: **data preparation**. No matter how sophisticated your algorithms are, they cannot compensate for poor quality data.\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "- **Data Quality Assessment**: How to systematically evaluate your data\n",
    "- **Missing Data Analysis**: Understanding patterns and mechanisms\n",
    "- **Imputation Strategies**: When and how to fill missing values\n",
    "- **Decision Frameworks**: Making informed choices about data handling\n",
    "\n",
    "Remember: *\"Data preparation is not just a step in the ML pipeline—it's the foundation upon which all successful models are built.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the libraries we'll need throughout this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Missing data visualization\n",
    "!pip install missingno -q\n",
    "import missingno as msno\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Data Quality\n",
    "\n",
    "### 1.1 What Makes Data \"Good Quality\"?\n",
    "\n",
    "High-quality data exhibits several key characteristics:\n",
    "\n",
    "1. **Completeness**: Minimal missing values\n",
    "2. **Consistency**: Uniform formats and conventions\n",
    "3. **Accuracy**: Values reflect reality\n",
    "4. **Uniqueness**: No unwanted duplicates\n",
    "5. **Timeliness**: Data is current and relevant\n",
    "6. **Validity**: Values conform to defined rules\n",
    "\n",
    "Let's create a realistic dataset with various quality issues to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic e-commerce dataset with quality issues\n",
    "np.random.seed(42)\n",
    "\n",
    "n_customers = 1000\n",
    "\n",
    "# Generate base data\n",
    "customer_data = {\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'age': np.random.randint(18, 80, n_customers),\n",
    "    'income': np.random.normal(50000, 20000, n_customers),\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], \n",
    "                            n_customers, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n_customers, freq='6H'),\n",
    "    'total_purchases': np.random.poisson(5, n_customers),\n",
    "    'avg_order_value': np.random.exponential(75, n_customers),\n",
    "    'email_domain': np.random.choice(['gmail', 'yahoo', 'outlook', 'company'], \n",
    "                                   n_customers, p=[0.4, 0.2, 0.2, 0.2]),\n",
    "    'customer_segment': np.random.choice(['Bronze', 'Silver', 'Gold', 'Platinum'], \n",
    "                                        n_customers, p=[0.4, 0.3, 0.2, 0.1])\n",
    "}\n",
    "\n",
    "df_original = pd.DataFrame(customer_data)\n",
    "\n",
    "# Introduce realistic data quality issues\n",
    "df = df_original.copy()\n",
    "\n",
    "# 1. Missing values (MCAR - Missing Completely At Random)\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices, 'income'] = np.nan\n",
    "\n",
    "# 2. Missing values (MAR - Missing At Random, younger people less likely to report income)\n",
    "young_indices = df[df['age'] < 30].index\n",
    "missing_young = np.random.choice(young_indices, size=int(0.15 * len(young_indices)), replace=False)\n",
    "df.loc[missing_young, 'income'] = np.nan\n",
    "\n",
    "# 3. Missing values (MNAR - Missing Not At Random, high earners don't report)\n",
    "high_income_indices = df[df['income'] > 80000].index\n",
    "missing_high = np.random.choice(high_income_indices, size=int(0.1 * len(high_income_indices)), replace=False)\n",
    "df.loc[missing_high, 'income'] = np.nan\n",
    "\n",
    "# 4. Introduce outliers\n",
    "outlier_indices = np.random.choice(df.index, size=5, replace=False)\n",
    "df.loc[outlier_indices[0], 'age'] = 999  # Data entry error\n",
    "df.loc[outlier_indices[1], 'income'] = -5000  # Negative income\n",
    "df.loc[outlier_indices[2], 'avg_order_value'] = 10000  # Extreme purchase\n",
    "\n",
    "# 5. Introduce duplicates\n",
    "duplicate_indices = np.random.choice(df.index, size=10, replace=False)\n",
    "df = pd.concat([df, df.loc[duplicate_indices]], ignore_index=True)\n",
    "\n",
    "# 6. Introduce inconsistencies in city names\n",
    "inconsistent_indices = np.random.choice(df.index, size=20, replace=False)\n",
    "df.loc[inconsistent_indices[0:5], 'city'] = 'new york'  # lowercase\n",
    "df.loc[inconsistent_indices[5:10], 'city'] = 'LA'  # abbreviation\n",
    "df.loc[inconsistent_indices[10:15], 'city'] = 'Chicago, IL'  # with state\n",
    "df.loc[inconsistent_indices[15:20], 'city'] = 'Houston '  # trailing space\n",
    "\n",
    "# 7. Add some missing values to other columns\n",
    "df.loc[np.random.choice(df.index, 30), 'customer_segment'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 15), 'email_domain'] = np.nan\n",
    "\n",
    "print(f\"Dataset created with {len(df)} records and various quality issues\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Comprehensive Data Quality Assessment\n",
    "\n",
    "Let's create a comprehensive function to assess data quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_data_quality_report(df, target_col=None):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to analyze\n",
    "    target_col : str, optional\n",
    "        The target column for ML (if applicable)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Quality report with various metrics\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    \n",
    "    # Basic information\n",
    "    report['basic_info'] = {\n",
    "        'n_rows': len(df),\n",
    "        'n_columns': len(df.columns),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'duplicated_rows': df.duplicated().sum(),\n",
    "        'duplicated_rows_pct': (df.duplicated().sum() / len(df)) * 100\n",
    "    }\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing_data = pd.DataFrame({\n",
    "        'column': df.columns,\n",
    "        'missing_count': df.isnull().sum(),\n",
    "        'missing_pct': (df.isnull().sum() / len(df)) * 100,\n",
    "        'dtype': df.dtypes\n",
    "    })\n",
    "    missing_data = missing_data[missing_data['missing_count'] > 0].sort_values('missing_pct', ascending=False)\n",
    "    report['missing_data'] = missing_data\n",
    "    \n",
    "    # Data type analysis\n",
    "    report['dtypes'] = {\n",
    "        'numeric': list(df.select_dtypes(include=[np.number]).columns),\n",
    "        'categorical': list(df.select_dtypes(include=['object', 'category']).columns),\n",
    "        'datetime': list(df.select_dtypes(include=['datetime64']).columns)\n",
    "    }\n",
    "    \n",
    "    # Numeric columns analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_analysis = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        col_data = df[col].dropna()\n",
    "        if len(col_data) > 0:\n",
    "            q1, q3 = col_data.quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
    "            \n",
    "            numeric_analysis.append({\n",
    "                'column': col,\n",
    "                'mean': col_data.mean(),\n",
    "                'median': col_data.median(),\n",
    "                'std': col_data.std(),\n",
    "                'min': col_data.min(),\n",
    "                'max': col_data.max(),\n",
    "                'outliers_count': len(outliers),\n",
    "                'outliers_pct': (len(outliers) / len(col_data)) * 100,\n",
    "                'negative_values': (col_data < 0).sum(),\n",
    "                'zeros': (col_data == 0).sum()\n",
    "            })\n",
    "    \n",
    "    report['numeric_analysis'] = pd.DataFrame(numeric_analysis)\n",
    "    \n",
    "    # Categorical columns analysis\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_analysis = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        categorical_analysis.append({\n",
    "            'column': col,\n",
    "            'unique_values': df[col].nunique(),\n",
    "            'most_frequent': df[col].mode().iloc[0] if len(df[col].mode()) > 0 else None,\n",
    "            'most_frequent_pct': (df[col].value_counts().iloc[0] / len(df[col].dropna())) * 100 if len(df[col].value_counts()) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    report['categorical_analysis'] = pd.DataFrame(categorical_analysis)\n",
    "    \n",
    "    # Data quality score (0-100)\n",
    "    quality_score = 100\n",
    "    quality_score -= report['basic_info']['duplicated_rows_pct'] * 2  # Penalize duplicates\n",
    "    quality_score -= missing_data['missing_pct'].mean() if len(missing_data) > 0 else 0  # Penalize missing data\n",
    "    quality_score = max(0, quality_score)  # Ensure non-negative\n",
    "    \n",
    "    report['quality_score'] = quality_score\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate the quality report\n",
    "quality_report = comprehensive_data_quality_report(df)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n📊 Basic Information:\")\n",
    "for key, value in quality_report['basic_info'].items():\n",
    "    print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n❌ Missing Data:\")\n",
    "print(quality_report['missing_data'])\n",
    "\n",
    "print(f\"\\n📈 Numeric Columns Analysis:\")\n",
    "print(quality_report['numeric_analysis'])\n",
    "\n",
    "print(f\"\\n📝 Categorical Columns Analysis:\")\n",
    "print(quality_report['categorical_analysis'])\n",
    "\n",
    "print(f\"\\n⭐ Overall Data Quality Score: {quality_report['quality_score']:.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualizing Data Quality Issues\n",
    "\n",
    "Visualization helps us understand patterns in data quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Missing data matrix\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "msno.matrix(df, ax=ax1, sparkline=False)\n",
    "ax1.set_title('Missing Data Pattern Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Missing data bar chart\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "missing_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "ax2.bar(range(len(missing_counts)), missing_counts.values)\n",
    "ax2.set_xticks(range(len(missing_counts)))\n",
    "ax2.set_xticklabels(missing_counts.index, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Number of Missing Values')\n",
    "ax2.set_title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Missing data heatmap\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "msno.heatmap(df, ax=ax3)\n",
    "ax3.set_title('Missing Data Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Distribution of numeric columns with outliers\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns[:4]  # First 4 numeric columns\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    data = df[col].dropna()\n",
    "    ax4.boxplot(data, positions=[i], widths=0.6, patch_artist=True,\n",
    "                boxprops=dict(facecolor=f'C{i}', alpha=0.5))\n",
    "ax4.set_xticks(range(len(numeric_cols)))\n",
    "ax4.set_xticklabels(numeric_cols, rotation=45, ha='right')\n",
    "ax4.set_title('Outlier Detection (Boxplots)', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Data completeness by row\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "row_completeness = (df.notna().sum(axis=1) / len(df.columns)) * 100\n",
    "ax5.hist(row_completeness, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax5.set_xlabel('Row Completeness (%)')\n",
    "ax5.set_ylabel('Number of Rows')\n",
    "ax5.set_title('Distribution of Row Completeness', fontsize=14, fontweight='bold')\n",
    "ax5.axvline(x=row_completeness.mean(), color='red', linestyle='--', label=f'Mean: {row_completeness.mean():.1f}%')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Duplicate analysis\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "duplicate_counts = df.duplicated(keep=False).value_counts()\n",
    "colors = ['green', 'red']\n",
    "labels = ['Unique', 'Duplicate']\n",
    "ax6.pie(duplicate_counts.values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax6.set_title('Duplicate Records Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comprehensive Data Quality Visualization', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏋️ Practice Exercise 1: Data Quality Assessment\n",
    "\n",
    "Now it's your turn! Complete the following data quality assessment tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: Identify specific data quality issues\n",
    "\n",
    "# TODO: Find all customers with age > 100 (likely data entry errors)\n",
    "age_errors = df[___]  # Fill in the condition\n",
    "\n",
    "# TODO: Find all records with negative income\n",
    "negative_income = df[___]  # Fill in the condition\n",
    "\n",
    "# TODO: Find all city names that might be inconsistent (hint: use str.lower() and value_counts())\n",
    "city_variations = df['city'].___  # Complete the analysis\n",
    "\n",
    "# TODO: Calculate the percentage of rows that have at least one missing value\n",
    "rows_with_missing = ___  # Calculate this percentage\n",
    "\n",
    "print(f\"Customers with age > 100: {len(age_errors)}\")\n",
    "print(f\"Records with negative income: {len(negative_income)}\")\n",
    "print(f\"\\nCity name variations:\")\n",
    "print(city_variations)\n",
    "print(f\"\\nPercentage of rows with missing data: {rows_with_missing:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "age_errors = df[df['age'] > 100]\n",
    "negative_income = df[df['income'] < 0]\n",
    "city_variations = df['city'].str.lower().value_counts()\n",
    "rows_with_missing = (df.isnull().any(axis=1).sum() / len(df)) * 100\n",
    "\n",
    "print(f\"Customers with age > 100: {len(age_errors)}\")\n",
    "print(f\"Records with negative income: {len(negative_income)}\")\n",
    "print(f\"\\nCity name variations:\")\n",
    "print(city_variations.head(10))\n",
    "print(f\"\\nPercentage of rows with missing data: {rows_with_missing:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Missing Data Strategies\n",
    "\n",
    "### 2.1 Understanding Missing Data Mechanisms\n",
    "\n",
    "Before deciding how to handle missing data, we need to understand WHY it's missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data patterns by age group\n",
    "def analyze_missing_patterns(df, column, group_by):\n",
    "    \"\"\"\n",
    "    Analyze missing data patterns for a column grouped by another variable\n",
    "    \"\"\"\n",
    "    analysis = df.groupby(group_by).agg({\n",
    "        column: lambda x: x.isnull().sum(),\n",
    "        'customer_id': 'count'\n",
    "    }).rename(columns={column: 'missing_count', 'customer_id': 'total_count'})\n",
    "    \n",
    "    analysis['missing_pct'] = (analysis['missing_count'] / analysis['total_count']) * 100\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Create age groups\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 100, 1000], \n",
    "                         labels=['<30', '30-50', '50-100', '>100'])\n",
    "\n",
    "# Analyze missing income by age group\n",
    "missing_by_age = analyze_missing_patterns(df, 'income', 'age_group')\n",
    "print(\"Missing Income by Age Group:\")\n",
    "print(missing_by_age)\n",
    "print()\n",
    "\n",
    "# Visualize the pattern\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Missing percentage by age group\n",
    "axes[0].bar(missing_by_age.index.astype(str), missing_by_age['missing_pct'])\n",
    "axes[0].set_xlabel('Age Group')\n",
    "axes[0].set_ylabel('Missing Income (%)')\n",
    "axes[0].set_title('Missing Income by Age Group\\n(Evidence of MAR - younger people report less)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Income distribution for those who reported\n",
    "for age_grp in df['age_group'].unique():\n",
    "    if pd.notna(age_grp):\n",
    "        income_data = df[df['age_group'] == age_grp]['income'].dropna()\n",
    "        if len(income_data) > 0:\n",
    "            axes[1].hist(income_data, alpha=0.5, label=str(age_grp), bins=20)\n",
    "\n",
    "axes[1].set_xlabel('Income')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Income Distribution by Age Group\\n(For non-missing values)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Insight: The missing pattern suggests MAR (Missing At Random) - \"\n",
    "      \"younger people are less likely to report income.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Decision Framework: Drop vs Impute\n",
    "\n",
    "Let's implement a systematic decision framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_decision_framework(df, column):\n",
    "    \"\"\"\n",
    "    Recommend whether to drop or impute based on missing data characteristics\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    missing_count = df[column].isnull().sum()\n",
    "    missing_pct = (missing_count / total_rows) * 100\n",
    "    \n",
    "    # Decision rules\n",
    "    decision = {\n",
    "        'column': column,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_pct': missing_pct,\n",
    "        'recommendation': '',\n",
    "        'reasoning': []\n",
    "    }\n",
    "    \n",
    "    # Apply decision rules\n",
    "    if missing_pct > 50:\n",
    "        decision['recommendation'] = 'DROP COLUMN'\n",
    "        decision['reasoning'].append(f'More than 50% missing ({missing_pct:.1f}%)')\n",
    "    elif missing_pct > 30:\n",
    "        decision['recommendation'] = 'IMPUTE (CAREFULLY)'\n",
    "        decision['reasoning'].append(f'30-50% missing - significant but manageable')\n",
    "        decision['reasoning'].append('Consider advanced imputation methods')\n",
    "    elif missing_pct > 10:\n",
    "        decision['recommendation'] = 'IMPUTE'\n",
    "        decision['reasoning'].append(f'10-30% missing - standard imputation appropriate')\n",
    "    elif missing_pct > 5:\n",
    "        decision['recommendation'] = 'IMPUTE or DROP ROWS'\n",
    "        decision['reasoning'].append(f'5-10% missing - either approach viable')\n",
    "        decision['reasoning'].append(f'Consider dataset size ({total_rows} rows)')\n",
    "    else:\n",
    "        decision['recommendation'] = 'DROP ROWS'\n",
    "        decision['reasoning'].append(f'Less than 5% missing ({missing_pct:.1f}%)')\n",
    "        decision['reasoning'].append(f'Minimal data loss ({missing_count} rows)')\n",
    "    \n",
    "    # Additional considerations\n",
    "    if df[column].dtype in ['object', 'category']:\n",
    "        unique_values = df[column].nunique()\n",
    "        if unique_values > 50:\n",
    "            decision['reasoning'].append(f'High cardinality ({unique_values} unique values)')\n",
    "            if decision['recommendation'] != 'DROP COLUMN':\n",
    "                decision['recommendation'] += ' (or DROP COLUMN)'\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Apply framework to all columns with missing data\n",
    "print(\"=\"*70)\n",
    "print(\"MISSING DATA DECISION FRAMEWORK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "columns_with_missing = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "for col in columns_with_missing:\n",
    "    decision = missing_data_decision_framework(df, col)\n",
    "    print(f\"\\n📌 Column: {decision['column']}\")\n",
    "    print(f\"   Missing: {decision['missing_count']} ({decision['missing_pct']:.1f}%)\")\n",
    "    print(f\"   ✅ Recommendation: {decision['recommendation']}\")\n",
    "    print(f\"   Reasoning:\")\n",
    "    for reason in decision['reasoning']:\n",
    "        print(f\"      • {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Imputation Techniques Comparison\n",
    "\n",
    "Let's compare different imputation methods and their effectiveness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for imputation comparison\n",
    "# We'll use income as our target variable since it has missing values\n",
    "\n",
    "# Create a version where we know the true values\n",
    "df_complete = df[df['income'].notna()].copy()\n",
    "\n",
    "# Artificially create missing values for testing\n",
    "test_missing_indices = np.random.choice(df_complete.index, \n",
    "                                       size=int(0.2 * len(df_complete)), \n",
    "                                       replace=False)\n",
    "df_test = df_complete.copy()\n",
    "df_test.loc[test_missing_indices, 'income'] = np.nan\n",
    "\n",
    "# Store true values for comparison\n",
    "true_values = df_complete.loc[test_missing_indices, 'income']\n",
    "\n",
    "# Prepare features for imputation (numeric only for this example)\n",
    "feature_cols = ['age', 'total_purchases', 'avg_order_value']\n",
    "X = df_test[feature_cols + ['income']].copy()\n",
    "\n",
    "# Dictionary to store results\n",
    "imputation_results = {}\n",
    "\n",
    "# Method 1: Mean imputation\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "X_mean = X.copy()\n",
    "X_mean['income'] = mean_imputer.fit_transform(X[['income']])\n",
    "imputation_results['Mean'] = X_mean.loc[test_missing_indices, 'income']\n",
    "\n",
    "# Method 2: Median imputation\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "X_median = X.copy()\n",
    "X_median['income'] = median_imputer.fit_transform(X[['income']])\n",
    "imputation_results['Median'] = X_median.loc[test_missing_indices, 'income']\n",
    "\n",
    "# Method 3: KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_knn = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "imputation_results['KNN'] = X_knn.loc[test_missing_indices, 'income']\n",
    "\n",
    "# Method 4: Iterative imputation (MICE-like)\n",
    "iterative_imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "X_iterative = pd.DataFrame(\n",
    "    iterative_imputer.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "imputation_results['Iterative'] = X_iterative.loc[test_missing_indices, 'income']\n",
    "\n",
    "# Calculate errors\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "comparison_results = []\n",
    "for method, imputed_values in imputation_results.items():\n",
    "    mae = mean_absolute_error(true_values, imputed_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, imputed_values))\n",
    "    r2 = r2_score(true_values, imputed_values)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Method': method,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results).sort_values('MAE')\n",
    "\n",
    "print(\"Imputation Methods Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n📊 Lower MAE and RMSE is better, higher R² is better\")\n",
    "\n",
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: MAE comparison\n",
    "axes[0, 0].bar(comparison_df['Method'], comparison_df['MAE'])\n",
    "axes[0, 0].set_ylabel('Mean Absolute Error')\n",
    "axes[0, 0].set_title('MAE by Imputation Method')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plots for each method\n",
    "for i, (method, imputed_values) in enumerate(imputation_results.items()):\n",
    "    ax_idx = (i + 1) // 2, (i + 1) % 2\n",
    "    if i < 3:  # We have 4 methods, so use 3 subplots for scatter\n",
    "        axes[ax_idx].scatter(true_values, imputed_values, alpha=0.5, s=20)\n",
    "        axes[ax_idx].plot([true_values.min(), true_values.max()], \n",
    "                         [true_values.min(), true_values.max()], \n",
    "                         'r--', alpha=0.5)\n",
    "        axes[ax_idx].set_xlabel('True Values')\n",
    "        axes[ax_idx].set_ylabel('Imputed Values')\n",
    "        axes[ax_idx].set_title(f'{method} Imputation')\n",
    "        axes[ax_idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distribution comparison\n",
    "axes[1, 1].hist(true_values, alpha=0.5, label='True', bins=20)\n",
    "axes[1, 1].hist(imputation_results['KNN'], alpha=0.5, label='KNN Imputed', bins=20)\n",
    "axes[1, 1].set_xlabel('Income')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution: True vs KNN Imputed')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Imputation Methods Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Key Insight: Advanced methods (KNN, Iterative) generally perform better than simple methods\")\n",
    "print(\"   because they consider relationships between variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Practical Imputation Pipeline\n",
    "\n",
    "Let's create a practical imputation pipeline for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartImputer:\n",
    "    \"\"\"\n",
    "    A smart imputation class that handles different data types appropriately\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, numeric_strategy='knn', categorical_strategy='most_frequent', \n",
    "                 missing_threshold=0.5):\n",
    "        self.numeric_strategy = numeric_strategy\n",
    "        self.categorical_strategy = categorical_strategy\n",
    "        self.missing_threshold = missing_threshold\n",
    "        self.columns_to_drop = []\n",
    "        self.numeric_imputers = {}\n",
    "        self.categorical_imputers = {}\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit the imputer on the dataframe\n",
    "        \"\"\"\n",
    "        # Identify columns to drop (too many missing values)\n",
    "        for col in df.columns:\n",
    "            missing_pct = df[col].isnull().sum() / len(df)\n",
    "            if missing_pct > self.missing_threshold:\n",
    "                self.columns_to_drop.append(col)\n",
    "                print(f\"Will drop column '{col}': {missing_pct:.1%} missing\")\n",
    "        \n",
    "        # Prepare imputers for remaining columns\n",
    "        remaining_cols = [col for col in df.columns if col not in self.columns_to_drop]\n",
    "        \n",
    "        for col in remaining_cols:\n",
    "            if df[col].isnull().any():\n",
    "                if df[col].dtype in ['object', 'category']:\n",
    "                    # Categorical imputation\n",
    "                    imputer = SimpleImputer(strategy=self.categorical_strategy)\n",
    "                    imputer.fit(df[[col]])\n",
    "                    self.categorical_imputers[col] = imputer\n",
    "                else:\n",
    "                    # Numeric imputation\n",
    "                    if self.numeric_strategy == 'knn':\n",
    "                        # For KNN, we need all numeric columns\n",
    "                        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "                        numeric_cols = [c for c in numeric_cols if c not in self.columns_to_drop]\n",
    "                        if col not in self.numeric_imputers:  # Avoid duplicate KNN imputers\n",
    "                            imputer = KNNImputer(n_neighbors=5)\n",
    "                            imputer.fit(df[numeric_cols])\n",
    "                            for nc in numeric_cols:\n",
    "                                self.numeric_imputers[nc] = (imputer, numeric_cols)\n",
    "                    else:\n",
    "                        imputer = SimpleImputer(strategy=self.numeric_strategy)\n",
    "                        imputer.fit(df[[col]])\n",
    "                        self.numeric_imputers[col] = (imputer, [col])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transform the dataframe by imputing missing values\n",
    "        \"\"\"\n",
    "        df_imputed = df.copy()\n",
    "        \n",
    "        # Drop columns with too many missing values\n",
    "        df_imputed = df_imputed.drop(columns=self.columns_to_drop, errors='ignore')\n",
    "        \n",
    "        # Apply categorical imputers\n",
    "        for col, imputer in self.categorical_imputers.items():\n",
    "            if col in df_imputed.columns:\n",
    "                df_imputed[col] = imputer.transform(df_imputed[[col]]).ravel()\n",
    "        \n",
    "        # Apply numeric imputers\n",
    "        if self.numeric_strategy == 'knn' and self.numeric_imputers:\n",
    "            # For KNN, impute all numeric columns at once\n",
    "            imputer, numeric_cols = list(self.numeric_imputers.values())[0]\n",
    "            numeric_cols = [c for c in numeric_cols if c in df_imputed.columns]\n",
    "            if numeric_cols:\n",
    "                df_imputed[numeric_cols] = imputer.transform(df_imputed[numeric_cols])\n",
    "        else:\n",
    "            # For other strategies, impute column by column\n",
    "            for col, (imputer, cols) in self.numeric_imputers.items():\n",
    "                if col in df_imputed.columns:\n",
    "                    df_imputed[col] = imputer.transform(df_imputed[[col]]).ravel()\n",
    "        \n",
    "        return df_imputed\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"\n",
    "        Fit and transform in one step\n",
    "        \"\"\"\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "\n",
    "# Apply the smart imputer to our dataset\n",
    "print(\"Applying Smart Imputation Pipeline...\\n\")\n",
    "\n",
    "smart_imputer = SmartImputer(\n",
    "    numeric_strategy='knn',\n",
    "    categorical_strategy='most_frequent',\n",
    "    missing_threshold=0.5\n",
    ")\n",
    "\n",
    "df_imputed = smart_imputer.fit_transform(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Imputation Complete!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compare before and after\n",
    "print(f\"\\nBefore imputation:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nAfter imputation:\")\n",
    "print(f\"  Shape: {df_imputed.shape}\")\n",
    "print(f\"  Missing values: {df_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# Show remaining missing values by column\n",
    "remaining_missing = df_imputed.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "if len(remaining_missing) > 0:\n",
    "    print(f\"\\nRemaining missing values by column:\")\n",
    "    print(remaining_missing)\n",
    "else:\n",
    "    print(f\"\\n✅ All missing values have been handled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏋️ Practice Exercise 2: Missing Data Handling\n",
    "\n",
    "Now practice your missing data handling skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: Create your own imputation strategy\n",
    "\n",
    "# Create a test dataset with specific missing patterns\n",
    "exercise_data = pd.DataFrame({\n",
    "    'feature_1': [1, 2, np.nan, 4, 5, np.nan, 7, 8, 9, 10],\n",
    "    'feature_2': [10, np.nan, 30, 40, np.nan, 60, 70, np.nan, 90, 100],\n",
    "    'feature_3': ['A', 'B', np.nan, 'A', 'C', 'B', np.nan, 'A', 'B', 'C'],\n",
    "    'target': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "})\n",
    "\n",
    "print(\"Exercise Dataset:\")\n",
    "print(exercise_data)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(exercise_data.isnull().sum())\n",
    "\n",
    "# TODO: Implement the following:\n",
    "# 1. Impute feature_1 using the median\n",
    "# 2. Impute feature_2 using forward fill\n",
    "# 3. Impute feature_3 using the mode\n",
    "# 4. Calculate the correlation between imputed features and target\n",
    "\n",
    "# Your code here:\n",
    "exercise_imputed = exercise_data.copy()\n",
    "\n",
    "# Impute feature_1\n",
    "# exercise_imputed['feature_1'] = ...\n",
    "\n",
    "# Impute feature_2\n",
    "# exercise_imputed['feature_2'] = ...\n",
    "\n",
    "# Impute feature_3\n",
    "# exercise_imputed['feature_3'] = ...\n",
    "\n",
    "# Calculate correlations\n",
    "# correlations = ...\n",
    "\n",
    "print(\"\\nYour imputed dataset:\")\n",
    "# print(exercise_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "exercise_imputed = exercise_data.copy()\n",
    "\n",
    "# Impute feature_1 with median\n",
    "exercise_imputed['feature_1'].fillna(exercise_imputed['feature_1'].median(), inplace=True)\n",
    "\n",
    "# Impute feature_2 with forward fill\n",
    "exercise_imputed['feature_2'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Impute feature_3 with mode\n",
    "mode_value = exercise_imputed['feature_3'].mode()[0]\n",
    "exercise_imputed['feature_3'].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Calculate correlations with target\n",
    "# First, encode categorical feature_3\n",
    "exercise_imputed['feature_3_encoded'] = exercise_imputed['feature_3'].map({'A': 0, 'B': 1, 'C': 2})\n",
    "\n",
    "correlations = exercise_imputed[['feature_1', 'feature_2', 'feature_3_encoded', 'target']].corr()['target'].drop('target')\n",
    "\n",
    "print(\"\\nImputed dataset:\")\n",
    "print(exercise_imputed)\n",
    "print(\"\\nCorrelations with target:\")\n",
    "print(correlations)\n",
    "print(\"\\n✅ Exercise complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### 📚 What We've Learned\n",
    "\n",
    "1. **Data Quality Assessment**\n",
    "   - Systematic evaluation of completeness, consistency, accuracy, and uniqueness\n",
    "   - Visual and statistical methods to identify quality issues\n",
    "   - Importance of understanding data before modeling\n",
    "\n",
    "2. **Missing Data Mechanisms**\n",
    "   - **MCAR**: Truly random missingness\n",
    "   - **MAR**: Missingness depends on observed variables\n",
    "   - **MNAR**: Missingness depends on the missing value itself\n",
    "\n",
    "3. **Decision Framework**\n",
    "   - **Drop columns**: >50% missing or irrelevant\n",
    "   - **Drop rows**: <5% affected and large dataset\n",
    "   - **Impute**: 10-50% missing in important features\n",
    "\n",
    "4. **Imputation Strategies**\n",
    "   - Simple methods: Mean, median, mode\n",
    "   - Advanced methods: KNN, iterative imputation\n",
    "   - Consider data type and missing mechanism\n",
    "\n",
    "### 🎯 Best Practices\n",
    "\n",
    "1. **Always assess data quality first** - understand before acting\n",
    "2. **Document your decisions** - explain why you chose specific strategies\n",
    "3. **Validate imputation impact** - check if relationships are preserved\n",
    "4. **Consider domain knowledge** - statistical rules aren't everything\n",
    "5. **Create reproducible pipelines** - consistency is key\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "In the next notebook (week03b), we'll explore:\n",
    "- Advanced filtering techniques\n",
    "- Outlier detection and treatment\n",
    "- Data grouping and aggregation\n",
    "- Feature engineering strategies\n",
    "\n",
    "Remember: **Quality data preparation is the foundation of successful machine learning!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}