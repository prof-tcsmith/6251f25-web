{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3B: Data Filtering and Grouping Operations\n",
    "\n",
    "## ISM6251: Machine Learning for Business Applications\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Apply advanced filtering techniques to extract relevant data subsets\n",
    "- Use grouping operations for aggregation and analysis\n",
    "- Combine filtering and grouping for complex data transformations\n",
    "- Handle real-world data manipulation scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Filtering Techniques\n",
    "\n",
    "Filtering is one of the most fundamental operations in data preparation. It allows us to:\n",
    "- Focus on relevant subsets of data\n",
    "- Remove outliers and anomalies\n",
    "- Create training/validation splits\n",
    "- Prepare data for specific analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Sample Dataset\n",
    "\n",
    "Let's create a comprehensive sales dataset to demonstrate filtering and grouping operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample sales data\n",
    "n_records = 1000\n",
    "\n",
    "# Create date range\n",
    "start_date = datetime(2023, 1, 1)\n",
    "dates = [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_records)]\n",
    "\n",
    "# Create sample data\n",
    "sales_data = pd.DataFrame({\n",
    "    'transaction_id': range(1001, 1001 + n_records),\n",
    "    'date': dates,\n",
    "    'customer_id': np.random.choice(range(100, 200), n_records),\n",
    "    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse'], n_records),\n",
    "    'category': np.random.choice(['Electronics', 'Accessories'], n_records),\n",
    "    'quantity': np.random.randint(1, 6, n_records),\n",
    "    'unit_price': np.random.uniform(50, 2000, n_records),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_records),\n",
    "    'sales_rep': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'], n_records),\n",
    "    'customer_type': np.random.choice(['Regular', 'Premium', 'New'], n_records, p=[0.5, 0.3, 0.2])\n",
    "})\n",
    "\n",
    "# Calculate total amount\n",
    "sales_data['total_amount'] = sales_data['quantity'] * sales_data['unit_price']\n",
    "\n",
    "# Add some missing values for demonstration\n",
    "missing_indices = np.random.choice(sales_data.index, 50, replace=False)\n",
    "sales_data.loc[missing_indices, 'customer_type'] = np.nan\n",
    "\n",
    "# Sort by date\n",
    "sales_data = sales_data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset created with {len(sales_data)} records\")\n",
    "print(f\"\\nDataset shape: {sales_data.shape}\")\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Boolean Indexing\n",
    "\n",
    "Boolean indexing is the foundation of filtering in pandas. It creates a mask of True/False values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple boolean condition\n",
    "high_value_mask = sales_data['total_amount'] > 5000\n",
    "\n",
    "print(\"Boolean mask (first 10 values):\")\n",
    "print(high_value_mask.head(10))\n",
    "print(f\"\\nNumber of True values: {high_value_mask.sum()}\")\n",
    "print(f\"Percentage of high-value transactions: {high_value_mask.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the boolean mask\n",
    "high_value_sales = sales_data[high_value_mask]\n",
    "\n",
    "print(f\"Original dataset: {len(sales_data)} records\")\n",
    "print(f\"Filtered dataset: {len(high_value_sales)} records\")\n",
    "print(f\"\\nSample of high-value transactions:\")\n",
    "high_value_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Multiple Conditions\n",
    "\n",
    "Combine conditions using `&` (and), `|` (or), and `~` (not):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex filtering with multiple conditions\n",
    "filtered_data = sales_data[\n",
    "    (sales_data['region'] == 'North') & \n",
    "    (sales_data['total_amount'] > 3000) & \n",
    "    (sales_data['product'].isin(['Laptop', 'Phone']))\n",
    "]\n",
    "\n",
    "print(f\"Filtered records: {len(filtered_data)}\")\n",
    "print(f\"\\nAverage transaction value: ${filtered_data['total_amount'].mean():.2f}\")\n",
    "print(f\"\\nProduct distribution:\")\n",
    "print(filtered_data['product'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Query Method\n",
    "\n",
    "The `.query()` method provides a more readable syntax for filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using query method for filtering\n",
    "premium_electronics = sales_data.query(\n",
    "    \"customer_type == 'Premium' and category == 'Electronics' and total_amount > 2000\"\n",
    ")\n",
    "\n",
    "print(f\"Premium electronics transactions: {len(premium_electronics)}\")\n",
    "print(f\"\\nTop 5 transactions by amount:\")\n",
    "premium_electronics.nlargest(5, 'total_amount')[['date', 'product', 'total_amount', 'sales_rep']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Advanced Filtering Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date range\n",
    "q2_start = pd.Timestamp('2023-04-01')\n",
    "q2_end = pd.Timestamp('2023-06-30')\n",
    "\n",
    "q2_sales = sales_data[\n",
    "    (sales_data['date'] >= q2_start) & \n",
    "    (sales_data['date'] <= q2_end)\n",
    "]\n",
    "\n",
    "print(f\"Q2 2023 Sales: {len(q2_sales)} transactions\")\n",
    "print(f\"Total Q2 Revenue: ${q2_sales['total_amount'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using string methods\n",
    "phone_related = sales_data[\n",
    "    sales_data['product'].str.contains('Phone', case=False, na=False)\n",
    "]\n",
    "\n",
    "print(f\"Phone-related products: {len(phone_related)} transactions\")\n",
    "print(f\"Unique products: {phone_related['product'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by percentile\n",
    "percentile_75 = sales_data['total_amount'].quantile(0.75)\n",
    "top_quartile = sales_data[sales_data['total_amount'] > percentile_75]\n",
    "\n",
    "print(f\"75th percentile threshold: ${percentile_75:.2f}\")\n",
    "print(f\"Transactions in top quartile: {len(top_quartile)}\")\n",
    "print(f\"\\nTop quartile statistics:\")\n",
    "print(top_quartile['total_amount'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Exercise 1: Filtering\n",
    "\n",
    "Complete the following filtering tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find all transactions from the 'West' region in Q3 2023 (July-September)\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# TODO: Find transactions where the sales rep is either 'Alice' or 'Bob' \n",
    "# and the quantity is greater than 3\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# TODO: Find the top 10% of transactions by total_amount\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Grouping and Aggregation\n",
    "\n",
    "Grouping allows us to:\n",
    "- Summarize data by categories\n",
    "- Calculate statistics for different segments\n",
    "- Identify patterns and trends\n",
    "- Prepare data for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by single column\n",
    "region_summary = sales_data.groupby('region')['total_amount'].sum()\n",
    "\n",
    "print(\"Total sales by region:\")\n",
    "print(region_summary.sort_values(ascending=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "region_summary.sort_values().plot(kind='barh')\n",
    "plt.title('Total Sales by Region')\n",
    "plt.xlabel('Total Sales ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiple Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregation functions\n",
    "product_stats = sales_data.groupby('product')['total_amount'].agg([\n",
    "    'count',\n",
    "    'mean',\n",
    "    'median',\n",
    "    'std',\n",
    "    'min',\n",
    "    'max',\n",
    "    'sum'\n",
    "]).round(2)\n",
    "\n",
    "product_stats.columns = ['Transactions', 'Avg_Amount', 'Median_Amount', \n",
    "                         'Std_Dev', 'Min_Amount', 'Max_Amount', 'Total_Sales']\n",
    "\n",
    "print(\"Product Performance Statistics:\")\n",
    "product_stats.sort_values('Total_Sales', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Grouping by Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "region_product = sales_data.groupby(['region', 'product']).agg({\n",
    "    'total_amount': ['sum', 'mean'],\n",
    "    'quantity': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "region_product.columns = ['_'.join(col).strip() for col in region_product.columns]\n",
    "region_product = region_product.rename(columns={'transaction_id_count': 'num_transactions'})\n",
    "\n",
    "print(\"Top 10 Region-Product Combinations by Total Sales:\")\n",
    "region_product.sort_values('total_amount_sum', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Custom Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom aggregation functions\n",
    "def revenue_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "def coefficient_of_variation(x):\n",
    "    return (x.std() / x.mean()) * 100 if x.mean() > 0 else 0\n",
    "\n",
    "# Apply custom functions\n",
    "sales_rep_analysis = sales_data.groupby('sales_rep')['total_amount'].agg([\n",
    "    'count',\n",
    "    'sum',\n",
    "    'mean',\n",
    "    ('range', revenue_range),\n",
    "    ('cv', coefficient_of_variation)\n",
    "]).round(2)\n",
    "\n",
    "sales_rep_analysis.columns = ['Num_Sales', 'Total_Revenue', 'Avg_Sale', \n",
    "                              'Revenue_Range', 'Coefficient_Variation']\n",
    "\n",
    "print(\"Sales Representative Performance Analysis:\")\n",
    "print(sales_rep_analysis.sort_values('Total_Revenue', ascending=False))\n",
    "print(\"\\nNote: Coefficient of Variation measures consistency (lower = more consistent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Transform and Apply Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add group-level statistics to original dataframe\n",
    "sales_data['region_avg'] = sales_data.groupby('region')['total_amount'].transform('mean')\n",
    "sales_data['above_region_avg'] = sales_data['total_amount'] > sales_data['region_avg']\n",
    "\n",
    "# Calculate percentage above regional average\n",
    "pct_above = sales_data.groupby('region')['above_region_avg'].mean() * 100\n",
    "\n",
    "print(\"Percentage of transactions above regional average:\")\n",
    "for region, pct in pct_above.items():\n",
    "    print(f\"{region}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize sales within each product category\n",
    "def normalize_group(x):\n",
    "    return (x - x.mean()) / x.std() if x.std() > 0 else 0\n",
    "\n",
    "sales_data['normalized_amount'] = sales_data.groupby('product')['total_amount'].transform(normalize_group)\n",
    "\n",
    "# Find outliers (transactions with normalized amount > 2 or < -2)\n",
    "outliers = sales_data[abs(sales_data['normalized_amount']) > 2]\n",
    "\n",
    "print(f\"Number of outlier transactions: {len(outliers)}\")\n",
    "print(f\"\\nOutlier distribution by product:\")\n",
    "print(outliers['product'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Time-based Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time components\n",
    "sales_data['month'] = sales_data['date'].dt.month\n",
    "sales_data['quarter'] = sales_data['date'].dt.quarter\n",
    "sales_data['day_of_week'] = sales_data['date'].dt.dayofweek\n",
    "sales_data['week_of_year'] = sales_data['date'].dt.isocalendar().week\n",
    "\n",
    "# Monthly sales trend\n",
    "monthly_sales = sales_data.groupby('month').agg({\n",
    "    'total_amount': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "monthly_sales.columns = ['Total_Revenue', 'Num_Transactions', 'Total_Units']\n",
    "monthly_sales['Avg_Transaction'] = monthly_sales['Total_Revenue'] / monthly_sales['Num_Transactions']\n",
    "\n",
    "# Visualize monthly trend\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "monthly_sales['Total_Revenue'].plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "axes[0, 0].set_title('Monthly Revenue')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Revenue ($)')\n",
    "\n",
    "monthly_sales['Num_Transactions'].plot(kind='bar', ax=axes[0, 1], color='lightgreen')\n",
    "axes[0, 1].set_title('Monthly Transaction Count')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Number of Transactions')\n",
    "\n",
    "monthly_sales['Avg_Transaction'].plot(kind='line', ax=axes[1, 0], marker='o', color='coral')\n",
    "axes[1, 0].set_title('Average Transaction Value by Month')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Avg Transaction ($)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Day of week analysis\n",
    "dow_sales = sales_data.groupby('day_of_week')['total_amount'].sum()\n",
    "dow_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "dow_sales.index = dow_labels\n",
    "dow_sales.plot(kind='bar', ax=axes[1, 1], color='plum')\n",
    "axes[1, 1].set_title('Sales by Day of Week')\n",
    "axes[1, 1].set_xlabel('Day')\n",
    "axes[1, 1].set_ylabel('Total Sales ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Monthly Sales Summary:\")\n",
    "print(monthly_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Exercise 2: Grouping\n",
    "\n",
    "Complete the following grouping tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the average quantity sold by each sales rep for each product\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# TODO: Find the best performing day of the week for each region\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# TODO: Calculate the running total of sales for each sales rep over time\n",
    "# Hint: Use groupby with cumsum()\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Combining Filtering and Grouping\n",
    "\n",
    "Real-world analysis often requires combining filtering and grouping operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Filter Then Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for premium customers, then analyze by region\n",
    "premium_only = sales_data[sales_data['customer_type'] == 'Premium']\n",
    "\n",
    "premium_by_region = premium_only.groupby('region').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'customer_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "premium_by_region.columns = ['Total_Revenue', 'Avg_Transaction', \n",
    "                             'Num_Transactions', 'Unique_Customers']\n",
    "\n",
    "print(\"Premium Customer Analysis by Region:\")\n",
    "print(premium_by_region)\n",
    "print(f\"\\nTotal Premium Revenue: ${premium_by_region['Total_Revenue'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Group Then Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate customer lifetime value and filter top customers\n",
    "customer_ltv = sales_data.groupby('customer_id').agg({\n",
    "    'total_amount': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'date': ['min', 'max']\n",
    "}).round(2)\n",
    "\n",
    "customer_ltv.columns = ['Total_Spent', 'Num_Purchases', 'First_Purchase', 'Last_Purchase']\n",
    "customer_ltv['Days_Active'] = (customer_ltv['Last_Purchase'] - customer_ltv['First_Purchase']).dt.days\n",
    "customer_ltv['Avg_Purchase'] = customer_ltv['Total_Spent'] / customer_ltv['Num_Purchases']\n",
    "\n",
    "# Filter top 20% of customers by total spent\n",
    "threshold = customer_ltv['Total_Spent'].quantile(0.8)\n",
    "top_customers = customer_ltv[customer_ltv['Total_Spent'] > threshold]\n",
    "\n",
    "print(f\"Top 20% Customer Threshold: ${threshold:.2f}\")\n",
    "print(f\"Number of top customers: {len(top_customers)}\")\n",
    "print(f\"\\nTop 10 Customers by Lifetime Value:\")\n",
    "print(top_customers.nlargest(10, 'Total_Spent')[['Total_Spent', 'Num_Purchases', 'Avg_Purchase']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Complex Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex analysis: Product performance by customer segment in Q2\n",
    "\n",
    "# Step 1: Filter for Q2\n",
    "q2_data = sales_data[\n",
    "    (sales_data['date'] >= '2023-04-01') & \n",
    "    (sales_data['date'] <= '2023-06-30')\n",
    "].copy()\n",
    "\n",
    "# Step 2: Clean customer_type (fill NaN with 'Unknown')\n",
    "q2_data['customer_type'] = q2_data['customer_type'].fillna('Unknown')\n",
    "\n",
    "# Step 3: Group by customer type and product\n",
    "segment_product_analysis = q2_data.groupby(['customer_type', 'product']).agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "segment_product_analysis.columns = ['Revenue', 'Avg_Transaction', 'Num_Orders', 'Units_Sold']\n",
    "segment_product_analysis = segment_product_analysis.reset_index()\n",
    "\n",
    "# Step 4: Calculate metrics\n",
    "segment_product_analysis['Revenue_per_Unit'] = (\n",
    "    segment_product_analysis['Revenue'] / segment_product_analysis['Units_Sold']\n",
    ").round(2)\n",
    "\n",
    "# Step 5: Find top product for each segment\n",
    "top_products = segment_product_analysis.loc[\n",
    "    segment_product_analysis.groupby('customer_type')['Revenue'].idxmax()\n",
    "]\n",
    "\n",
    "print(\"Top Product by Customer Segment (Q2 2023):\")\n",
    "print(\"=\"*60)\n",
    "for _, row in top_products.iterrows():\n",
    "    print(f\"\\n{row['customer_type']} Customers:\")\n",
    "    print(f\"  Top Product: {row['product']}\")\n",
    "    print(f\"  Revenue: ${row['Revenue']:,.2f}\")\n",
    "    print(f\"  Orders: {int(row['Num_Orders'])}\")\n",
    "    print(f\"  Avg Transaction: ${row['Avg_Transaction']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Pivot Tables for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for region-product analysis\n",
    "pivot_revenue = pd.pivot_table(\n",
    "    sales_data,\n",
    "    values='total_amount',\n",
    "    index='product',\n",
    "    columns='region',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").round(2)\n",
    "\n",
    "# Add totals\n",
    "pivot_revenue['Total'] = pivot_revenue.sum(axis=1)\n",
    "pivot_revenue.loc['Total'] = pivot_revenue.sum()\n",
    "\n",
    "print(\"Revenue by Product and Region:\")\n",
    "print(pivot_revenue)\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    pivot_revenue.iloc[:-1, :-1],  # Exclude totals for heatmap\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Revenue ($)'}\n",
    ")\n",
    "plt.title('Revenue Heatmap: Products vs Regions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Exercise 3: Combined Operations\n",
    "\n",
    "Complete these combined filtering and grouping tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the best performing sales rep in each region for Electronics only\n",
    "# Best = highest total revenue\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# TODO: Calculate the month-over-month growth rate for each product\n",
    "# Hint: Use groupby, then calculate percentage change\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# TODO: Identify customers who have purchased from all 4 regions\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Performance Optimization\n",
    "\n",
    "When working with large datasets, optimization becomes crucial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Efficient Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a larger dataset for performance testing\n",
    "large_data = pd.concat([sales_data] * 100, ignore_index=True)\n",
    "print(f\"Large dataset size: {len(large_data):,} records\")\n",
    "\n",
    "# Method 1: Chained filtering (inefficient)\n",
    "start = time.time()\n",
    "result1 = large_data[large_data['region'] == 'North']\n",
    "result1 = result1[result1['product'] == 'Laptop']\n",
    "result1 = result1[result1['total_amount'] > 1000]\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Method 2: Single boolean mask (efficient)\n",
    "start = time.time()\n",
    "result2 = large_data[\n",
    "    (large_data['region'] == 'North') & \n",
    "    (large_data['product'] == 'Laptop') & \n",
    "    (large_data['total_amount'] > 1000)\n",
    "]\n",
    "time2 = time.time() - start\n",
    "\n",
    "# Method 3: Query method\n",
    "start = time.time()\n",
    "result3 = large_data.query(\n",
    "    \"region == 'North' and product == 'Laptop' and total_amount > 1000\"\n",
    ")\n",
    "time3 = time.time() - start\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"Chained filtering: {time1:.4f} seconds\")\n",
    "print(f\"Single mask: {time2:.4f} seconds\")\n",
    "print(f\"Query method: {time3:.4f} seconds\")\n",
    "print(f\"\\nSpeedup: {time1/time2:.2f}x for single mask vs chained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Efficient Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize data types before grouping\n",
    "optimized_data = large_data.copy()\n",
    "\n",
    "# Convert to categorical for repeated values\n",
    "categorical_cols = ['region', 'product', 'category', 'sales_rep', 'customer_type']\n",
    "for col in categorical_cols:\n",
    "    optimized_data[col] = optimized_data[col].astype('category')\n",
    "\n",
    "# Compare memory usage\n",
    "original_memory = large_data.memory_usage(deep=True).sum() / 1024**2\n",
    "optimized_memory = optimized_data.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"Original memory: {original_memory:.2f} MB\")\n",
    "print(f\"Optimized memory: {optimized_memory:.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - optimized_memory/original_memory)*100:.1f}%\")\n",
    "\n",
    "# Compare grouping performance\n",
    "start = time.time()\n",
    "result1 = large_data.groupby(['region', 'product'])['total_amount'].sum()\n",
    "time1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result2 = optimized_data.groupby(['region', 'product'])['total_amount'].sum()\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"\\nGrouping performance:\")\n",
    "print(f\"Original data: {time1:.4f} seconds\")\n",
    "print(f\"Optimized data: {time2:.4f} seconds\")\n",
    "print(f\"Speedup: {time1/time2:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Filtering Best Practices:\n",
    "1. **Use vectorized operations** instead of loops\n",
    "2. **Combine conditions** in a single mask when possible\n",
    "3. **Use `.isin()` for multiple value checks**\n",
    "4. **Consider `.query()` for complex conditions**\n",
    "5. **Filter early** in your data pipeline\n",
    "\n",
    "### Grouping Best Practices:\n",
    "1. **Use appropriate aggregation functions**\n",
    "2. **Consider memory usage** with large groups\n",
    "3. **Use `.agg()` for multiple aggregations**\n",
    "4. **Optimize data types** before grouping\n",
    "5. **Use pivot tables** for cross-tabulation\n",
    "\n",
    "### Performance Tips:\n",
    "1. **Convert to categorical** for repeated strings\n",
    "2. **Use appropriate data types** (int32 vs int64)\n",
    "3. **Filter before grouping** when possible\n",
    "4. **Cache intermediate results**\n",
    "5. **Consider chunking** for very large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercise: Real-World Scenario\n",
    "\n",
    "You're tasked with identifying high-value customer segments for a targeted marketing campaign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this analysis\n",
    "\n",
    "# 1. Identify customers who:\n",
    "#    - Have made at least 5 purchases\n",
    "#    - Have average transaction > $2000\n",
    "#    - Have purchased in at least 2 different quarters\n",
    "\n",
    "# 2. For these high-value customers, calculate:\n",
    "#    - Their preferred product category\n",
    "#    - Their most active region\n",
    "#    - Their total lifetime value\n",
    "\n",
    "# 3. Create a summary report showing:\n",
    "#    - Number of high-value customers\n",
    "#    - Their contribution to total revenue\n",
    "#    - Recommendations for marketing campaign\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've covered:\n",
    "- **Advanced filtering techniques** using boolean indexing, query methods, and complex conditions\n",
    "- **Grouping and aggregation** operations for data summarization\n",
    "- **Combining operations** for complex data analysis\n",
    "- **Performance optimization** strategies for large datasets\n",
    "\n",
    "These skills form the foundation for effective data manipulation and are essential for preparing data for machine learning models.\n",
    "\n",
    "### Next Steps:\n",
    "- Practice with your own datasets\n",
    "- Explore more advanced pandas functionality\n",
    "- Learn about SQL for database operations\n",
    "- Apply these techniques to real business problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}