{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3C: Building Data Preparation Pipelines\n",
    "\n",
    "## ISM6251: Machine Learning for Business Applications\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Design and implement complete data preparation pipelines\n",
    "- Apply data cleaning, transformation, and validation in sequence\n",
    "- Create reusable data processing functions\n",
    "- Handle real-world data challenges systematically\n",
    "- Build pipelines that scale to production environments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Why Data Pipelines?\n",
    "\n",
    "Data pipelines are essential for:\n",
    "- **Reproducibility**: Ensure consistent data processing\n",
    "- **Scalability**: Handle growing data volumes\n",
    "- **Maintainability**: Easy to update and debug\n",
    "- **Automation**: Reduce manual intervention\n",
    "- **Quality Control**: Systematic validation at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import logging\n",
    "\n",
    "# Configure display and warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a Realistic Dataset\n",
    "\n",
    "Let's create a comprehensive e-commerce dataset with intentional data quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ecommerce_dataset(n_records: int = 2000, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a realistic e-commerce dataset with various data quality issues.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate base data\n",
    "    data = {\n",
    "        'order_id': [f'ORD{str(i).zfill(6)}' for i in range(1, n_records + 1)],\n",
    "        'customer_id': np.random.choice(range(1000, 2000), n_records),\n",
    "        'order_date': [datetime(2023, 1, 1) + timedelta(days=np.random.randint(0, 365)) \n",
    "                      for _ in range(n_records)],\n",
    "        'product_name': np.random.choice(\n",
    "            ['iPhone 14', 'Samsung Galaxy S23', 'iPad Pro', 'MacBook Pro', 'AirPods Pro',\n",
    "             'Dell XPS 13', 'Sony WH-1000XM5', 'Nintendo Switch', 'PS5', 'Xbox Series X'],\n",
    "            n_records\n",
    "        ),\n",
    "        'category': np.random.choice(\n",
    "            ['Electronics', 'electronics', 'ELECTRONICS', 'Electrnics', 'Electronic'],  # Intentional inconsistency\n",
    "            n_records\n",
    "        ),\n",
    "        'quantity': np.random.randint(1, 10, n_records),\n",
    "        'unit_price': np.random.uniform(50, 2500, n_records),\n",
    "        'discount_percent': np.random.choice([0, 5, 10, 15, 20, 25, 100, -10], n_records, p=[0.3, 0.2, 0.2, 0.15, 0.1, 0.03, 0.01, 0.01]),  # Invalid values\n",
    "        'shipping_cost': np.random.uniform(0, 50, n_records),\n",
    "        'payment_method': np.random.choice(\n",
    "            ['Credit Card', 'credit card', 'PayPal', 'paypal', 'Debit Card', 'Cash', None],\n",
    "            n_records, p=[0.3, 0.1, 0.2, 0.05, 0.2, 0.1, 0.05]\n",
    "        ),\n",
    "        'shipping_address': np.random.choice(\n",
    "            ['123 Main St, New York, NY 10001', '456 Oak Ave, Los Angeles, CA 90001',\n",
    "             '789 Pine Rd, Chicago, IL 60601', '', None, 'Invalid Address'],\n",
    "            n_records, p=[0.4, 0.3, 0.2, 0.05, 0.03, 0.02]\n",
    "        ),\n",
    "        'customer_email': [f'customer{i}@example.com' if np.random.random() > 0.05 \n",
    "                          else np.random.choice(['invalid-email', '', None])\n",
    "                          for i in np.random.choice(range(100, 500), n_records)],\n",
    "        'order_status': np.random.choice(\n",
    "            ['Completed', 'completed', 'Pending', 'Cancelled', 'Shipped', None],\n",
    "            n_records, p=[0.4, 0.1, 0.2, 0.1, 0.15, 0.05]\n",
    "        ),\n",
    "        'customer_age': np.random.choice(\n",
    "            list(range(18, 80)) + [-1, 150, 999],  # Invalid ages\n",
    "            n_records, p=[0.97] + [0.01, 0.01, 0.01]\n",
    "        ),\n",
    "        'review_score': np.random.choice(\n",
    "            [1, 2, 3, 4, 5, -1, 10, None],  # Invalid scores\n",
    "            n_records, p=[0.05, 0.1, 0.15, 0.3, 0.35, 0.01, 0.01, 0.03]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add calculated fields with potential errors\n",
    "    df['total_before_discount'] = df['quantity'] * df['unit_price']\n",
    "    df['discount_amount'] = df['total_before_discount'] * df['discount_percent'] / 100\n",
    "    df['total_amount'] = df['total_before_discount'] - df['discount_amount'] + df['shipping_cost']\n",
    "    \n",
    "    # Introduce some duplicates\n",
    "    duplicate_indices = np.random.choice(df.index, size=50, replace=False)\n",
    "    duplicates = df.loc[duplicate_indices].copy()\n",
    "    df = pd.concat([df, duplicates], ignore_index=True)\n",
    "    \n",
    "    # Introduce some missing values\n",
    "    for col in ['quantity', 'unit_price', 'shipping_cost']:\n",
    "        missing_indices = np.random.choice(df.index, size=30, replace=False)\n",
    "        df.loc[missing_indices, col] = np.nan\n",
    "    \n",
    "    return df.sample(frac=1).reset_index(drop=True)  # Shuffle the data\n",
    "\n",
    "# Create the dataset\n",
    "raw_data = create_ecommerce_dataset(2000)\n",
    "print(f\"Dataset created with {len(raw_data)} records and {len(raw_data.columns)} columns\")\n",
    "print(f\"\\nData types:\")\n",
    "print(raw_data.dtypes)\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Quality Assessment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityAssessor:\n",
    "    \"\"\"\n",
    "    A comprehensive data quality assessment tool.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.report = {}\n",
    "    \n",
    "    def check_missing_values(self) -> Dict:\n",
    "        \"\"\"Check for missing values in the dataset.\"\"\"\n",
    "        missing_count = self.df.isnull().sum()\n",
    "        missing_percent = (missing_count / len(self.df)) * 100\n",
    "        \n",
    "        missing_report = pd.DataFrame({\n",
    "            'Missing_Count': missing_count,\n",
    "            'Missing_Percentage': missing_percent\n",
    "        })\n",
    "        \n",
    "        self.report['missing_values'] = missing_report[missing_report['Missing_Count'] > 0]\n",
    "        return self.report['missing_values']\n",
    "    \n",
    "    def check_duplicates(self) -> Dict:\n",
    "        \"\"\"Check for duplicate records.\"\"\"\n",
    "        duplicates = self.df.duplicated()\n",
    "        duplicate_count = duplicates.sum()\n",
    "        \n",
    "        self.report['duplicates'] = {\n",
    "            'total_duplicates': duplicate_count,\n",
    "            'duplicate_percentage': (duplicate_count / len(self.df)) * 100,\n",
    "            'duplicate_indices': self.df[duplicates].index.tolist()[:10]  # First 10\n",
    "        }\n",
    "        return self.report['duplicates']\n",
    "    \n",
    "    def check_data_types(self) -> Dict:\n",
    "        \"\"\"Check for potential data type issues.\"\"\"\n",
    "        type_issues = []\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'object':\n",
    "                # Check if column should be numeric\n",
    "                try:\n",
    "                    pd.to_numeric(self.df[col].dropna(), errors='raise')\n",
    "                    type_issues.append({\n",
    "                        'column': col,\n",
    "                        'current_type': str(self.df[col].dtype),\n",
    "                        'suggested_type': 'numeric'\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Check if column should be datetime\n",
    "                if 'date' in col.lower() or 'time' in col.lower():\n",
    "                    type_issues.append({\n",
    "                        'column': col,\n",
    "                        'current_type': str(self.df[col].dtype),\n",
    "                        'suggested_type': 'datetime'\n",
    "                    })\n",
    "        \n",
    "        self.report['type_issues'] = type_issues\n",
    "        return type_issues\n",
    "    \n",
    "    def check_outliers(self, columns: List[str] = None, method: str = 'iqr') -> Dict:\n",
    "        \"\"\"Check for outliers in numeric columns.\"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        outlier_report = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                data = self.df[col].dropna()\n",
    "                \n",
    "                if method == 'iqr':\n",
    "                    Q1 = data.quantile(0.25)\n",
    "                    Q3 = data.quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "                \n",
    "                elif method == 'zscore':\n",
    "                    z_scores = np.abs((data - data.mean()) / data.std())\n",
    "                    outliers = data[z_scores > 3]\n",
    "                \n",
    "                if len(outliers) > 0:\n",
    "                    outlier_report[col] = {\n",
    "                        'count': len(outliers),\n",
    "                        'percentage': (len(outliers) / len(data)) * 100,\n",
    "                        'min_outlier': outliers.min(),\n",
    "                        'max_outlier': outliers.max()\n",
    "                    }\n",
    "        \n",
    "        self.report['outliers'] = outlier_report\n",
    "        return outlier_report\n",
    "    \n",
    "    def check_data_consistency(self) -> Dict:\n",
    "        \"\"\"Check for data consistency issues.\"\"\"\n",
    "        consistency_issues = []\n",
    "        \n",
    "        # Check for inconsistent categorical values\n",
    "        for col in self.df.select_dtypes(include=['object']).columns:\n",
    "            unique_values = self.df[col].dropna().unique()\n",
    "            \n",
    "            # Check for case inconsistencies\n",
    "            lower_values = [str(v).lower() for v in unique_values]\n",
    "            if len(unique_values) != len(set(lower_values)):\n",
    "                consistency_issues.append({\n",
    "                    'column': col,\n",
    "                    'issue': 'case_inconsistency',\n",
    "                    'unique_values': unique_values.tolist()[:10]\n",
    "                })\n",
    "        \n",
    "        # Check for invalid values\n",
    "        if 'discount_percent' in self.df.columns:\n",
    "            invalid_discounts = self.df[\n",
    "                (self.df['discount_percent'] < 0) | \n",
    "                (self.df['discount_percent'] > 100)\n",
    "            ]\n",
    "            if len(invalid_discounts) > 0:\n",
    "                consistency_issues.append({\n",
    "                    'column': 'discount_percent',\n",
    "                    'issue': 'invalid_range',\n",
    "                    'count': len(invalid_discounts)\n",
    "                })\n",
    "        \n",
    "        self.report['consistency_issues'] = consistency_issues\n",
    "        return consistency_issues\n",
    "    \n",
    "    def generate_full_report(self) -> Dict:\n",
    "        \"\"\"Generate a comprehensive data quality report.\"\"\"\n",
    "        print(\"===== DATA QUALITY ASSESSMENT REPORT =====\")\n",
    "        print(f\"\\nDataset Shape: {self.df.shape}\")\n",
    "        \n",
    "        print(\"\\n1. MISSING VALUES:\")\n",
    "        missing = self.check_missing_values()\n",
    "        if not missing.empty:\n",
    "            print(missing)\n",
    "        else:\n",
    "            print(\"No missing values found.\")\n",
    "        \n",
    "        print(\"\\n2. DUPLICATES:\")\n",
    "        duplicates = self.check_duplicates()\n",
    "        print(f\"Total duplicates: {duplicates['total_duplicates']} ({duplicates['duplicate_percentage']:.2f}%)\")\n",
    "        \n",
    "        print(\"\\n3. DATA TYPE ISSUES:\")\n",
    "        type_issues = self.check_data_types()\n",
    "        if type_issues:\n",
    "            for issue in type_issues:\n",
    "                print(f\"  - {issue['column']}: {issue['current_type']} → {issue['suggested_type']}\")\n",
    "        else:\n",
    "            print(\"No data type issues found.\")\n",
    "        \n",
    "        print(\"\\n4. OUTLIERS:\")\n",
    "        outliers = self.check_outliers()\n",
    "        if outliers:\n",
    "            for col, stats in outliers.items():\n",
    "                print(f\"  - {col}: {stats['count']} outliers ({stats['percentage']:.2f}%)\")\n",
    "        else:\n",
    "            print(\"No outliers found.\")\n",
    "        \n",
    "        print(\"\\n5. CONSISTENCY ISSUES:\")\n",
    "        consistency = self.check_data_consistency()\n",
    "        if consistency:\n",
    "            for issue in consistency:\n",
    "                print(f\"  - {issue['column']}: {issue['issue']}\")\n",
    "        else:\n",
    "            print(\"No consistency issues found.\")\n",
    "        \n",
    "        return self.report\n",
    "\n",
    "# Run assessment\n",
    "assessor = DataQualityAssessor(raw_data)\n",
    "quality_report = assessor.generate_full_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building the Data Pipeline\n",
    "\n",
    "Now let's build a comprehensive data pipeline to clean and prepare our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    A comprehensive data preparation pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        self.verbose = verbose\n",
    "        self.steps_executed = []\n",
    "        self.original_shape = None\n",
    "        self.final_shape = None\n",
    "    \n",
    "    def _log(self, message: str):\n",
    "        \"\"\"Log message if verbose mode is enabled.\"\"\"\n",
    "        if self.verbose:\n",
    "            logger.info(message)\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Remove duplicate records.\"\"\"\n",
    "        self._log(\"Removing duplicates...\")\n",
    "        initial_count = len(df)\n",
    "        df_clean = df.drop_duplicates()\n",
    "        removed_count = initial_count - len(df_clean)\n",
    "        self._log(f\"  Removed {removed_count} duplicate records\")\n",
    "        self.steps_executed.append('remove_duplicates')\n",
    "        return df_clean\n",
    "    \n",
    "    def standardize_text_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Standardize text columns (case, whitespace, etc.).\"\"\"\n",
    "        self._log(\"Standardizing text columns...\")\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        text_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in text_columns:\n",
    "            # Strip whitespace and standardize case for specific columns\n",
    "            if col in ['category', 'payment_method', 'order_status']:\n",
    "                df_clean[col] = df_clean[col].str.strip().str.title()\n",
    "                self._log(f\"  Standardized {col}\")\n",
    "        \n",
    "        # Fix specific inconsistencies\n",
    "        if 'category' in df_clean.columns:\n",
    "            df_clean['category'] = df_clean['category'].replace({\n",
    "                'Electrnics': 'Electronics',\n",
    "                'Electronic': 'Electronics'\n",
    "            })\n",
    "        \n",
    "        self.steps_executed.append('standardize_text')\n",
    "        return df_clean\n",
    "    \n",
    "    def handle_missing_values(self, df: pd.DataFrame, strategy: Dict[str, str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values based on specified strategies.\"\"\"\n",
    "        self._log(\"Handling missing values...\")\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        if strategy is None:\n",
    "            strategy = {\n",
    "                'quantity': 'median',\n",
    "                'unit_price': 'median',\n",
    "                'shipping_cost': 'mean',\n",
    "                'payment_method': 'mode',\n",
    "                'shipping_address': 'drop',\n",
    "                'order_status': 'fill_pending',\n",
    "                'review_score': 'drop'\n",
    "            }\n",
    "        \n",
    "        for col, method in strategy.items():\n",
    "            if col in df_clean.columns:\n",
    "                missing_count = df_clean[col].isnull().sum()\n",
    "                \n",
    "                if missing_count > 0:\n",
    "                    if method == 'mean':\n",
    "                        df_clean[col].fillna(df_clean[col].mean(), inplace=True)\n",
    "                    elif method == 'median':\n",
    "                        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "                    elif method == 'mode':\n",
    "                        mode_value = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
    "                        df_clean[col].fillna(mode_value, inplace=True)\n",
    "                    elif method == 'fill_pending':\n",
    "                        df_clean[col].fillna('Pending', inplace=True)\n",
    "                    elif method == 'drop':\n",
    "                        df_clean = df_clean.dropna(subset=[col])\n",
    "                    \n",
    "                    self._log(f\"  {col}: {method} (handled {missing_count} missing values)\")\n",
    "        \n",
    "        self.steps_executed.append('handle_missing')\n",
    "        return df_clean\n",
    "    \n",
    "    def fix_data_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Convert columns to appropriate data types.\"\"\"\n",
    "        self._log(\"Fixing data types...\")\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Convert date columns\n",
    "        date_columns = [col for col in df_clean.columns if 'date' in col.lower()]\n",
    "        for col in date_columns:\n",
    "            if df_clean[col].dtype == 'object':\n",
    "                df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "                self._log(f\"  Converted {col} to datetime\")\n",
    "        \n",
    "        # Ensure numeric columns are float/int\n",
    "        numeric_columns = ['quantity', 'unit_price', 'discount_percent', \n",
    "                          'shipping_cost', 'customer_age', 'review_score']\n",
    "        for col in numeric_columns:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        \n",
    "        self.steps_executed.append('fix_data_types')\n",
    "        return df_clean\n",
    "    \n",
    "    def validate_and_clean_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Validate and clean invalid values.\"\"\"\n",
    "        self._log(\"Validating and cleaning values...\")\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Clean discount percentages\n",
    "        if 'discount_percent' in df_clean.columns:\n",
    "            invalid_discounts = (df_clean['discount_percent'] < 0) | (df_clean['discount_percent'] > 100)\n",
    "            df_clean.loc[invalid_discounts, 'discount_percent'] = df_clean.loc[~invalid_discounts, 'discount_percent'].median()\n",
    "            self._log(f\"  Fixed {invalid_discounts.sum()} invalid discount values\")\n",
    "        \n",
    "        # Clean customer age\n",
    "        if 'customer_age' in df_clean.columns:\n",
    "            invalid_ages = (df_clean['customer_age'] < 18) | (df_clean['customer_age'] > 120)\n",
    "            df_clean.loc[invalid_ages, 'customer_age'] = df_clean.loc[~invalid_ages, 'customer_age'].median()\n",
    "            self._log(f\"  Fixed {invalid_ages.sum()} invalid age values\")\n",
    "        \n",
    "        # Clean review scores\n",
    "        if 'review_score' in df_clean.columns:\n",
    "            invalid_scores = (df_clean['review_score'] < 1) | (df_clean['review_score'] > 5)\n",
    "            df_clean.loc[invalid_scores, 'review_score'] = np.nan\n",
    "            self._log(f\"  Set {invalid_scores.sum()} invalid review scores to NaN\")\n",
    "        \n",
    "        # Validate email addresses\n",
    "        if 'customer_email' in df_clean.columns:\n",
    "            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "            invalid_emails = ~df_clean['customer_email'].str.match(email_pattern, na=False)\n",
    "            df_clean.loc[invalid_emails, 'customer_email'] = 'invalid@example.com'\n",
    "            self._log(f\"  Fixed {invalid_emails.sum()} invalid email addresses\")\n",
    "        \n",
    "        self.steps_executed.append('validate_values')\n",
    "        return df_clean\n",
    "    \n",
    "    def create_derived_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create new features from existing data.\"\"\"\n",
    "        self._log(\"Creating derived features...\")\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Recalculate totals\n",
    "        if all(col in df_clean.columns for col in ['quantity', 'unit_price', 'discount_percent', 'shipping_cost']):\n",
    "            df_clean['total_before_discount'] = df_clean['quantity'] * df_clean['unit_price']\n",
    "            df_clean['discount_amount'] = df_clean['total_before_discount'] * df_clean['discount_percent'] / 100\n",
    "            df_clean['total_amount'] = df_clean['total_before_discount'] - df_clean['discount_amount'] + df_clean['shipping_cost']\n",
    "            self._log(\"  Recalculated order totals\")\n",
    "        \n",
    "        # Extract date components\n",
    "        if 'order_date' in df_clean.columns:\n",
    "            df_clean['order_year'] = df_clean['order_date'].dt.year\n",
    "            df_clean['order_month'] = df_clean['order_date'].dt.month\n",
    "            df_clean['order_day'] = df_clean['order_date'].dt.day\n",
    "            df_clean['order_dayofweek'] = df_clean['order_date'].dt.dayofweek\n",
    "            df_clean['order_quarter'] = df_clean['order_date'].dt.quarter\n",
    "            self._log(\"  Created date-based features\")\n",
    "        \n",
    "        # Create customer segments\n",
    "        if 'total_amount' in df_clean.columns:\n",
    "            df_clean['order_size'] = pd.cut(\n",
    "                df_clean['total_amount'],\n",
    "                bins=[0, 100, 500, 1000, float('inf')],\n",
    "                labels=['Small', 'Medium', 'Large', 'Extra Large']\n",
    "            )\n",
    "            self._log(\"  Created order size categories\")\n",
    "        \n",
    "        # Flag high-value customers\n",
    "        if 'customer_id' in df_clean.columns and 'total_amount' in df_clean.columns:\n",
    "            customer_totals = df_clean.groupby('customer_id')['total_amount'].transform('sum')\n",
    "            df_clean['is_high_value_customer'] = customer_totals > customer_totals.quantile(0.8)\n",
    "            self._log(\"  Flagged high-value customers\")\n",
    "        \n",
    "        self.steps_executed.append('create_features')\n",
    "        return df_clean\n",
    "    \n",
    "    def run_pipeline(self, df: pd.DataFrame, steps: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Run the complete pipeline.\"\"\"\n",
    "        self.original_shape = df.shape\n",
    "        self._log(f\"\\nStarting pipeline with {self.original_shape[0]} rows and {self.original_shape[1]} columns\")\n",
    "        self._log(\"=\"*60)\n",
    "        \n",
    "        if steps is None:\n",
    "            steps = [\n",
    "                'remove_duplicates',\n",
    "                'standardize_text',\n",
    "                'fix_data_types',\n",
    "                'validate_values',\n",
    "                'handle_missing',\n",
    "                'create_features'\n",
    "            ]\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        for step in steps:\n",
    "            if step == 'remove_duplicates':\n",
    "                df_processed = self.remove_duplicates(df_processed)\n",
    "            elif step == 'standardize_text':\n",
    "                df_processed = self.standardize_text_columns(df_processed)\n",
    "            elif step == 'fix_data_types':\n",
    "                df_processed = self.fix_data_types(df_processed)\n",
    "            elif step == 'validate_values':\n",
    "                df_processed = self.validate_and_clean_values(df_processed)\n",
    "            elif step == 'handle_missing':\n",
    "                df_processed = self.handle_missing_values(df_processed)\n",
    "            elif step == 'create_features':\n",
    "                df_processed = self.create_derived_features(df_processed)\n",
    "        \n",
    "        self.final_shape = df_processed.shape\n",
    "        self._log(\"=\"*60)\n",
    "        self._log(f\"Pipeline completed with {self.final_shape[0]} rows and {self.final_shape[1]} columns\")\n",
    "        self._log(f\"Rows removed: {self.original_shape[0] - self.final_shape[0]}\")\n",
    "        self._log(f\"Columns added: {self.final_shape[1] - self.original_shape[1]}\")\n",
    "        \n",
    "        return df_processed\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline = DataPipeline(verbose=True)\n",
    "cleaned_data = pipeline.run_pipeline(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Validate Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEFORE AND AFTER COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run quality assessment on cleaned data\n",
    "print(\"\\nQuality Assessment After Cleaning:\")\n",
    "assessor_after = DataQualityAssessor(cleaned_data)\n",
    "quality_report_after = assessor_after.generate_full_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the improvements\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Missing values comparison\n",
    "missing_before = raw_data.isnull().sum().sum()\n",
    "missing_after = cleaned_data.isnull().sum().sum()\n",
    "\n",
    "axes[0, 0].bar(['Before', 'After'], [missing_before, missing_after], color=['red', 'green'])\n",
    "axes[0, 0].set_title('Missing Values')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# 2. Duplicate records\n",
    "duplicates_before = raw_data.duplicated().sum()\n",
    "duplicates_after = cleaned_data.duplicated().sum()\n",
    "\n",
    "axes[0, 1].bar(['Before', 'After'], [duplicates_before, duplicates_after], color=['red', 'green'])\n",
    "axes[0, 1].set_title('Duplicate Records')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# 3. Dataset size\n",
    "axes[0, 2].bar(['Before', 'After'], \n",
    "               [len(raw_data), len(cleaned_data)], \n",
    "               color=['blue', 'navy'])\n",
    "axes[0, 2].set_title('Total Records')\n",
    "axes[0, 2].set_ylabel('Count')\n",
    "\n",
    "# 4. Category distribution\n",
    "if 'category' in raw_data.columns:\n",
    "    raw_categories = raw_data['category'].value_counts()\n",
    "    clean_categories = cleaned_data['category'].value_counts()\n",
    "    \n",
    "    axes[1, 0].bar(range(len(raw_categories)), raw_categories.values, alpha=0.5, label='Before')\n",
    "    axes[1, 0].bar(range(len(clean_categories)), clean_categories.values, alpha=0.5, label='After')\n",
    "    axes[1, 0].set_title('Category Distribution')\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "# 5. Discount percentage distribution\n",
    "if 'discount_percent' in cleaned_data.columns:\n",
    "    axes[1, 1].hist([raw_data['discount_percent'].dropna(), \n",
    "                     cleaned_data['discount_percent'].dropna()],\n",
    "                    bins=20, alpha=0.5, label=['Before', 'After'])\n",
    "    axes[1, 1].set_title('Discount Percentage Distribution')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "# 6. Feature count\n",
    "axes[1, 2].bar(['Before', 'After'], \n",
    "               [raw_data.shape[1], cleaned_data.shape[1]], \n",
    "               color=['orange', 'green'])\n",
    "axes[1, 2].set_title('Number of Features')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Pipeline Features\n",
    "\n",
    "### 5.1 Pipeline with Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurablePipeline:\n",
    "    \"\"\"\n",
    "    A pipeline that can be configured via JSON/dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.pipeline = DataPipeline(verbose=config.get('verbose', True))\n",
    "    \n",
    "    def run(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run pipeline based on configuration.\"\"\"\n",
    "        steps = self.config.get('steps', [])\n",
    "        return self.pipeline.run_pipeline(df, steps)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_path: str):\n",
    "        \"\"\"Load configuration from JSON file.\"\"\"\n",
    "        with open(json_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        return cls(config)\n",
    "\n",
    "# Example configuration\n",
    "pipeline_config = {\n",
    "    'name': 'ecommerce_cleaning_pipeline',\n",
    "    'version': '1.0.0',\n",
    "    'verbose': True,\n",
    "    'steps': [\n",
    "        'remove_duplicates',\n",
    "        'standardize_text',\n",
    "        'fix_data_types',\n",
    "        'validate_values',\n",
    "        'handle_missing',\n",
    "        'create_features'\n",
    "    ],\n",
    "    'missing_value_strategy': {\n",
    "        'quantity': 'median',\n",
    "        'unit_price': 'median',\n",
    "        'shipping_cost': 'mean'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run configurable pipeline\n",
    "config_pipeline = ConfigurablePipeline(pipeline_config)\n",
    "result = config_pipeline.run(raw_data.head(100))  # Test on subset\n",
    "print(f\"Configured pipeline result: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Pipeline with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Validates data against predefined rules.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rules: Dict):\n",
    "        self.rules = rules\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> Tuple[bool, Dict]:\n",
    "        \"\"\"Validate dataframe against rules.\"\"\"\n",
    "        all_valid = True\n",
    "        \n",
    "        for rule_name, rule_config in self.rules.items():\n",
    "            if rule_config['type'] == 'not_null':\n",
    "                column = rule_config['column']\n",
    "                is_valid = df[column].notna().all()\n",
    "                \n",
    "            elif rule_config['type'] == 'unique':\n",
    "                column = rule_config['column']\n",
    "                is_valid = df[column].nunique() == len(df)\n",
    "                \n",
    "            elif rule_config['type'] == 'range':\n",
    "                column = rule_config['column']\n",
    "                min_val = rule_config.get('min', float('-inf'))\n",
    "                max_val = rule_config.get('max', float('inf'))\n",
    "                is_valid = df[column].between(min_val, max_val).all()\n",
    "                \n",
    "            elif rule_config['type'] == 'in_list':\n",
    "                column = rule_config['column']\n",
    "                valid_values = rule_config['values']\n",
    "                is_valid = df[column].isin(valid_values).all()\n",
    "            \n",
    "            else:\n",
    "                is_valid = True\n",
    "            \n",
    "            self.validation_results[rule_name] = is_valid\n",
    "            if not is_valid:\n",
    "                all_valid = False\n",
    "        \n",
    "        return all_valid, self.validation_results\n",
    "\n",
    "# Define validation rules\n",
    "validation_rules = {\n",
    "    'order_id_unique': {\n",
    "        'type': 'unique',\n",
    "        'column': 'order_id'\n",
    "    },\n",
    "    'quantity_positive': {\n",
    "        'type': 'range',\n",
    "        'column': 'quantity',\n",
    "        'min': 1,\n",
    "        'max': 100\n",
    "    },\n",
    "    'discount_valid': {\n",
    "        'type': 'range',\n",
    "        'column': 'discount_percent',\n",
    "        'min': 0,\n",
    "        'max': 100\n",
    "    },\n",
    "    'category_valid': {\n",
    "        'type': 'in_list',\n",
    "        'column': 'category',\n",
    "        'values': ['Electronics', 'Accessories']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Validate cleaned data\n",
    "validator = DataValidator(validation_rules)\n",
    "is_valid, validation_results = validator.validate(cleaned_data)\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(\"=\"*40)\n",
    "for rule, passed in validation_results.items():\n",
    "    status = \"✓ PASSED\" if passed else \"✗ FAILED\"\n",
    "    print(f\"{rule}: {status}\")\n",
    "print(f\"\\nOverall validation: {'PASSED' if is_valid else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Exercise 1: Custom Pipeline Step\n",
    "\n",
    "Create a custom pipeline step that detects and handles outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function that:\n",
    "# 1. Detects outliers using the IQR method\n",
    "# 2. Offers options to: cap, remove, or transform outliers\n",
    "# 3. Logs the number of outliers handled\n",
    "\n",
    "def handle_outliers(df: pd.DataFrame, columns: List[str], method: str = 'cap') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle outliers in specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input dataframe\n",
    "    - columns: List of columns to check for outliers\n",
    "    - method: 'cap' (cap at bounds), 'remove' (remove rows), or 'transform' (log transform)\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "# test_data = cleaned_data.copy()\n",
    "# result = handle_outliers(test_data, ['total_amount', 'quantity'], method='cap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production-Ready Pipeline\n",
    "\n",
    "### 6.1 Error Handling and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPipeline:\n",
    "    \"\"\"\n",
    "    A production-ready pipeline with error handling and detailed logging.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_file: str = 'pipeline.log'):\n",
    "        self.setup_logging(log_file)\n",
    "        self.errors = []\n",
    "        self.warnings = []\n",
    "    \n",
    "    def setup_logging(self, log_file: str):\n",
    "        \"\"\"Set up file and console logging.\"\"\"\n",
    "        self.logger = logging.getLogger('RobustPipeline')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # File handler\n",
    "        fh = logging.FileHandler(log_file)\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        \n",
    "        # Console handler\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        \n",
    "        # Formatter\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        fh.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "        \n",
    "        self.logger.addHandler(fh)\n",
    "        self.logger.addHandler(ch)\n",
    "    \n",
    "    def safe_execute(self, func, *args, **kwargs):\n",
    "        \"\"\"Safely execute a function with error handling.\"\"\"\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self.logger.info(f\"Successfully executed: {func.__name__}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in {func.__name__}: {str(e)}\"\n",
    "            self.logger.error(error_msg)\n",
    "            self.errors.append(error_msg)\n",
    "            return args[0] if args else None  # Return original data on error\n",
    "    \n",
    "    def run(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Run the pipeline with comprehensive error handling.\"\"\"\n",
    "        self.logger.info(\"=\"*60)\n",
    "        self.logger.info(\"Starting Robust Pipeline\")\n",
    "        self.logger.info(f\"Input shape: {df.shape}\")\n",
    "        \n",
    "        # Create pipeline instance\n",
    "        pipeline = DataPipeline(verbose=False)\n",
    "        \n",
    "        # Execute each step safely\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        steps = [\n",
    "            (pipeline.remove_duplicates, 'Removing duplicates'),\n",
    "            (pipeline.standardize_text_columns, 'Standardizing text'),\n",
    "            (pipeline.fix_data_types, 'Fixing data types'),\n",
    "            (pipeline.validate_and_clean_values, 'Validating values'),\n",
    "            (pipeline.handle_missing_values, 'Handling missing values'),\n",
    "            (pipeline.create_derived_features, 'Creating features')\n",
    "        ]\n",
    "        \n",
    "        for step_func, step_name in steps:\n",
    "            self.logger.info(f\"Executing: {step_name}\")\n",
    "            df_processed = self.safe_execute(step_func, df_processed)\n",
    "            \n",
    "            # Validate after each step\n",
    "            if df_processed is None or df_processed.empty:\n",
    "                self.logger.error(f\"Pipeline failed at: {step_name}\")\n",
    "                break\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = {\n",
    "            'input_shape': df.shape,\n",
    "            'output_shape': df_processed.shape if df_processed is not None else None,\n",
    "            'errors': self.errors,\n",
    "            'warnings': self.warnings,\n",
    "            'success': len(self.errors) == 0\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"Pipeline completed. Success: {summary['success']}\")\n",
    "        self.logger.info(f\"Output shape: {summary['output_shape']}\")\n",
    "        self.logger.info(\"=\"*60)\n",
    "        \n",
    "        return df_processed, summary\n",
    "\n",
    "# Test robust pipeline\n",
    "robust_pipeline = RobustPipeline('pipeline_test.log')\n",
    "result_df, execution_summary = robust_pipeline.run(raw_data.head(100))\n",
    "\n",
    "print(\"\\nExecution Summary:\")\n",
    "print(f\"Success: {execution_summary['success']}\")\n",
    "print(f\"Input shape: {execution_summary['input_shape']}\")\n",
    "print(f\"Output shape: {execution_summary['output_shape']}\")\n",
    "print(f\"Errors: {len(execution_summary['errors'])}\")\n",
    "print(f\"Warnings: {len(execution_summary['warnings'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Pipeline Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timer_decorator(func):\n",
    "    \"\"\"Decorator to time function execution.\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {execution_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class MonitoredPipeline(DataPipeline):\n",
    "    \"\"\"\n",
    "    Pipeline with performance monitoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        super().__init__(verbose)\n",
    "        self.performance_metrics = {}\n",
    "    \n",
    "    @timer_decorator\n",
    "    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return super().remove_duplicates(df)\n",
    "    \n",
    "    @timer_decorator\n",
    "    def standardize_text_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return super().standardize_text_columns(df)\n",
    "    \n",
    "    @timer_decorator\n",
    "    def handle_missing_values(self, df: pd.DataFrame, strategy: Dict[str, str] = None) -> pd.DataFrame:\n",
    "        return super().handle_missing_values(df, strategy)\n",
    "\n",
    "# Test monitored pipeline\n",
    "monitored_pipeline = MonitoredPipeline(verbose=False)\n",
    "print(\"Performance Monitoring:\")\n",
    "print(\"=\"*40)\n",
    "result = monitored_pipeline.run_pipeline(raw_data.head(500), \n",
    "                                         ['remove_duplicates', 'standardize_text', 'handle_missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Exercise 2: End-to-End Pipeline\n",
    "\n",
    "Create a complete pipeline for a new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a pipeline that:\n",
    "# 1. Loads customer churn data\n",
    "# 2. Performs quality assessment\n",
    "# 3. Cleans and prepares the data\n",
    "# 4. Creates features for ML\n",
    "# 5. Validates the output\n",
    "# 6. Saves the processed data\n",
    "\n",
    "def create_churn_pipeline():\n",
    "    \"\"\"\n",
    "    Create a complete pipeline for customer churn analysis.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate sample churn data\n",
    "    # Your code here:\n",
    "    \n",
    "    # Step 2: Quality assessment\n",
    "    # Your code here:\n",
    "    \n",
    "    # Step 3: Data cleaning\n",
    "    # Your code here:\n",
    "    \n",
    "    # Step 4: Feature engineering\n",
    "    # Your code here:\n",
    "    \n",
    "    # Step 5: Validation\n",
    "    # Your code here:\n",
    "    \n",
    "    # Step 6: Save results\n",
    "    # Your code here:\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your pipeline\n",
    "# create_churn_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Pipeline Design Principles:**\n",
    "   - Modular and reusable components\n",
    "   - Clear separation of concerns\n",
    "   - Consistent error handling\n",
    "   - Comprehensive logging\n",
    "\n",
    "2. **Data Quality Steps:**\n",
    "   - Always start with assessment\n",
    "   - Handle duplicates early\n",
    "   - Standardize before processing\n",
    "   - Validate at multiple stages\n",
    "\n",
    "3. **Performance Considerations:**\n",
    "   - Monitor execution time\n",
    "   - Optimize data types\n",
    "   - Use vectorized operations\n",
    "   - Consider chunking for large datasets\n",
    "\n",
    "4. **Production Readiness:**\n",
    "   - Configuration management\n",
    "   - Error recovery mechanisms\n",
    "   - Detailed logging and monitoring\n",
    "   - Version control for pipelines\n",
    "\n",
    "### Best Practices Checklist:\n",
    "\n",
    "- [ ] Document all assumptions and decisions\n",
    "- [ ] Create unit tests for each pipeline step\n",
    "- [ ] Implement data validation rules\n",
    "- [ ] Set up monitoring and alerting\n",
    "- [ ] Version control your pipeline code\n",
    "- [ ] Create rollback mechanisms\n",
    "- [ ] Document data lineage\n",
    "- [ ] Implement incremental processing where possible\n",
    "- [ ] Create data quality dashboards\n",
    "- [ ] Establish SLAs for pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercise: Complete Pipeline Implementation\n",
    "\n",
    "Build a complete, production-ready pipeline for your own use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose one of the following scenarios and build a complete pipeline:\n",
    "\n",
    "# Scenario 1: Financial Transaction Pipeline\n",
    "# - Detect and flag suspicious transactions\n",
    "# - Handle currency conversions\n",
    "# - Create risk scores\n",
    "# - Generate compliance reports\n",
    "\n",
    "# Scenario 2: Healthcare Data Pipeline\n",
    "# - Anonymize patient information\n",
    "# - Standardize medical codes\n",
    "# - Handle missing lab results\n",
    "# - Create patient risk profiles\n",
    "\n",
    "# Scenario 3: Social Media Analytics Pipeline\n",
    "# - Clean and normalize text data\n",
    "# - Extract mentions and hashtags\n",
    "# - Calculate engagement metrics\n",
    "# - Identify trending topics\n",
    "\n",
    "# Your implementation here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "- **Building modular data pipelines** with reusable components\n",
    "- **Implementing data quality checks** at each stage\n",
    "- **Creating configurable pipelines** for different use cases\n",
    "- **Adding error handling and logging** for production readiness\n",
    "- **Monitoring pipeline performance** and optimization\n",
    "\n",
    "These skills are essential for:\n",
    "- Preparing data for machine learning models\n",
    "- Ensuring reproducible data processing\n",
    "- Building scalable data solutions\n",
    "- Maintaining data quality in production\n",
    "\n",
    "### Next Steps:\n",
    "1. Apply these concepts to your own datasets\n",
    "2. Explore pipeline orchestration tools (Airflow, Prefect)\n",
    "3. Learn about streaming data pipelines\n",
    "4. Study MLOps and model deployment pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}