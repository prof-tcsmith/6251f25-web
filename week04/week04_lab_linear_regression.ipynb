{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Lab: Linear Regression for Real Estate Price Prediction\n",
    "\n",
    "## Business Scenario\n",
    "\n",
    "You've been hired as a data scientist by **Sunshine Realty**, a real estate company operating in a growing metropolitan area. The company wants to:\n",
    "1. **Accurately price properties** for sale to remain competitive\n",
    "2. **Identify key factors** that drive property values\n",
    "3. **Predict future property values** for investment decisions\n",
    "\n",
    "Your task is to build a linear regression model that can predict house prices based on various features like size, location, age, and amenities.\n",
    "\n",
    "## Learning Objectives\n",
    "By completing this lab, you will:\n",
    "- Understand the ML perspective vs. statistical perspective on linear regression\n",
    "- Implement train/test/validation splits\n",
    "- Recognize and handle overfitting vs. underfitting\n",
    "- Apply feature engineering techniques\n",
    "- Evaluate models using multiple metrics\n",
    "- Use regularization to improve model performance\n",
    "- Create production-ready pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Generation\n",
    "\n",
    "First, let's import necessary libraries and generate synthetic real estate data that mimics real-world patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Real Estate Data\n",
    "\n",
    "We'll create realistic property data with the following features:\n",
    "- **Square footage**: Size of the property\n",
    "- **Bedrooms**: Number of bedrooms\n",
    "- **Bathrooms**: Number of bathrooms\n",
    "- **Age**: Age of the property in years\n",
    "- **Distance to city center**: In miles\n",
    "- **Crime rate**: Area crime index (0-100)\n",
    "- **School rating**: Local school quality (1-10)\n",
    "- **Has pool**: Binary feature\n",
    "- **Has garage**: Binary feature\n",
    "\n",
    "The price will be a function of these features with some non-linear relationships and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_estate_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic real estate data with realistic relationships.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate features\n",
    "    sqft = np.random.normal(2000, 600, n_samples)\n",
    "    sqft = np.clip(sqft, 500, 5000)  # Realistic bounds\n",
    "    \n",
    "    bedrooms = np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.1, 0.25, 0.35, 0.25, 0.05])\n",
    "    bathrooms = bedrooms + np.random.choice([-1, 0, 1], n_samples, p=[0.3, 0.5, 0.2])\n",
    "    bathrooms = np.clip(bathrooms, 1, 4)\n",
    "    \n",
    "    age = np.random.exponential(15, n_samples)\n",
    "    age = np.clip(age, 0, 50)\n",
    "    \n",
    "    distance_to_city = np.random.exponential(10, n_samples)\n",
    "    distance_to_city = np.clip(distance_to_city, 1, 40)\n",
    "    \n",
    "    crime_rate = np.random.beta(2, 5, n_samples) * 100  # Skewed toward lower crime\n",
    "    \n",
    "    school_rating = np.random.normal(7, 1.5, n_samples)\n",
    "    school_rating = np.clip(school_rating, 1, 10)\n",
    "    \n",
    "    has_pool = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "    has_garage = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n",
    "    \n",
    "    # Generate price with realistic relationships\n",
    "    price = (\n",
    "        150 * sqft  # Base price per sqft\n",
    "        + 20000 * bedrooms  # Value per bedroom\n",
    "        + 15000 * bathrooms  # Value per bathroom\n",
    "        - 2000 * age  # Depreciation\n",
    "        - 5000 * distance_to_city  # Distance penalty\n",
    "        - 1000 * crime_rate  # Crime penalty\n",
    "        + 10000 * school_rating  # School quality premium\n",
    "        + 25000 * has_pool  # Pool premium\n",
    "        + 15000 * has_garage  # Garage premium\n",
    "        + 50000  # Base price\n",
    "    )\n",
    "    \n",
    "    # Add non-linear effects\n",
    "    price += 0.5 * sqft * bedrooms  # Interaction: larger bedrooms in larger houses worth more\n",
    "    price *= (1 - 0.01 * age) ** 2  # Non-linear depreciation\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 30000, n_samples)\n",
    "    price = price + noise\n",
    "    price = np.clip(price, 50000, None)  # Minimum price\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'sqft': sqft,\n",
    "        'bedrooms': bedrooms,\n",
    "        'bathrooms': bathrooms,\n",
    "        'age': age,\n",
    "        'distance_to_city': distance_to_city,\n",
    "        'crime_rate': crime_rate,\n",
    "        'school_rating': school_rating,\n",
    "        'has_pool': has_pool,\n",
    "        'has_garage': has_garage,\n",
    "        'price': price\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_real_estate_data(1000)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis\n",
    "\n",
    "Before building models, let's understand our data through visualization and statistics.\n",
    "\n",
    "### Exercise 2.1: Basic Statistics\n",
    "**Task**: Display summary statistics and check for any data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display basic statistics about the dataset\n",
    "# Hint: Use describe() method and info() method\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "# YOUR CODE HERE: Display summary statistics\n",
    "______\n",
    "\n",
    "print(\"\\nData Types and Missing Values:\")\n",
    "# YOUR CODE HERE: Display data types and check for missing values\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Visualize Relationships\n",
    "**Task**: Create visualizations to understand the relationship between features and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots for key relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# TODO: Create scatter plots for continuous variables vs price\n",
    "# Plot 1: Square footage vs Price\n",
    "axes[0, 0].scatter(df['sqft'], df['price'], alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Square Footage')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].set_title('Price vs Square Footage')\n",
    "\n",
    "# YOUR CODE HERE: Create similar plots for age and distance_to_city\n",
    "# Plot 2: Age vs Price\n",
    "axes[0, 1].______(______)\n",
    "axes[0, 1].set_xlabel('______')\n",
    "axes[0, 1].set_ylabel('______')\n",
    "axes[0, 1].set_title('______')\n",
    "\n",
    "# Plot 3: Distance to City vs Price\n",
    "______\n",
    "\n",
    "# Plot 4: School Rating vs Price\n",
    "axes[1, 0].scatter(df['school_rating'], df['price'], alpha=0.5)\n",
    "axes[1, 0].set_xlabel('School Rating')\n",
    "axes[1, 0].set_ylabel('Price ($)')\n",
    "axes[1, 0].set_title('Price vs School Rating')\n",
    "\n",
    "# YOUR CODE HERE: Create box plots for categorical variables\n",
    "# Plot 5: Price by number of bedrooms\n",
    "df.boxplot(column='price', by='bedrooms', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Number of Bedrooms')\n",
    "axes[1, 1].set_ylabel('Price ($)')\n",
    "axes[1, 1].set_title('Price by Bedrooms')\n",
    "\n",
    "# Plot 6: Price by pool availability\n",
    "______\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Correlation Analysis\n",
    "**Task**: Analyze correlations between features and identify potential multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# YOUR CODE HERE: Calculate correlation matrix and create heatmap\n",
    "correlation_matrix = ______\n",
    "sns.heatmap(______, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features\n",
    "print(\"Highly correlated feature pairs (|correlation| > 0.7):\")\n",
    "# YOUR CODE HERE: Find and print highly correlated pairs\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Splitting - The ML Approach\n",
    "\n",
    "Following the machine learning approach, we'll split our data into training, validation, and test sets.\n",
    "\n",
    "### Why Split Data?\n",
    "- **Training set**: Used to fit the model\n",
    "- **Validation set**: Used for model selection and hyperparameter tuning\n",
    "- **Test set**: Used for final, unbiased evaluation\n",
    "\n",
    "### Exercise 3.1: Create Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# TODO: Create a 60/20/20 train/validation/test split\n",
    "# Step 1: First split into temp (80%) and test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=______, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split temp into train (75% of temp = 60% of total) and validation (25% of temp = 20% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=______, random_state=42\n",
    ")\n",
    "\n",
    "# Print the shapes\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Baseline Model - Simple Linear Regression\n",
    "\n",
    "Let's start with a simple linear regression model as our baseline.\n",
    "\n",
    "### Exercise 4.1: Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a simple linear regression model\n",
    "baseline_model = LinearRegression()\n",
    "\n",
    "# YOUR CODE HERE: Fit the model on training data\n",
    "______\n",
    "\n",
    "# Make predictions on all three sets\n",
    "y_pred_train = baseline_model.predict(X_train)\n",
    "y_pred_val = ______  # YOUR CODE HERE\n",
    "y_pred_test = ______  # YOUR CODE HERE\n",
    "\n",
    "print(\"Baseline model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Evaluate Baseline Model\n",
    "**Task**: Calculate R², RMSE, and MAE for all three sets to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, set_name):\n",
    "    \"\"\"\n",
    "    Calculate and display evaluation metrics.\n",
    "    \"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = ______  # YOUR CODE HERE: Calculate MAE\n",
    "    \n",
    "    print(f\"{set_name} Set:\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"  MAE: ${mae:,.2f}\")\n",
    "    return r2, rmse, mae\n",
    "\n",
    "# Evaluate on all sets\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(\"=\"*40)\n",
    "train_metrics = evaluate_model(y_train, y_pred_train, \"Training\")\n",
    "val_metrics = evaluate_model(______) # YOUR CODE HERE\n",
    "test_metrics = evaluate_model(______) # YOUR CODE HERE\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "print(f\"Train-Val R² Gap: {train_metrics[0] - val_metrics[0]:.4f}\")\n",
    "print(f\"Train-Test R² Gap: {train_metrics[0] - test_metrics[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Feature Engineering\n",
    "\n",
    "Let's improve our model by engineering new features.\n",
    "\n",
    "### Exercise 5.1: Create Polynomial and Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add engineered features\n",
    "def add_engineered_features(df):\n",
    "    \"\"\"\n",
    "    Add polynomial and interaction features.\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # Add polynomial features\n",
    "    df_eng['sqft_squared'] = df_eng['sqft'] ** 2\n",
    "    df_eng['age_squared'] = ______  # YOUR CODE HERE\n",
    "    \n",
    "    # Add interaction features\n",
    "    df_eng['sqft_bedrooms'] = df_eng['sqft'] * df_eng['bedrooms']\n",
    "    df_eng['sqft_bathrooms'] = ______  # YOUR CODE HERE\n",
    "    df_eng['age_distance'] = ______  # YOUR CODE HERE: age * distance_to_city\n",
    "    \n",
    "    # Add ratio features\n",
    "    df_eng['bath_bed_ratio'] = df_eng['bathrooms'] / (df_eng['bedrooms'] + 1)\n",
    "    df_eng['sqft_per_bedroom'] = ______  # YOUR CODE HERE\n",
    "    \n",
    "    # Add derived features\n",
    "    df_eng['luxury_score'] = (\n",
    "        df_eng['has_pool'] + \n",
    "        df_eng['has_garage'] + \n",
    "        (df_eng['bathrooms'] > 2).astype(int)\n",
    "    )\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Apply feature engineering\n",
    "X_train_eng = add_engineered_features(X_train)\n",
    "X_val_eng = add_engineered_features(X_val)\n",
    "X_test_eng = add_engineered_features(X_test)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Engineered features: {X_train_eng.shape[1]}\")\n",
    "print(f\"\\nNew features added:\")\n",
    "new_features = set(X_train_eng.columns) - set(X_train.columns)\n",
    "for feat in new_features:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Feature Scaling\n",
    "**Task**: Scale features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform all sets\n",
    "X_train_scaled = scaler.fit_transform(X_train_eng)\n",
    "X_val_scaled = ______  # YOUR CODE HERE: Only transform, don't fit!\n",
    "X_test_scaled = ______  # YOUR CODE HERE: Only transform, don't fit!\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"\\nExample of scaling effect on 'sqft':\")\n",
    "print(f\"  Original mean: {X_train_eng['sqft'].mean():.2f}\")\n",
    "print(f\"  Original std: {X_train_eng['sqft'].std():.2f}\")\n",
    "print(f\"  Scaled mean: {X_train_scaled[:, 0].mean():.2f}\")\n",
    "print(f\"  Scaled std: {X_train_scaled[:, 0].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Complexity and Overfitting\n",
    "\n",
    "Let's explore different levels of model complexity and see how they affect performance.\n",
    "\n",
    "### Exercise 6.1: Compare Different Polynomial Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different polynomial degrees\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_val_poly = ______  # YOUR CODE HERE: Transform validation set\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(______) # YOUR CODE HERE: Fit on polynomial training data\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = model.score(X_train_poly, y_train)\n",
    "    val_score = ______  # YOUR CODE HERE: Score on validation set\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "    \n",
    "    print(f\"Degree {degree}: Train R²={train_score:.4f}, Val R²={val_score:.4f}, \"\n",
    "          f\"Gap={train_score-val_score:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_scores, 'o-', label='Training R²', linewidth=2)\n",
    "plt.plot(degrees, val_scores, 'o-', label='Validation R²', linewidth=2)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Model Complexity vs Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Regularization\n",
    "\n",
    "To combat overfitting, let's apply regularization techniques.\n",
    "\n",
    "### Exercise 7.1: Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different alpha values for Ridge regression\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "ridge_train_scores = []\n",
    "ridge_val_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Train Ridge model\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(______) # YOUR CODE HERE: Fit on scaled training data\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = ridge.score(X_train_scaled, y_train)\n",
    "    val_score = ______  # YOUR CODE HERE\n",
    "    \n",
    "    ridge_train_scores.append(train_score)\n",
    "    ridge_val_scores.append(val_score)\n",
    "    \n",
    "    print(f\"Alpha={alpha:7.3f}: Train R²={train_score:.4f}, Val R²={val_score:.4f}\")\n",
    "\n",
    "# Find best alpha\n",
    "best_alpha_idx = np.argmax(ridge_val_scores)\n",
    "best_alpha = alphas[best_alpha_idx]\n",
    "print(f\"\\nBest alpha: {best_alpha} with validation R² = {ridge_val_scores[best_alpha_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Lasso Regression (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Lasso regression for feature selection\n",
    "lasso = Lasso(alpha=10)\n",
    "lasso.fit(______) # YOUR CODE HERE\n",
    "\n",
    "# Check which features were selected (non-zero coefficients)\n",
    "feature_names = X_train_eng.columns\n",
    "lasso_coef = lasso.coef_\n",
    "\n",
    "# Count non-zero coefficients\n",
    "n_selected = ______  # YOUR CODE HERE: Count non-zero coefficients\n",
    "print(f\"Lasso selected {n_selected} out of {len(feature_names)} features\")\n",
    "\n",
    "# Display selected features\n",
    "print(\"\\nSelected features (non-zero coefficients):\")\n",
    "selected_features = [(name, coef) for name, coef in zip(feature_names, lasso_coef) if coef != 0]\n",
    "selected_features.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "for name, coef in selected_features[:10]:  # Show top 10\n",
    "    print(f\"  {name:20s}: {coef:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Cross-Validation\n",
    "\n",
    "Let's use cross-validation for more robust model evaluation.\n",
    "\n",
    "### Exercise 8.1: K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform 5-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Combine train and validation for cross-validation\n",
    "X_train_val = np.vstack([X_train_scaled, X_val_scaled])\n",
    "y_train_val = np.concatenate([y_train, y_val])\n",
    "\n",
    "# Test different models with cross-validation\n",
    "models = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=best_alpha),\n",
    "    'Lasso': Lasso(alpha=10),\n",
    "    'ElasticNet': ElasticNet(alpha=10, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_val, y_train_val, \n",
    "        cv=______, # YOUR CODE HERE: Number of folds\n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    cv_results[name] = cv_scores\n",
    "    print(f\"{name:12s}: Mean R² = {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(cv_results.values(), labels=cv_results.keys())\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Cross-Validation Results Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Learning Curves\n",
    "\n",
    "Learning curves help us understand if our model would benefit from more data.\n",
    "\n",
    "### Exercise 9.1: Generate Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    Ridge(alpha=best_alpha), \n",
    "    X_train_val, \n",
    "    y_train_val,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = ______  # YOUR CODE HERE: Calculate mean across CV folds\n",
    "train_std = ______   # YOUR CODE HERE: Calculate std across CV folds\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label='Training Score', linewidth=2)\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.plot(train_sizes, val_mean, 'o-', label='Validation Score', linewidth=2)\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Learning Curves - Ridge Regression')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Interpret the curves\n",
    "gap = train_mean[-1] - val_mean[-1]\n",
    "print(f\"Final training score: {train_mean[-1]:.4f}\")\n",
    "print(f\"Final validation score: {val_mean[-1]:.4f}\")\n",
    "print(f\"Gap: {gap:.4f}\")\n",
    "\n",
    "if gap > 0.1:\n",
    "    print(\"\\nModel shows signs of overfitting. Consider:\")\n",
    "    print(\"- Increasing regularization\")\n",
    "    print(\"- Reducing model complexity\")\n",
    "    print(\"- Adding more training data\")\n",
    "elif val_mean[-1] < 0.7:\n",
    "    print(\"\\nModel shows signs of underfitting. Consider:\")\n",
    "    print(\"- Adding more features\")\n",
    "    print(\"- Increasing model complexity\")\n",
    "    print(\"- Reducing regularization\")\n",
    "else:\n",
    "    print(\"\\nModel appears well-fitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Production Pipeline\n",
    "\n",
    "Let's create a production-ready pipeline that combines all preprocessing and modeling steps.\n",
    "\n",
    "### Exercise 10.1: Build Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# TODO: Create a complete pipeline\n",
    "# Define numeric features\n",
    "numeric_features = X_train.columns.tolist()\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', ______),  # YOUR CODE HERE: Add polynomial features (degree=2)\n",
    "    ('regressor', ______)  # YOUR CODE HERE: Add Ridge regression with best alpha\n",
    "])\n",
    "\n",
    "# Fit pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on all sets\n",
    "print(\"Pipeline Performance:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Training R²: {pipeline.score(X_train, y_train):.4f}\")\n",
    "print(f\"Validation R²: {pipeline.score(X_val, y_val):.4f}\")\n",
    "print(f\"Test R²: {pipeline.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Final Model Evaluation and Business Insights\n",
    "\n",
    "### Exercise 11.1: Final Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions on test set\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate final metrics\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"FINAL MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\"*40)\n",
    "print(f\"R² Score: {test_r2:.4f}\")\n",
    "print(f\"RMSE: ${test_rmse:,.2f}\")\n",
    "print(f\"MAE: ${test_mae:,.2f}\")\n",
    "print(f\"\\nThis means:\")\n",
    "print(f\"- The model explains {test_r2*100:.1f}% of price variance\")\n",
    "print(f\"- Average prediction error is ${test_mae:,.0f}\")\n",
    "print(f\"- 68% of predictions are within ${test_rmse:,.0f} of actual price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2: Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze residuals\n",
    "residuals = ______  # YOUR CODE HERE: Calculate residuals (actual - predicted)\n",
    "\n",
    "# Create residual plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Residuals vs Predicted\n",
    "axes[0, 0].scatter(y_test_pred, residuals, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Predicted Price')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residuals vs Predicted Values')\n",
    "\n",
    "# Plot 2: Histogram of residuals\n",
    "axes[0, 1].hist(residuals, bins=30, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Residuals')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Residuals')\n",
    "\n",
    "# Plot 3: Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot')\n",
    "\n",
    "# Plot 4: Actual vs Predicted\n",
    "axes[1, 1].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('Actual Price')\n",
    "axes[1, 1].set_ylabel('Predicted Price')\n",
    "axes[1, 1].set_title('Actual vs Predicted Prices')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3: Feature Importance for Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the final model\n",
    "# Note: This is simplified - in production you'd need to account for scaling\n",
    "final_model = Ridge(alpha=best_alpha)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_eng.columns,\n",
    "    'coefficient': final_model.coef_,\n",
    "    'abs_coefficient': np.abs(final_model.coef_)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(\"=\"*50)\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"{row['feature']:20s}: {direction} price\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['abs_coefficient'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.title('Top 10 Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Business Recommendations\n",
    "\n",
    "Based on our analysis, let's provide actionable insights for Sunshine Realty.\n",
    "\n",
    "### Exercise 12.1: Generate Business Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BUSINESS INSIGHTS REPORT FOR SUNSHINE REALTY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. MODEL PERFORMANCE\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   • Accuracy: The model can predict house prices with an average\")\n",
    "print(f\"     error of ${test_mae:,.0f}, which is {test_mae/y_test.mean()*100:.1f}% of the average price.\")\n",
    "print(f\"   • Confidence: {test_r2*100:.1f}% of price variation is explained by the model.\")\n",
    "\n",
    "print(\"\\n2. KEY VALUE DRIVERS\")\n",
    "print(\"-\"*40)\n",
    "print(\"   Top factors that increase property value:\")\n",
    "for idx, row in feature_importance[feature_importance['coefficient'] > 0].head(5).iterrows():\n",
    "    print(f\"   • {row['feature']}\")\n",
    "\n",
    "print(\"\\n3. PRICING RECOMMENDATIONS\")\n",
    "print(\"-\"*40)\n",
    "print(\"   • Properties with pools command a premium\")\n",
    "print(\"   • School quality significantly impacts price\")\n",
    "print(\"   • Square footage remains the strongest predictor\")\n",
    "\n",
    "print(\"\\n4. INVESTMENT INSIGHTS\")\n",
    "print(\"-\"*40)\n",
    "# Calculate some specific insights\n",
    "pool_premium = final_model.coef_[list(X_train_eng.columns).index('has_pool')] * scaler.scale_[list(X_train_eng.columns).index('has_pool')]\n",
    "print(f\"   • Adding a pool could increase value by approximately ${abs(pool_premium):,.0f}\")\n",
    "print(f\"   • Properties closer to city center have higher values\")\n",
    "print(f\"   • Newer properties maintain value better\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS\")\n",
    "print(\"-\"*40)\n",
    "print(\"   • Deploy model for real-time price predictions\")\n",
    "print(\"   • Monitor model performance monthly\")\n",
    "print(\"   • Retrain quarterly with new sales data\")\n",
    "print(\"   • Consider adding neighborhood-specific features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully completed a comprehensive linear regression analysis for real estate price prediction. \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **ML Approach**: We focused on prediction accuracy rather than statistical inference\n",
    "2. **Data Splitting**: Used train/validation/test splits to avoid overfitting\n",
    "3. **Feature Engineering**: Created polynomial and interaction features to capture complex relationships\n",
    "4. **Regularization**: Applied Ridge and Lasso to prevent overfitting\n",
    "5. **Model Selection**: Used validation set and cross-validation for unbiased model selection\n",
    "6. **Production Pipeline**: Built a complete pipeline ready for deployment\n",
    "7. **Business Value**: Translated technical results into actionable business insights\n",
    "\n",
    "### Skills Practiced:\n",
    "- Data exploration and visualization\n",
    "- Feature engineering and scaling\n",
    "- Model training and evaluation\n",
    "- Overfitting detection and prevention\n",
    "- Cross-validation and hyperparameter tuning\n",
    "- Pipeline creation for production\n",
    "- Business communication of technical results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}