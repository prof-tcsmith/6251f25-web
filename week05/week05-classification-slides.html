<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 5: Classification & Logistic Regression - Complete</title>
    
    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/white.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/monokai.css">
    
    <!-- Common slide styles -->
    <link rel="stylesheet" href="../common-slides.css">
    
    <!-- Page-specific fixes for code blocks -->
    <style>
        /* Fix standalone code blocks (not in columns) */
        .reveal section > pre,
        .reveal section > div > pre {
            max-height: 500px !important;
            font-size: 0.52em !important;
        }
        
        /* Ensure code blocks in columns use full width */
        .reveal .columns .code-column {
            width: 100% !important;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section>
<h1>Classification &amp; Logistic Regression</h1>
<h3 style="border: none; text-align: center; color: #666;">From Continuous to Categorical Predictions</h3>
<p style="text-align: center; font-style: italic; color: #888;">Making Binary Decisions with Machine Learning</p>
<p style="text-align: center; margin-top: 50px;">
<strong>ISM6251 | Week 5</strong><br/>
                    Binary Classification ‚Ä¢ Evaluation Metrics ‚Ä¢ Logistic Regression ‚Ä¢ Business Applications
                </p>
</section>
<section>
<h2>Week 5: Classification - Topic Hierarchy</h2>
<!-- Learning Path at the top -->
<div style="padding: 12px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 8px; margin-bottom: 15px;">
<p style="font-size: 1em; text-align: center; margin: 0; color: white; font-weight: bold;">
                        Learning Path: Part 1: Introduction ‚Üí Part 2: Evaluation Metrics ‚Üí Part 3: Logistic Regression ‚Üí Part 4: Advanced Topics ‚Üí Part 5: Business Cases
                    </p>
</div>
<!-- Two-column ASCII tree layout -->
<div style="display: flex; gap: 20px; height: 450px;">
<!-- Left Column -->
<div style="flex: 1; background: #f8f9fa; padding: 30px 20px; border-radius: 10px; font-family: 'Courier New', monospace; font-size: 0.65em; line-height: 1.5; height: 100%;">
<pre style="margin: 0; color: #2c3e50; height: 430px !important; overflow-y: auto !important; max-height: none !important;">
<strong style="color: #e74c3c;">CLASSIFICATION: CATEGORICAL PREDICTION FUNDAMENTALS</strong>

<strong style="color: #3498db;">üìö Learning Objectives</strong>
‚îú‚îÄ‚îÄ Understand Classification vs Regression
‚îú‚îÄ‚îÄ Master Confusion Matrices
‚îú‚îÄ‚îÄ Choose Appropriate Metrics
‚îú‚îÄ‚îÄ Implement Logistic Regression
‚îú‚îÄ‚îÄ Handle Class Imbalance
‚îú‚îÄ‚îÄ Apply ROC/AUC Analysis
‚îî‚îÄ‚îÄ Solve Business Problems

<strong style="color: #9b59b6;">üéØ Part 1: Introduction to Classification</strong>
‚îú‚îÄ‚îÄ What is Classification?
‚îÇ   ‚îú‚îÄ‚îÄ Categorical Targets
‚îÇ   ‚îú‚îÄ‚îÄ Binary vs Multi-class
‚îÇ   ‚îú‚îÄ‚îÄ Probabilistic Outputs
‚îÇ   ‚îî‚îÄ‚îÄ Decision Boundaries
‚îú‚îÄ‚îÄ Real-World Applications
‚îÇ   ‚îú‚îÄ‚îÄ Medical Diagnosis
‚îÇ   ‚îú‚îÄ‚îÄ Fraud Detection
‚îÇ   ‚îú‚îÄ‚îÄ Customer Churn
‚îÇ   ‚îî‚îÄ‚îÄ Email Spam
‚îú‚îÄ‚îÄ Key Challenges
‚îÇ   ‚îú‚îÄ‚îÄ Class Imbalance
‚îÇ   ‚îú‚îÄ‚îÄ Threshold Selection
‚îÇ   ‚îú‚îÄ‚îÄ Cost Sensitivity
‚îÇ   ‚îî‚îÄ‚îÄ Interpretability
‚îî‚îÄ‚îÄ Evaluation Framework
    ‚îú‚îÄ‚îÄ Train/Test Split
    ‚îú‚îÄ‚îÄ Stratification
    ‚îú‚îÄ‚îÄ Cross-Validation
    ‚îî‚îÄ‚îÄ Performance Metrics

<strong style="color: #e67e22;">üìä Part 2: Evaluation Metrics</strong>
‚îú‚îÄ‚îÄ Confusion Matrix
‚îÇ   ‚îú‚îÄ‚îÄ True Positives (TP)
‚îÇ   ‚îú‚îÄ‚îÄ False Positives (FP)
‚îÇ   ‚îú‚îÄ‚îÄ True Negatives (TN)
‚îÇ   ‚îî‚îÄ‚îÄ False Negatives (FN)
‚îú‚îÄ‚îÄ Core Metrics
‚îÇ   ‚îú‚îÄ‚îÄ Accuracy
‚îÇ   ‚îú‚îÄ‚îÄ Precision
‚îÇ   ‚îú‚îÄ‚îÄ Recall (Sensitivity)
‚îÇ   ‚îî‚îÄ‚îÄ F1 Score
‚îú‚îÄ‚îÄ Choosing Metrics
‚îÇ   ‚îú‚îÄ‚îÄ When Recall Matters
‚îÇ   ‚îú‚îÄ‚îÄ When Precision Matters
‚îÇ   ‚îú‚îÄ‚îÄ Balanced Approaches
‚îÇ   ‚îî‚îÄ‚îÄ Business Context
‚îî‚îÄ‚îÄ Visual Analysis
    ‚îú‚îÄ‚îÄ Confusion Matrix Plots
    ‚îú‚îÄ‚îÄ Metric Comparisons
    ‚îî‚îÄ‚îÄ Trade-off Curves
                        </pre>
</div>
<!-- Right Column -->
<div style="flex: 1; background: #f8f9fa; padding: 30px 20px; border-radius: 10px; font-family: 'Courier New', monospace; font-size: 0.65em; line-height: 1.5; height: 100%;">
<pre style="margin: 0; color: #2c3e50; height: 430px !important; overflow-y: auto !important; max-height: none !important;">
<strong style="color: #16a085;">üîß Part 3: Logistic Regression</strong>
‚îú‚îÄ‚îÄ The Sigmoid Function
‚îÇ   ‚îú‚îÄ‚îÄ œÉ(z) = 1/(1+e^-z)
‚îÇ   ‚îú‚îÄ‚îÄ Maps to [0,1]
‚îÇ   ‚îú‚îÄ‚îÄ Probability Interpretation
‚îÇ   ‚îî‚îÄ‚îÄ Decision Boundary
‚îú‚îÄ‚îÄ Model Training
‚îÇ   ‚îú‚îÄ‚îÄ Maximum Likelihood
‚îÇ   ‚îú‚îÄ‚îÄ Log Loss Function
‚îÇ   ‚îú‚îÄ‚îÄ Gradient Descent
‚îÇ   ‚îî‚îÄ‚îÄ Regularization
‚îú‚îÄ‚îÄ Implementation
‚îÇ   ‚îú‚îÄ‚îÄ sklearn.LogisticRegression
‚îÇ   ‚îú‚îÄ‚îÄ Key Parameters
‚îÇ   ‚îú‚îÄ‚îÄ Class Weights
‚îÇ   ‚îî‚îÄ‚îÄ Solver Selection
‚îî‚îÄ‚îÄ Interpretation
    ‚îú‚îÄ‚îÄ Coefficients
    ‚îú‚îÄ‚îÄ Odds Ratios
    ‚îú‚îÄ‚îÄ Feature Importance
    ‚îî‚îÄ‚îÄ Probability Calibration

<strong style="color: #8e44ad;">üìà Part 4: Advanced Topics</strong>
‚îú‚îÄ‚îÄ Class Imbalance
‚îÇ   ‚îú‚îÄ‚îÄ Problem Overview
‚îÇ   ‚îú‚îÄ‚îÄ Resampling Methods
‚îÇ   ‚îú‚îÄ‚îÄ Class Weights
‚îÇ   ‚îî‚îÄ‚îÄ SMOTE
‚îú‚îÄ‚îÄ ROC Analysis
‚îÇ   ‚îú‚îÄ‚îÄ ROC Curves
‚îÇ   ‚îú‚îÄ‚îÄ AUC Score
‚îÇ   ‚îú‚îÄ‚îÄ Threshold Selection
‚îÇ   ‚îî‚îÄ‚îÄ Interpretation
‚îú‚îÄ‚îÄ Precision-Recall
‚îÇ   ‚îú‚îÄ‚îÄ PR Curves
‚îÇ   ‚îú‚îÄ‚îÄ Average Precision
‚îÇ   ‚îú‚îÄ‚îÄ Trade-offs
‚îÇ   ‚îî‚îÄ‚îÄ Imbalanced Data
‚îî‚îÄ‚îÄ Cost-Sensitive Learning
    ‚îú‚îÄ‚îÄ Cost Matrices
    ‚îú‚îÄ‚îÄ Business Costs
    ‚îú‚îÄ‚îÄ Optimization
    ‚îî‚îÄ‚îÄ Decision Making

<strong style="color: #27ae60;">üíº Part 5: Business Cases</strong>
‚îú‚îÄ‚îÄ Healthcare: Diabetes Screening
‚îÇ   ‚îú‚îÄ‚îÄ High Recall Priority
‚îÇ   ‚îú‚îÄ‚îÄ Cost of Missed Diagnosis
‚îÇ   ‚îî‚îÄ‚îÄ Early Intervention
‚îú‚îÄ‚îÄ E-Commerce: Churn Prediction
‚îÇ   ‚îú‚îÄ‚îÄ Balanced Approach
‚îÇ   ‚îú‚îÄ‚îÄ ROI Calculation
‚îÇ   ‚îî‚îÄ‚îÄ Retention Strategies
‚îî‚îÄ‚îÄ Finance: Fraud Detection
    ‚îú‚îÄ‚îÄ Extreme Imbalance
    ‚îú‚îÄ‚îÄ Real-time Scoring
    ‚îî‚îÄ‚îÄ Cost Optimization
                        </pre>
</div>
</div>
</section>
<section>
<!-- Title Slide -->
<section>
<h2>Part 1: Introduction to Classification</h2>
<h3>Moving from Continuous to Categorical Predictions</h3>
<div class="info-box">
<strong>Key Question:</strong><br/>
                        How do we predict discrete categories instead of continuous values?
                    </div>
<div class="columns mt-30">
<div class="column">
<h4>What We'll Cover</h4>
<ul>
<li>Regression vs Classification</li>
<li>Types of classification problems</li>
<li>Binary vs multi-class</li>
<li>Real-world applications</li>
<li>Business use cases</li>
</ul>
</div>
<div class="column">
<h4>Learning Outcomes</h4>
<ul>
<li>Distinguish classification from regression</li>
<li>Identify appropriate use cases</li>
<li>Understand problem formulation</li>
<li>Map business problems to ML tasks</li>
</ul>
</div>
</div>
</section>
<!-- Regression vs Classification -->
<section>
<h2>Regression vs Classification</h2>
<div class="columns">
<div class="column-50">
<h4>Regression</h4>
<div class="info-box">
<strong>Predicts continuous values:</strong><br/>
                                ‚Ä¢ Sales revenue: $125,432<br/>
                                ‚Ä¢ Temperature: 72.5¬∞F<br/>
                                ‚Ä¢ Stock price: $148.92<br/>
                                ‚Ä¢ Customer lifetime value: $5,234
                            </div>
<div class="warning-box mt-20">
<strong>Output:</strong> Any real number<br/>
<strong>Loss:</strong> Mean Squared Error<br/>
<strong>Example:</strong> Linear Regression
                            </div>
</div>
<div class="column-50">
<h4>Classification</h4>
<div class="success-box">
<strong>Predicts discrete categories:</strong><br/>
                                ‚Ä¢ Customer will churn: Yes/No<br/>
                                ‚Ä¢ Email is spam: Spam/Not Spam<br/>
                                ‚Ä¢ Transaction is fraud: Fraud/Legitimate<br/>
                                ‚Ä¢ Product quality: Good/Defective
                            </div>
<div class="info-box mt-20">
<strong>Output:</strong> Class label or probability<br/>
<strong>Loss:</strong> Cross-entropy<br/>
<strong>Example:</strong> Logistic Regression
                            </div>
</div>
</div>
<img alt="Classification vs Regression" src="../images/week05-class-imbalance.png" style="width: 60%; margin: 20px auto; display: block;"/>
</section>
<!-- Types of Classification -->
<section>
<h2>Types of Classification Problems</h2>
<div class="columns">
<div class="column-33">
<h4>Binary Classification</h4>
<div class="info-box">
<strong>Two Classes:</strong><br/>
                                ‚Ä¢ Yes/No<br/>
                                ‚Ä¢ True/False<br/>
                                ‚Ä¢ Positive/Negative
                            </div>
<ul class="small-text mt-10">
<li>Customer churns or stays</li>
<li>Loan defaults or repaid</li>
<li>Email spam or not spam</li>
<li>Product defective or good</li>
</ul>
</div>
<div class="column-33">
<h4>Multi-class Classification</h4>
<div class="warning-box">
<strong>Multiple Classes:</strong><br/>
                                ‚Ä¢ One of N categories<br/>
                                ‚Ä¢ Mutually exclusive
                            </div>
<ul class="small-text mt-10">
<li>Customer segment (A, B, C, D)</li>
<li>Product category</li>
<li>Risk level (Low, Med, High)</li>
<li>Image recognition</li>
</ul>
</div>
<div class="column-33">
<h4>Multi-label Classification</h4>
<div class="success-box">
<strong>Multiple Labels:</strong><br/>
                                ‚Ä¢ Multiple categories<br/>
                                ‚Ä¢ Not mutually exclusive
                            </div>
<ul class="small-text mt-10">
<li>Movie genres</li>
<li>Article topics</li>
<li>Medical diagnoses</li>
<li>Product attributes</li>
</ul>
</div>
</div>
</section>
<!-- Business Applications -->
<section>
<h2>Business Applications of Classification</h2>
<div class="columns">
<div class="column-33">
<div class="metric-card">
<h4>Marketing &amp; Sales</h4>
<ul style="text-align: left; font-size: 0.8em;">
<li>Lead scoring</li>
<li>Customer churn prediction</li>
<li>Cross-sell/up-sell targeting</li>
<li>Campaign response prediction</li>
<li>Customer segmentation</li>
</ul>
</div>
</div>
<div class="column-33">
<div class="metric-card">
<h4>Finance &amp; Risk</h4>
<ul style="text-align: left; font-size: 0.8em;">
<li>Credit approval</li>
<li>Fraud detection</li>
<li>Default prediction</li>
<li>Insurance claim classification</li>
<li>Trading signals</li>
</ul>
</div>
</div>
<div class="column-33">
<div class="metric-card">
<h4>Operations</h4>
<ul style="text-align: left; font-size: 0.8em;">
<li>Quality control</li>
<li>Equipment failure prediction</li>
<li>Inventory categorization</li>
<li>Supply chain risk</li>
<li>Anomaly detection</li>
</ul>
</div>
</div>
</div>
<div class="warning-box mt-20">
<strong>Key Business Consideration:</strong> The cost of different types of errors varies dramatically across applications. 
                        Missing a fraudulent transaction might cost thousands, while flagging a legitimate transaction as fraud causes customer frustration.
                    </div>
</section>
<!-- Why Classification is Challenging -->
<section>
<h2>Why Classification is Challenging</h2>
<div class="columns">
<div class="column-50">
<h4>Technical Challenges</h4>
<ul>
<li><strong>Class Imbalance:</strong>
<ul class="text-90 ml-20">
<li>Fraud: ~0.1% of transactions</li>
<li>Churn: ~5-10% of customers</li>
<li>Defects: ~1% of products</li>
</ul>
</li>
<li><strong>Feature Engineering:</strong>
<ul class="text-90 ml-20">
<li>Which features predict the outcome?</li>
<li>How to handle categorical variables?</li>
<li>Interaction effects</li>
</ul>
</li>
<li><strong>Model Selection:</strong>
<ul class="text-90 ml-20">
<li>Linear vs non-linear boundaries</li>
<li>Interpretability vs accuracy</li>
</ul>
</li>
</ul>
</div>
<div class="column-50">
<h4>Business Challenges</h4>
<ul>
<li><strong>Cost Asymmetry:</strong>
<ul class="text-90 ml-20">
<li>False positives vs false negatives</li>
<li>Different costs for different errors</li>
<li>Customer experience impact</li>
</ul>
</li>
<li><strong>Threshold Selection:</strong>
<ul class="text-90 ml-20">
<li>Default 0.5 rarely optimal</li>
<li>Business-driven thresholds</li>
<li>Trade-offs between metrics</li>
</ul>
</li>
<li><strong>Model Monitoring:</strong>
<ul class="text-90 ml-20">
<li>Concept drift over time</li>
<li>Changing class distributions</li>
<li>Performance degradation</li>
</ul>
</li>
</ul>
</div>
</div>
</section>
</section>
<section>
<!-- Title Slide -->
<section>
<h2>Part 2: Evaluation &amp; Metrics</h2>
<h3>Measuring and Understanding Classification Performance</h3>
<div class="info-box">
<strong>Key Question:</strong><br/>
How do we measure success in classification, and what metrics matter for different business contexts?
</div>
<div class="columns mt-30">
<div class="column">
<h4>What We'll Cover</h4>
<ul>
<li>Binary classification outputs</li>
<li>The confusion matrix</li>
<li>Core evaluation metrics</li>
<li>Precision-recall trade-offs</li>
<li>Business context examples</li>
<li>Threshold optimization</li>
</ul>
</div>
<div class="column">
<h4>Learning Outcomes</h4>
<ul>
<li>Build and interpret confusion matrices</li>
<li>Calculate key metrics</li>
<li>Choose appropriate metrics</li>
<li>Optimize for business objectives</li>
<li>Understand metric trade-offs</li>
</ul>
</div>
</div>
</section>
<!-- 1. Binary Classification Fundamentals -->
<section>
<h2>Binary Classification Outputs</h2>
<div class="columns">
<div class="column-50">
<h4>Two Forms of Predictions</h4>
<ul>
<li><strong>Class Labels:</strong> Direct prediction (0 or 1, Yes or No)</li>
<li><strong>Probabilities:</strong> Likelihood of positive class (0.0 to 1.0)</li>
</ul>
<div class="info-box mt-20">
<strong>Decision Threshold:</strong><br/>
Default threshold = 0.5<br/>
‚Ä¢ P(class=1) ‚â• 0.5 ‚Üí Predict 1<br/>
‚Ä¢ P(class=1) &lt; 0.5 ‚Üí Predict 0
</div>
<div class="warning-box mt-20">
<strong>Key Insight:</strong> The threshold is a business decision, not a statistical one!
</div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Binary classification predictions
from sklearn.linear_model import LogisticRegression

# Train model
model = LogisticRegression()
model.fit(X_train, y_train)

# Get class predictions (using default 0.5 threshold)
y_pred = model.predict(X_test)
# Output: array([0, 1, 1, 0, 1, ...])

# Get probability predictions
y_proba = model.predict_proba(X_test)
# Output: array([[0.82, 0.18],  # 82% class 0, 18% class 1
#                [0.31, 0.69],  # 31% class 0, 69% class 1
#                [0.15, 0.85],  # 15% class 0, 85% class 1
#                ...])

# Custom threshold based on business needs
threshold = 0.7  # More conservative
y_custom = (y_proba[:, 1] &gt;= threshold).astype(int)

# Different thresholds ‚Üí Different predictions
print(f"Default (0.5): {y_pred[:5]}")
print(f"Conservative (0.7): {y_custom[:5]}")</code></pre>
</div>
</div>
</section>
<!-- 2. The Confusion Matrix Foundation -->
<section>
<h2>The Confusion Matrix: Foundation of Evaluation</h2>
<div class="columns">
<div class="column-60">
<h4>Four Types of Outcomes</h4>
<table class="confusion-matrix">
<tr>
<th colspan="2" rowspan="2"></th>
<th colspan="2">Predicted</th>
</tr>
<tr>
<th>Negative (0)</th>
<th>Positive (1)</th>
</tr>
<tr>
<th rowspan="2">Actual</th>
<th>Negative (0)</th>
<td class="tn"><strong>True Negative (TN)</strong><br/>Correctly predicted negative</td>
<td class="fp"><strong>False Positive (FP)</strong><br/>Type I Error<br/>(False Alarm)</td>
</tr>
<tr>
<th>Positive (1)</th>
<td class="fn"><strong>False Negative (FN)</strong><br/>Type II Error<br/>(Missed Detection)</td>
<td class="tp"><strong>True Positive (TP)</strong><br/>Correctly predicted positive</td>
</tr>
</table>
</div>
<div class="column-40">
<h4>Why It Matters</h4>
<div class="success-box">
<strong>Correct Predictions:</strong><br/>
‚Ä¢ TN &amp; TP: Model got it right
</div>
<div class="danger-box mt-10">
<strong>Errors Have Different Costs:</strong><br/>
‚Ä¢ FP: False alarm costs<br/>
‚Ä¢ FN: Missed opportunity costs
</div>
<div class="info-box mt-10">
<strong>Key Point:</strong> The confusion matrix is the foundation for ALL classification metrics
</div>
</div>
</div>
</section>
<!-- 3. Core Metrics -->
<section>
<h2>Core Classification Metrics</h2>
<div class="columns">
<div class="column-50">
<div class="metric-card">
<h4>Accuracy</h4>
<div class="formula">Accuracy = (TP + TN) / Total</div>
<p class="small-text">Overall correctness</p>
<div class="warning-box" style="font-size: 0.7em;">
<strong>‚ö†Ô∏è Misleading when imbalanced!</strong><br/>
99% accuracy might mean predicting all negative
</div>
</div>
<div class="metric-card mt-20">
<h4>Precision</h4>
<div class="formula">Precision = TP / (TP + FP)</div>
<p class="small-text">Of predicted positives, how many correct?</p>
<div class="info-box" style="font-size: 0.7em;">
<strong>Focus:</strong> Minimize false alarms<br/>
<strong>Use when:</strong> FP cost is high
</div>
</div>
</div>
<div class="column-50">
<div class="metric-card">
<h4>Recall (Sensitivity, TPR)</h4>
<div class="formula">Recall = TP / (TP + FN)</div>
<p class="small-text">Of actual positives, how many caught?</p>
<div class="info-box" style="font-size: 0.7em;">
<strong>Focus:</strong> Don't miss positives<br/>
<strong>Use when:</strong> FN cost is high
</div>
</div>
<div class="metric-card mt-20">
<h4>F1 Score</h4>
<div class="formula">F1 = 2 √ó (P √ó R) / (P + R)</div>
<p class="small-text">Harmonic mean of precision and recall</p>
<div class="success-box" style="font-size: 0.7em;">
<strong>Use when:</strong> Need balance<br/>
<strong>Note:</strong> Assumes equal importance
</div>
</div>
</div>
</div>
<pre><code class="python">from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate all metrics
acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred)
rec = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}")</code></pre>
</section>
<!-- 4. Customer Churn Example -->
<section>
<h2>Example 1: Customer Churn Prediction</h2>
<div class="columns">
<div class="column-50">
<h4>Business Context</h4>
<table class="confusion-matrix">
<tr>
<th colspan="2" rowspan="2"></th>
<th colspan="2">Predicted</th>
</tr>
<tr>
<th>Will Stay</th>
<th>Will Churn</th>
</tr>
<tr>
<th rowspan="2">Actual</th>
<th>Stayed</th>
<td class="tn"><strong>850</strong><br/>No action needed</td>
<td class="fp"><strong>50</strong><br/>Unnecessary retention<br/>Cost: $5,000</td>
</tr>
<tr>
<th>Churned</th>
<td class="fn"><strong>30</strong><br/>Lost customers<br/>Cost: $30,000</td>
<td class="tp"><strong>70</strong><br/>Successfully retained<br/>Saved: $63,000</td>
</tr>
</table>
<div class="info-box mt-20">
<strong>Metrics:</strong><br/>
‚Ä¢ Accuracy: 92.0%<br/>
‚Ä¢ Precision: 58.3%<br/>
‚Ä¢ Recall: 70.0%<br/>
‚Ä¢ F1 Score: 63.6%
</div>
</div>
<div class="column-50">
<h4>Business Analysis</h4>
<pre><code class="python"># Cost-benefit analysis
tn, fp, fn, tp = 850, 50, 30, 70

# Define costs
retention_cost = 100    # Cost per retention offer
customer_value = 1000   # Lost revenue per churn

# Calculate impact
false_positive_cost = fp * retention_cost
false_negative_cost = fn * customer_value
saved_revenue = tp * (customer_value - retention_cost)

net_benefit = saved_revenue - false_positive_cost - false_negative_cost

print(f"FP Cost: ${false_positive_cost:,}")
print(f"FN Cost: ${false_negative_cost:,}")
print(f"Saved: ${saved_revenue:,}")
print(f"Net Benefit: ${net_benefit:,}")

# ROI calculation
roi = net_benefit / (false_positive_cost + tp * retention_cost)
print(f"ROI: {roi:.1%}")</code></pre>
<img alt="Churn Matrix" src="../images/week05-confusion-matrix.png" style="width: 54%; margin: 10px auto; display: block;"/>
</div>
</div>
</section>
<!-- 5. Medical Screening Example -->
<section>
<h2>Example 2: Medical Disease Screening</h2>
<div class="columns">
<div class="column-50">
<h4>High Stakes: Prioritize Recall</h4>
<table class="confusion-matrix" style="font-size: 0.85em;">
<tr>
<th colspan="2" rowspan="2"></th>
<th colspan="2">Test Result</th>
</tr>
<tr>
<th>Negative</th>
<th>Positive</th>
</tr>
<tr>
<th rowspan="2">Actual</th>
<th>Healthy</th>
<td class="tn"><strong>9,850</strong><br/>Correctly cleared</td>
<td class="fp"><strong>100</strong><br/>False alarm<br/>$200K follow-up</td>
</tr>
<tr>
<th>Disease</th>
<td class="fn"><strong>5</strong><br/>MISSED DISEASE<br/>Potential death</td>
<td class="tp"><strong>45</strong><br/>Caught early<br/>Lives saved</td>
</tr>
</table>
<div class="warning-box mt-10">
<strong>The Low Prevalence Challenge:</strong><br/>
‚Ä¢ Disease rate: 0.5% (50 of 10,000)<br/>
‚Ä¢ Recall: 90% (catches most cases)<br/>
‚Ä¢ Precision: 31% (many false alarms)<br/>
‚Ä¢ Trade-off: High recall means more false positives
</div>
</div>
<div class="column-50">
<h4>Standard Metrics Analysis</h4>
<pre><code class="python"># Medical screening metrics
tp, fp, fn, tn = 45, 100, 5, 9850
total = tp + fp + fn + tn

# Calculate standard metrics
accuracy = (tp + tn) / total
precision = tp / (tp + fp)  
recall = tp / (tp + fn)      
f1 = 2 * (precision * recall) / (precision + recall)

print(f"Accuracy: {accuracy:.1%}")
print(f"Precision: {precision:.1%}")  
print(f"Recall: {recall:.1%}")
print(f"F1 Score: {f1:.1%}")

print("\nInterpretation:")
print(f"‚Ä¢ {recall:.0%} of diseased patients detected")
print(f"‚Ä¢ Only {precision:.0%} of positive tests are correct")
print(f"‚Ä¢ {100-precision:.0%} of positive tests are false alarms")

# Cost-benefit analysis
false_alarm_cost = fp * 2000  # Follow-up testing
missed_disease_cost = fn * 500000  # Treatment/liability
early_detection_benefit = tp * 100000

net_benefit = early_detection_benefit - false_alarm_cost - missed_disease_cost
print(f"\nBusiness Impact:")
print(f"Net benefit: ${net_benefit:,.0f}")</code></pre>
<img alt="Medical Matrix" src="../images/week05-medical-screening-matrix.png" style="width: 48%; margin: 10px auto; display: block;"/>
</div>
</div>
</section>
<!-- 6. Spam Detection Example -->
<section>
<h2>Example 3: Email Spam Detection</h2>
<div class="columns">
<div class="column-50">
<h4>Business Priority: Precision</h4>
<table class="confusion-matrix" style="font-size: 0.85em;">
<tr>
<th colspan="2" rowspan="2"></th>
<th colspan="2">Predicted</th>
</tr>
<tr>
<th>Ham</th>
<th>Spam</th>
</tr>
<tr>
<th rowspan="2">Actual</th>
<th>Ham</th>
<td class="tn"><strong>8,500</strong><br/>Delivered correctly</td>
<td class="fp"><strong>20</strong><br/>BLOCKED EMAIL<br/>Angry customers</td>
</tr>
<tr>
<th>Spam</th>
<td class="fn"><strong>80</strong><br/>Spam in inbox<br/>Minor annoyance</td>
<td class="tp"><strong>1,400</strong><br/>Spam blocked<br/>Clean inbox</td>
</tr>
</table>
<div class="success-box mt-10">
<strong>Current Performance:</strong><br/>
‚Ä¢ Precision: 98.6% (critical!)<br/>
‚Ä¢ Recall: 94.6% (good enough)<br/>
‚Ä¢ F1 Score: 96.6%<br/>
‚Ä¢ Daily volume: 10,000 emails
</div>
</div>
<div class="column-50">
<h4>Threshold Impact Analysis</h4>
<pre><code class="python"># Compare different threshold strategies
import numpy as np

# Current (balanced) threshold = 0.5
current = {'threshold': 0.5, 'fp': 20, 'fn': 80}

# Conservative threshold = 0.8 (fewer FP)
conservative = {'threshold': 0.8, 'fp': 5, 'fn': 200}

# Aggressive threshold = 0.3 (fewer FN)
aggressive = {'threshold': 0.3, 'fp': 100, 'fn': 20}

# Business costs
cost_per_fp = 250  # Lost business opportunity
cost_per_fn = 2    # Time wasted on spam

for strategy in [current, conservative, aggressive]:
    total_cost = (strategy['fp'] * cost_per_fp + 
                  strategy['fn'] * cost_per_fn)
    print(f"Threshold {strategy['threshold']}:")
    print(f"  FP: {strategy['fp']}, FN: {strategy['fn']}")
    print(f"  Daily cost: ${total_cost:,}")
    print(f"  Annual cost: ${total_cost * 365:,}
")

# Conclusion: Conservative threshold is best!</code></pre>
<img alt="Spam Matrix" src="../images/week05-spam-detection-matrix.png" style="width: 48%; margin: 10px auto; display: block;"/>
</div>
</div>
</section>
<!-- 7. The Precision-Recall Trade-off -->
<section>
<h2>The Precision-Recall Trade-off</h2>
<div class="columns">
<div class="column-50">
<h4>Understanding the Relationship</h4>
<ul>
<li><strong>Inverse Relationship:</strong> As one goes up, the other typically goes down</li>
<li><strong>Threshold Control:</strong>
<ul class="text-90 ml-20">
<li>‚Üë Threshold ‚Üí ‚Üë Precision, ‚Üì Recall</li>
<li>‚Üì Threshold ‚Üí ‚Üì Precision, ‚Üë Recall</li>
</ul>
</li>
</ul>
<div class="info-box mt-20">
<strong>Business Translation:</strong><br/>
‚Ä¢ High Precision = "When we say yes, we're sure"<br/>
‚Ä¢ High Recall = "We catch most of the positives"
</div>
<div class="warning-box mt-20">
<strong>No Free Lunch:</strong> You can't maximize both simultaneously. Business context determines the right balance.
</div>
</div>
<div class="column-50">
<h4>Visualizing the Trade-off</h4>
<pre><code class="python"># Generate precision-recall curve
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(
    y_true, y_scores)

# Find optimal points for different objectives
idx_f1 = np.argmax(2 * precision * recall / 
                   (precision + recall))
idx_prec90 = np.where(precision &gt;= 0.9)[0][-1]
idx_rec90 = np.where(recall &gt;= 0.9)[0][0]

print(f"Max F1: P={precision[idx_f1]:.2f}, "
      f"R={recall[idx_f1]:.2f}, T={thresholds[idx_f1]:.2f}")
print(f"90% Precision: R={recall[idx_prec90]:.2f}, "
      f"T={thresholds[idx_prec90]:.2f}")
print(f"90% Recall: P={precision[idx_rec90]:.2f}, "
      f"T={thresholds[idx_rec90]:.2f}")</code></pre>
<img alt="PR Curve" src="../images/week05-precision-recall-curve.png" style="width: 90%; margin: 10px auto; display: block;"/>
</div>
</div>
</section>
<!-- 8. Comparing All Examples -->
<section>
<h2>Context Determines Metrics: A Comparison</h2>
<img alt="Comparison" src="../images/week05-confusion-matrix-comparison.png" style="width: 54%; margin: 20px auto; display: block;"/>
<div class="columns mt-20">
<div class="column-33">
<div class="info-box">
<h4>Customer Churn</h4>
<strong>Priority:</strong> Balance<br/>
<strong>Key Metric:</strong> F1 Score<br/>
<strong>Threshold:</strong> ~0.5<br/>
<strong>Why:</strong> Both errors costly
</div>
</div>
<div class="column-33">
<div class="warning-box">
<h4>Medical Screening</h4>
<strong>Priority:</strong> High Recall<br/>
<strong>Key Metric:</strong> Sensitivity<br/>
<strong>Threshold:</strong> ~0.2<br/>
<strong>Why:</strong> Can't miss disease
</div>
</div>
<div class="column-33">
<div class="success-box">
<h4>Spam Detection</h4>
<strong>Priority:</strong> High Precision<br/>
<strong>Key Metric:</strong> Precision<br/>
<strong>Threshold:</strong> ~0.8<br/>
<strong>Why:</strong> Can't block real email
</div>
</div>
</div>
<div class="danger-box mt-20">
<strong>Key Takeaway:</strong> There is no "best" metric or threshold. Success is defined by your business objectives and the relative costs of different types of errors.
</div>
</section>
<!-- 9. Threshold Optimization -->
<section>
<h2>Optimizing Decision Thresholds</h2>
<div class="columns">
<div class="column-50 code-column">
<pre><code class="python"># Threshold optimization based on business costs
def find_optimal_threshold(y_true, y_scores, 
                          cost_fp, cost_fn):
    """Find threshold that minimizes total cost."""
    
    thresholds = np.linspace(0, 1, 100)
    costs = []
    
    for threshold in thresholds:
        y_pred = (y_scores &gt;= threshold).astype(int)
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        total_cost = fp * cost_fp + fn * cost_fn
        costs.append(total_cost)
    
    optimal_idx = np.argmin(costs)
    optimal_threshold = thresholds[optimal_idx]
    min_cost = costs[optimal_idx]
    
    return optimal_threshold, min_cost

# Example: Fraud detection
# FP: Flag legitimate transaction ($25 customer service)
# FN: Miss fraud ($1,200 average loss)
optimal_t, cost = find_optimal_threshold(
    y_true, y_scores, cost_fp=25, cost_fn=1200)

print(f"Optimal threshold: {optimal_t:.3f}")
print(f"Expected daily cost: ${cost:,.0f}")
print(f"Annual savings vs default: ${(default_cost - cost) * 365:,.0f}")</code></pre>
</div>
<div class="column-50">
<h4>Threshold Selection Strategies</h4>
<div class="metric-card">
<h4>1. Cost-Based</h4>
<p class="small-text">Minimize total misclassification cost</p>
<div class="formula">Cost = FP √ó Cost_FP + FN √ó Cost_FN</div>
</div>
<div class="metric-card mt-10">
<h4>2. Metric-Based</h4>
<p class="small-text">Achieve target precision or recall</p>
<ul class="small-text">
<li>Medical: Recall ‚â• 95%</li>
<li>Spam: Precision ‚â• 99%</li>
</ul>
</div>
<div class="metric-card mt-10">
<h4>3. Operating Point</h4>
<p class="small-text">Balance at specific F1 or accuracy</p>
<div class="info-box" style="font-size: 0.7em;">
Often suboptimal for business!
</div>
</div>
<img alt="Threshold Optimization" src="../images/week05-threshold-optimization.png" style="width: 60%; margin: 10px auto; display: block;"/>
</div>
</div>
</section>
<!-- 10. Summary Slide -->
<section>
<h2>Key Takeaways: Evaluation &amp; Metrics</h2>
<div class="columns">
<div class="column-50">
<h4>Core Concepts</h4>
<ul>
<li><strong>Confusion Matrix:</strong> Foundation of all metrics</li>
<li><strong>Four Key Metrics:</strong>
<ul class="text-90 ml-20">
<li>Accuracy (often misleading)</li>
<li>Precision (minimize FP)</li>
<li>Recall (minimize FN)</li>
<li>F1 Score (balance)</li>
</ul>
</li>
<li><strong>Trade-offs:</strong> Can't maximize everything</li>
<li><strong>Thresholds:</strong> Business decision, not statistical</li>
</ul>
</div>
<div class="column-50">
<h4>Best Practices</h4>
<div class="success-box">
<ol>
<li>Start with confusion matrix</li>
<li>Understand your costs (FP vs FN)</li>
<li>Choose metrics aligned with business</li>
<li>Optimize threshold accordingly</li>
<li>Monitor performance over time</li>
</ol>
</div>
<div class="danger-box mt-20">
<strong>Remember:</strong> A 99% accurate model might be terrible for your business if it misses the 1% that matters most!
</div>
</div>
</div>
</section>
</section>
<section>
<section>
<h2>Part 3: Logistic Regression</h2>
<h3>The Workhorse of Binary Classification</h3>
<div class="columns">
<div class="column-50">
<h4>Why Not Linear Regression?</h4>
<ul>
<li>Linear regression predicts unbounded values</li>
<li>We need probabilities between 0 and 1</li>
<li>Linear regression assumes normal distribution of errors</li>
<li>Binary outcomes violate this assumption</li>
</ul>
<h4>The Logistic Function</h4>
<div class="info-box">
<strong>Sigmoid Function:</strong><br/>
<span class="font-mono">p = 1 / (1 + e^(-z))</span><br/>
                                where z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô
                            </div>
</div>
<div class="column-50 code-column">
<pre><code class="python">import numpy as np
import matplotlib.pyplot as plt

# Visualize sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-10, 10, 100)
p = sigmoid(z)

plt.figure(figsize=(10, 6))
plt.plot(z, p, 'b-', linewidth=2, label='Sigmoid')
plt.axhline(y=0.5, color='r', linestyle='--', 
            alpha=0.5, label='Decision Boundary')
plt.xlabel('z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô', fontsize=12)
plt.ylabel('Probability', fontsize=12)
plt.title('Logistic (Sigmoid) Function', fontsize=14)
plt.grid(True, alpha=0.3)
plt.legend()
plt.ylim(-0.05, 1.05)
plt.show()

# Properties
print("Key Properties:")
print(f"sigmoid(0) = {sigmoid(0):.3f}")
print(f"sigmoid(-‚àû) ‚Üí 0")
print(f"sigmoid(+‚àû) ‚Üí 1")</code></pre>
<img alt="Sigmoid Function" src="../images/week05-sigmoid-function.png" style="width: 105%; margin: 20px auto; display: block;"/>
</div>
</div>
</section>
<section>
<h2>Logistic Regression Interpretation</h2>
<div class="columns">
<div class="column-50">
<h4>Odds and Log-Odds</h4>
<div class="info-box">
<strong>Odds:</strong> p / (1 - p)<br/>
                                ‚Ä¢ Probability of success vs failure<br/>
                                ‚Ä¢ If p = 0.75, odds = 3:1
                            </div>
<div class="info-box">
<strong>Log-Odds (Logit):</strong> ln(p / (1 - p))<br/>
                                ‚Ä¢ Linear relationship with predictors<br/>
                                ‚Ä¢ log-odds = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ...
                            </div>
<h4>Coefficient Interpretation</h4>
<ul class="small-text">
<li>Œ≤ represents change in log-odds</li>
<li>e^Œ≤ represents odds ratio</li>
<li>For Œ≤ = 0.693: e^0.693 = 2 (doubles the odds)</li>
</ul>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Logistic Regression Example: Customer Churn
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Sample features
features = ['tenure', 'monthly_charges', 'total_charges', 
           'num_services', 'has_contract']

# Fit model
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train[features])

model = LogisticRegression()
model.fit(X_scaled, y_train)

# Examine coefficients
coef_df = pd.DataFrame({
    'Feature': features,
    'Coefficient': model.coef_[0],
    'Odds_Ratio': np.exp(model.coef_[0])
})

print(coef_df.to_string())
print(f"\nIntercept: {model.intercept_[0]:.3f}")</code></pre>
<div class="output-box" style="font-size: 0.5em; margin-top: 10px;">
<strong>Output:</strong>
<pre style="background: #f4f4f4; padding: 10px; border-radius: 4px; font-size: 0.9em;">           Feature  Coefficient  Odds_Ratio
0           tenure    -1.103114    0.331836
1  monthly_charges     0.406293    1.501242
2    total_charges     0.000349    1.000350
3     num_services     2.575832   13.142249
4     has_contract    -0.014544    0.985561

Intercept: -2.120

Interpretation:
‚Ä¢ tenure: Each month increases ‚Üí 67% lower odds of churn
‚Ä¢ num_services: Strong positive effect (13x odds)
‚Ä¢ monthly_charges: Higher charges ‚Üí 50% higher odds</pre>
</div>
</div>
</div>
</section>
<section>
<h2>Implementing Logistic Regression</h2>
<div class="columns">
<div class="column-50 code-column">
<h4>Model Training</h4>
<pre><code class="python">from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

# Prepare data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Scale features (important for logistic regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train model with regularization
model = LogisticRegression(
    penalty='l2',      # L2 regularization
    C=1.0,            # Inverse regularization strength
    max_iter=1000,    # Maximum iterations
    random_state=42
)

model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)

# Evaluate
print(classification_report(y_test, y_pred))</code></pre>
<div class="output-box" style="font-size: 0.5em; margin-top: 10px;">
<strong>Output:</strong>
<pre style="background: #f4f4f4; padding: 10px; border-radius: 4px; font-size: 0.9em;">              precision    recall  f1-score   support

    Stay (0)       0.94      0.91      0.93       138
   Churn (1)       0.82      0.87      0.84        62

    accuracy                           0.90       200
   macro avg       0.88      0.89      0.89       200
weighted avg       0.90      0.90      0.90       200</pre>
</div>
</div>
<div class="column-50 code-column">
<h4>Model Interpretation</h4>
<pre><code class="python"># Feature importance analysis
import matplotlib.pyplot as plt

# Get feature importance (absolute coefficients)
importance = np.abs(model.coef_[0])
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': importance
}).sort_values('importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['feature'][:10], 
         feature_importance['importance'][:10])
plt.xlabel('Absolute Coefficient Value')
plt.title('Top 10 Most Important Features')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Probability distribution
plt.figure(figsize=(10, 6))
plt.hist(y_proba[y_test==0, 1], bins=30, alpha=0.5, 
         label='Actual Negative', color='blue')
plt.hist(y_proba[y_test==1, 1], bins=30, alpha=0.5, 
         label='Actual Positive', color='red')
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities')
plt.legend()
plt.axvline(x=0.5, color='black', linestyle='--', 
           label='Default Threshold')
plt.show()</code></pre>
<img alt="Feature Importance" src="../images/week05-feature-importance.png" style="width: 70%; margin: 20px auto; display: block;"/>
<img alt="Probability Distributions" src="../images/week05-probability-distributions.png" style="width: 70%; margin: 20px auto; display: block;"/>
</div>
</div>
</section>
<section>
<h2>Understanding sklearn's LogisticRegression Parameters</h2>
<div class="columns">
<div class="column-50">
<h4>Key Parameters Overview</h4>
<div class="info-box">
<strong>Most Important Parameters:</strong>
<ul class="text-90">
<li><code>penalty</code>: Type of regularization</li>
<li><code>C</code>: Inverse regularization strength</li>
<li><code>solver</code>: Optimization algorithm</li>
<li><code>max_iter</code>: Maximum iterations</li>
<li><code>class_weight</code>: Handle imbalanced data</li>
</ul>
</div>
<div class="warning-box mt-20">
<strong>Common Pitfall:</strong><br/>
C is the INVERSE of regularization strength!<br/>
‚Ä¢ Smaller C = MORE regularization<br/>
‚Ä¢ Larger C = LESS regularization
</div>
<div class="success-box mt-20">
<strong>Best Practice:</strong> Always start with default parameters, then tune based on validation performance
</div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Essential LogisticRegression parameters
from sklearn.linear_model import LogisticRegression

# Basic usage with key parameters
model = LogisticRegression(
    # Regularization
    penalty='l2',           # 'l1', 'l2', 'elasticnet', None
    C=1.0,                  # Inverse regularization (0.001 to 1000)
    
    # Solver selection
    solver='lbfgs',         # 'lbfgs', 'liblinear', 'newton-cg', 
                           # 'sag', 'saga'
    
    # Convergence
    max_iter=100,          # Increase if not converging
    tol=1e-4,              # Tolerance for stopping
    
    # Class handling
    class_weight=None,      # None, 'balanced', or dict
    
    # Multiclass
    multi_class='auto',     # 'ovr', 'multinomial', 'auto'
    
    # Other
    random_state=42,        # For reproducibility
    n_jobs=-1,             # Parallel processing
    verbose=0              # Progress messages
)

# Example: Tuning for imbalanced data
model_balanced = LogisticRegression(
    penalty='l2',
    C=0.1,                 # More regularization
    class_weight='balanced',  # Auto-adjust for imbalance
    max_iter=500,          # More iterations for convergence
    solver='liblinear'     # Works well with L2 and small data
)</code></pre>
</div>
</div>
</section>
<section>
<h2>Regularization in Detail</h2>
<div class="columns">
<div class="column-50">
<h4>Understanding Regularization Types</h4>
<table class="styled-table">
<thead>
<tr>
<th>Penalty</th>
<th>Effect</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>L1 (Lasso)</strong></td>
<td>Sets coefficients to zero</td>
<td>Feature selection needed</td>
</tr>
<tr>
<td><strong>L2 (Ridge)</strong></td>
<td>Shrinks coefficients</td>
<td>Default choice, all features relevant</td>
</tr>
<tr>
<td><strong>ElasticNet</strong></td>
<td>Combination of L1 &amp; L2</td>
<td>Many correlated features</td>
</tr>
<tr>
<td><strong>None</strong></td>
<td>No regularization</td>
<td>Small datasets, no overfitting</td>
</tr>
</tbody>
</table>
<div class="info-box mt-20">
<strong>C Parameter Guidelines:</strong>
<ul class="text-90">
<li><strong>C = 0.001:</strong> Very strong regularization</li>
<li><strong>C = 0.01:</strong> Strong regularization</li>
<li><strong>C = 0.1:</strong> Moderate regularization</li>
<li><strong>C = 1.0:</strong> Default (light regularization)</li>
<li><strong>C = 10:</strong> Very light regularization</li>
<li><strong>C = 100+:</strong> Almost no regularization</li>
</ul>
</div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Comparing different regularization strengths
import numpy as np
from sklearn.model_selection import validation_curve

# Test different C values
C_values = [0.001, 0.01, 0.1, 1, 10, 100]
train_scores = []
val_scores = []

for C in C_values:
    model = LogisticRegression(penalty='l2', C=C, max_iter=1000)
    
    # Get cross-validation scores
    scores = cross_val_score(model, X_train, y_train, 
                           cv=5, scoring='roc_auc')
    val_scores.append(scores.mean())
    
    # Train score
    model.fit(X_train, y_train)
    train_scores.append(model.score(X_train, y_train))

# Plot validation curve
plt.figure(figsize=(10, 6))
plt.semilogx(C_values, train_scores, 'b-', label='Training score')
plt.semilogx(C_values, val_scores, 'r-', label='Validation score')
plt.xlabel('C (Inverse Regularization)', fontsize=12)
plt.ylabel('Score', fontsize=12)
plt.title('Effect of Regularization Strength', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Practical example: Feature selection with L1
model_l1 = LogisticRegression(penalty='l1', C=0.1, 
                              solver='liblinear')
model_l1.fit(X_train, y_train)

# Check which features were selected (non-zero coefficients)
selected_features = np.where(model_l1.coef_[0] != 0)[0]
print(f"L1 selected {len(selected_features)} of {X_train.shape[1]} features")</code></pre>
</div>
</div>
</section>
<section>
<h2>Choosing the Right Solver</h2>
<div class="columns">
<div class="column-50">
<h4>Solver Selection Guide</h4>
<table class="styled-table" style="font-size: 0.85em;">
<thead>
<tr>
<th>Solver</th>
<th>Best For</th>
<th>Supports</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>lbfgs</strong></td>
<td>Small datasets (default)</td>
<td>L2, None; Multinomial</td>
</tr>
<tr>
<td><strong>liblinear</strong></td>
<td>Small datasets, L1 penalty</td>
<td>L1, L2; OvR only</td>
</tr>
<tr>
<td><strong>newton-cg</strong></td>
<td>Multinomial loss</td>
<td>L2, None; Multinomial</td>
</tr>
<tr>
<td><strong>sag</strong></td>
<td>Large datasets</td>
<td>L2, None; Fast convergence</td>
</tr>
<tr>
<td><strong>saga</strong></td>
<td>Large datasets, L1</td>
<td>All penalties; All losses</td>
</tr>
</tbody>
</table>
<div class="warning-box mt-20">
<strong>Common Issues &amp; Solutions:</strong>
<ul class="text-90">
<li><strong>ConvergenceWarning:</strong> Increase max_iter</li>
<li><strong>Memory error:</strong> Use sag/saga solver</li>
<li><strong>L1 not supported:</strong> Switch to liblinear/saga</li>
</ul>
</div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Solver selection examples

# Example 1: Small dataset with L1 (feature selection)
model_small_l1 = LogisticRegression(
    penalty='l1',
    solver='liblinear',  # Only liblinear/saga support L1
    C=0.1
)

# Example 2: Large dataset (&gt;10K samples)
model_large = LogisticRegression(
    penalty='l2',
    solver='sag',        # Fast for large datasets
    max_iter=500,        # May need more iterations
    tol=1e-3            # Can relax tolerance for speed
)

# Example 3: Multinomial classification (3+ classes)
model_multi = LogisticRegression(
    multi_class='multinomial',
    solver='lbfgs',      # Supports multinomial
    max_iter=1000
)

# Example 4: Elastic Net (L1 + L2)
model_elastic = LogisticRegression(
    penalty='elasticnet',
    solver='saga',       # Only saga supports elastic net
    l1_ratio=0.5,       # Balance between L1 and L2
    max_iter=1000
)

# Performance comparison
import time

solvers = ['lbfgs', 'liblinear', 'sag', 'saga']
for solver in solvers:
    start = time.time()
    model = LogisticRegression(solver=solver, max_iter=100)
    model.fit(X_train, y_train)
    elapsed = time.time() - start
    score = model.score(X_test, y_test)
    print(f"{solver:10} Time: {elapsed:.3f}s Score: {score:.3f}")</code></pre>
</div>
</div>
</section>
<section>
<h2>Practical Tips for Using LogisticRegression</h2>
<div class="columns">
<div class="column-50">
<h4>Common Patterns &amp; Best Practices</h4>
<div class="success-box">
<strong>1. Always Scale Your Features!</strong>
<pre><code class="python">from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)</code></pre>
</div>
<div class="info-box mt-10">
<strong>2. Start Simple, Then Optimize</strong>
<ol class="text-90">
<li>Use default parameters first</li>
<li>Check for convergence warnings</li>
<li>Tune C using cross-validation</li>
<li>Consider L1 if many features</li>
</ol>
</div>
<div class="warning-box mt-10">
<strong>3. Watch for These Signs:</strong>
<ul class="text-90">
<li><strong>Overfitting:</strong> Decrease C (more regularization)</li>
<li><strong>Underfitting:</strong> Increase C (less regularization)</li>
<li><strong>Slow training:</strong> Try sag/saga solver</li>
<li><strong>Imbalanced data:</strong> Use class_weight='balanced'</li>
</ul>
</div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Complete practical workflow
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# 1. Create pipeline with scaling
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(random_state=42))
])

# 2. Define parameter grid
param_grid = {
    'classifier__C': [0.01, 0.1, 1, 10],
    'classifier__penalty': ['l1', 'l2'],
    'classifier__solver': ['liblinear'],  # Works with both L1 and L2
    'classifier__class_weight': [None, 'balanced']
}

# 3. Grid search with cross-validation
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

# 4. Fit and find best parameters
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best CV score:", grid_search.best_score_)

# 5. Evaluate on test set
best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)
print(f"Test score: {test_score:.3f}")

# 6. Interpretability check
if 'l1' in grid_search.best_params_['classifier__penalty']:
    coefficients = best_model.named_steps['classifier'].coef_[0]
    n_features_selected = np.sum(coefficients != 0)
    print(f"L1 selected {n_features_selected} features")</code></pre>
</div>
</div>
</section>
</section>
<section>
<section>
<h2>Part 4: Handling Class Imbalance</h2>
<h3>When Classes Are Not Equal</h3>
<div class="columns">
<div class="column-50">
<h4>The Imbalance Problem</h4>
<ul>
<li>Many business problems have imbalanced classes:
                                    <ul class="text-90 ml-20">
<li>Fraud detection: ~0.1% fraudulent</li>
<li>Customer churn: ~5-10% churn</li>
<li>Manufacturing defects: ~1% defective</li>
</ul>
</li>
<li>Model tends to predict majority class</li>
<li>High accuracy but poor minority class recall</li>
</ul>
<div class="warning-box">
<strong>Example:</strong> 99% accuracy in fraud detection might mean the model predicts "no fraud" for everything!
                            </div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Strategies for handling imbalance

# 1. Class weight adjustment
model_weighted = LogisticRegression(
    class_weight='balanced',  # Automatically adjust
    random_state=42
)

# 2. Custom class weights
model_custom = LogisticRegression(
    class_weight={0: 1, 1: 10},  # 10x weight for minority
    random_state=42
)

# 3. Resampling techniques
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# Oversampling minority class
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Undersampling majority class
rus = RandomUnderSampler(random_state=42)
X_under, y_under = rus.fit_resample(X_train, y_train)

# 4. Threshold adjustment
# Instead of 0.5, use optimal threshold
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_test, y_proba[:, 1])
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]</code></pre>
</div>
</div>
<img alt="Class Imbalance" src="../images/week05-class-imbalance.png" style="width: 60%; margin: 20px auto; display: block;"/>
</section>
<section>
<h2>Understanding ROC Curves: The Basics</h2>
<div class="columns">
<div class="column-50">
<h4>What is an ROC Curve?</h4>
<div class="info-box">
<strong>ROC = Receiver Operating Characteristic</strong><br/>
A graph showing how well a binary classifier performs at ALL possible thresholds
</div>
<ul>
<li><strong>X-axis (FPR):</strong> False Positive Rate
<ul class="text-90 ml-20">
<li>FPR = FP / (FP + TN)</li>
<li>"What % of negatives did we incorrectly call positive?"</li>
<li>Also called Fall-out or (1 - Specificity)</li>
</ul>
</li>
<li><strong>Y-axis (TPR):</strong> True Positive Rate
<ul class="text-90 ml-20">
<li>TPR = TP / (TP + FN)</li>
<li>"What % of positives did we correctly identify?"</li>
<li>Also called Sensitivity or Recall</li>
</ul>
</li>
</ul>
<div class="warning-box mt-20">
<strong>Key Insight:</strong> Each point on the ROC curve represents a different threshold setting
</div>
</div>
<div class="column-50">
<h4>How to Read an ROC Curve</h4>
<img alt="ROC Curve" src="../images/week05-roc-curve.png" style="width: 100%; margin: 10px auto; display: block;"/>
<div class="success-box">
<strong>Important Points on the Curve:</strong>
<ul class="text-90">
<li><strong>(0,0):</strong> Threshold = 1.0 (predict all negative)</li>
<li><strong>(1,1):</strong> Threshold = 0.0 (predict all positive)</li>
<li><strong>(0,1):</strong> Perfect classifier (the dream!)</li>
<li><strong>Diagonal:</strong> Random guessing (coin flip)</li>
<li><strong>Optimal Point (red dot):</strong> Best balance<br/>
    ¬†¬†‚Ä¢ Threshold = 0.333<br/>
    ¬†¬†‚Ä¢ TPR = 0.87, FPR = 0.13<br/>
    ¬†¬†‚Ä¢ Maximizes TPR - FPR (Youden Index)</li>
</ul>
</div>
<div class="info-box mt-10">
<strong>Better Models:</strong> Curve closer to top-left corner<br/>
<strong>Worse Models:</strong> Curve closer to diagonal line
</div>
</div>
</div>
</section>
<section>
<h2>Understanding How Thresholds Build the ROC Curve</h2>
<div class="columns">
<div class="column-40">
<h4>Each Point = Different Threshold</h4>
<div class="info-box">
<strong>Key Insight:</strong> The ROC curve is created by calculating TPR and FPR at every possible threshold from 0 to 1
</div>
<ul class="text-90">
<li><strong>High threshold (0.9):</strong>
<ul class="ml-20">
<li>Very conservative</li>
<li>Few positive predictions</li>
<li>Low TPR, Very low FPR</li>
</ul>
</li>
<li><strong>Medium threshold (0.5):</strong>
<ul class="ml-20">
<li>Balanced approach</li>
<li>Standard default</li>
<li>Moderate TPR and FPR</li>
</ul>
</li>
<li><strong>Low threshold (0.1):</strong>
<ul class="ml-20">
<li>Very aggressive</li>
<li>Many positive predictions</li>
<li>High TPR, High FPR</li>
</ul>
</li>
</ul>
<div class="warning-box mt-20">
<strong>Remember:</strong> Moving along the ROC curve = changing the threshold
</div>
</div>
<div class="column-60">
<h4>Visualizing Threshold Effects</h4>
<img alt="ROC Thresholds" src="../images/week05-roc-thresholds-explained.png" style="width: 100%; margin: 10px auto; display: block;"/>
<div class="success-box" style="font-size: 0.85em;">
<strong>How to Read This:</strong>
<ul class="text-90">
<li>Left plot shows where each threshold falls on ROC curve</li>
<li>Right plot shows how many positive/negative predictions at each threshold</li>
<li>As threshold ‚Üì, positive predictions ‚Üë, both TPR and FPR ‚Üë</li>
</ul>
</div>
</div>
</div>
</section>
<section>
<h2>AUC: Area Under the ROC Curve</h2>
<div class="columns">
<div class="column-50">
<h4>What Does AUC Mean?</h4>
<div class="info-box">
<strong>AUC = Area Under the Curve</strong><br/>
Single number summarizing classifier performance across all thresholds
</div>
<h4>Intuitive Interpretation</h4>
<div class="success-box">
<strong>AUC = Probability that the model ranks a random positive example higher than a random negative example</strong>
</div>
<p class="text-90">Example: AUC = 0.85 means:</p>
<ul class="text-90">
<li>If you pick one positive and one negative case randomly</li>
<li>85% chance the model gives higher probability to the positive</li>
<li>This works regardless of threshold!</li>
</ul>
<h4>AUC Score Interpretation</h4>
<table class="styled-table">
<thead>
<tr><th>AUC Range</th><th>Performance</th><th>Real-World Meaning</th></tr>
</thead>
<tbody>
<tr><td>0.5</td><td>No discrimination</td><td>Random guessing</td></tr>
<tr><td>0.5-0.6</td><td>Poor</td><td>Barely better than random</td></tr>
<tr><td>0.6-0.7</td><td>Fair</td><td>Some predictive value</td></tr>
<tr><td>0.7-0.8</td><td>Acceptable</td><td>Good for many applications</td></tr>
<tr><td>0.8-0.9</td><td>Excellent</td><td>Strong predictive power</td></tr>
<tr><td>0.9-1.0</td><td>Outstanding</td><td>Near perfect (check for leakage!)</td></tr>
</tbody>
</table>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Complete ROC-AUC Example
from sklearn.metrics import roc_curve, roc_auc_score
import numpy as np
import matplotlib.pyplot as plt

# Get predictions from your model
y_scores = model.predict_proba(X_test)[:, 1]

# Calculate ROC curve points
fpr, tpr, thresholds = roc_curve(y_test, y_scores)

# Calculate AUC
auc_score = roc_auc_score(y_test, y_scores)

# Create comprehensive ROC plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Left plot: ROC Curve
ax1.plot(fpr, tpr, 'b-', linewidth=2, 
         label=f'Model (AUC = {auc_score:.3f})')
ax1.plot([0, 1], [0, 1], 'r--', 
         label='Random (AUC = 0.500)')
ax1.fill_between(fpr, tpr, alpha=0.2)  # Shade AUC
ax1.set_xlabel('False Positive Rate', fontsize=12)
ax1.set_ylabel('True Positive Rate', fontsize=12)
ax1.set_title('ROC Curve', fontsize=14)
ax1.legend(loc='lower right')
ax1.grid(True, alpha=0.3)

# Right plot: Threshold Analysis
ax2.plot(thresholds[:-1], tpr[:-1], 'g-', label='TPR (Sensitivity)')
ax2.plot(thresholds[:-1], fpr[:-1], 'r-', label='FPR (1-Specificity)')
ax2.plot(thresholds[:-1], tpr[:-1] - fpr[:-1], 'b-', 
         label='TPR - FPR (Youden Index)')
ax2.set_xlabel('Threshold', fontsize=12)
ax2.set_ylabel('Rate', fontsize=12)
ax2.set_title('Performance vs Threshold', fontsize=14)
ax2.legend(loc='best')
ax2.grid(True, alpha=0.3)
ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)

plt.tight_layout()
plt.show()

# Find optimal threshold using Youden Index
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]
print(f"Optimal Threshold: {optimal_threshold:.3f}")
print(f"At this threshold: TPR={tpr[optimal_idx]:.3f}, FPR={fpr[optimal_idx]:.3f}")</code></pre>
<div class="output-box" style="font-size: 0.5em; margin-top: 10px;">
<strong>Output:</strong>
<pre style="background: #f4f4f4; padding: 10px; border-radius: 4px; font-size: 0.9em;">Optimal Threshold: 0.333
At this threshold: TPR=0.874, FPR=0.125</pre>
</div>
<img alt="ROC with AUC" src="../images/week05-auc-shaded.png" style="width: 90%; margin: 20px auto; display: block;"/>
</div>
</div>
</section>
<section>
<h2>ROC-AUC: Practical Applications &amp; Limitations</h2>
<div class="columns">
<div class="column-50">
<h4>When to Use ROC-AUC</h4>
<div class="success-box">
<strong>Best For:</strong>
<ul class="text-90">
<li>Comparing models (higher AUC = better)</li>
<li>Threshold-independent evaluation</li>
<li>Balanced or moderately imbalanced data</li>
<li>When both FP and FN matter equally</li>
</ul>
</div>
<h4>Real-World Examples</h4>
<ul>
<li><strong>Credit Scoring (AUC ~0.75-0.85):</strong>
<ul class="text-90 ml-20">
<li>Need to balance approval rates with default risk</li>
<li>Both false positives and negatives costly</li>
</ul>
</li>
<li><strong>Medical Diagnosis (AUC ~0.80-0.95):</strong>
<ul class="text-90 ml-20">
<li>High AUC critical for serious conditions</li>
<li>Can adjust threshold based on consequences</li>
</ul>
</li>
<li><strong>Email Spam (AUC ~0.95-0.99):</strong>
<ul class="text-90 ml-20">
<li>Very high AUC achievable</li>
<li>Clear separation between classes</li>
</ul>
</li>
</ul>
<div class="warning-box mt-20">
<strong>Limitations of ROC-AUC:</strong>
<ul class="text-90">
<li>Can be misleading with extreme imbalance</li>
<li>Doesn't reflect actual business costs</li>
<li>High AUC ‚â† Good precision at your threshold</li>
</ul>
</div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Comparing Multiple Models with ROC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

# Train multiple models
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'Naive Bayes': GaussianNB()
}

plt.figure(figsize=(10, 8))

# Plot ROC curve for each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_scores = model.predict_proba(X_test)[:, 1]
    
    fpr, tpr, _ = roc_curve(y_test, y_scores)
    auc_score = roc_auc_score(y_test, y_scores)
    
    plt.plot(fpr, tpr, linewidth=2, 
             label=f'{name} (AUC = {auc_score:.3f})')

# Add random classifier line
plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.500)')

# Formatting
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('Model Comparison: ROC Curves', fontsize=14)
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.show()

# Alternative: Precision-Recall AUC for imbalanced data
from sklearn.metrics import average_precision_score

for name, model in models.items():
    model.fit(X_train, y_train)
    y_scores = model.predict_proba(X_test)[:, 1]
    
    pr_auc = average_precision_score(y_test, y_scores)
    roc_auc = roc_auc_score(y_test, y_scores)
    
    print(f"{name:20} ROC-AUC: {roc_auc:.3f}, PR-AUC: {pr_auc:.3f}")

# Note: PR-AUC often better for imbalanced datasets!</code></pre>
<div class="output-box" style="font-size: 0.5em; margin-top: 10px;">
<strong>Output:</strong>
<pre style="background: #f4f4f4; padding: 10px; border-radius: 4px; font-size: 0.9em;">Logistic Regression  ROC-AUC: 0.847, PR-AUC: 0.712
Random Forest        ROC-AUC: 0.923, PR-AUC: 0.856
Naive Bayes          ROC-AUC: 0.798, PR-AUC: 0.651</pre>
</div>
<img alt="Model Comparison" src="../images/week05-confusion-matrix-comparison.png" style="width: 80%; margin: 20px auto; display: block;"/>
</div>
</div>
</section>
<section>
<h2>Choosing Thresholds from ROC Curves</h2>
<div class="columns">
<div class="column-50">
<h4>Methods for Threshold Selection</h4>
<div class="info-box">
<strong>1. Youden's Index (J-Statistic)</strong>
<div class="formula">J = Sensitivity + Specificity - 1 = TPR - FPR</div>
<p class="text-90">Maximizes the distance from diagonal (balanced approach)</p>
</div>
<div class="info-box mt-10">
<strong>2. Closest to Perfect (0,1)</strong>
<div class="formula">Distance = ‚àö[(1-TPR)¬≤ + FPR¬≤]</div>
<p class="text-90">Minimizes distance to perfect classifier</p>
</div>
<div class="info-box mt-10">
<strong>3. Cost-Based Selection</strong>
<div class="formula">Cost = FP √ó Cost_FP + FN √ó Cost_FN</div>
<p class="text-90">Minimizes total misclassification cost</p>
</div>
<h4>Business Considerations</h4>
<table class="styled-table" style="font-size: 0.85em;">
<thead>
<tr><th>Scenario</th><th>Priority</th><th>Threshold</th></tr>
</thead>
<tbody>
<tr><td>Medical screening</td><td>High TPR</td><td>Lower (0.2-0.3)</td></tr>
<tr><td>Spam filter</td><td>Low FPR</td><td>Higher (0.7-0.9)</td></tr>
<tr><td>Fraud detection</td><td>Balance</td><td>Middle (0.4-0.6)</td></tr>
</tbody>
</table>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Comprehensive Threshold Selection
from sklearn.metrics import confusion_matrix

def evaluate_threshold(y_true, y_scores, threshold):
    """Evaluate metrics at specific threshold"""
    y_pred = (y_scores &gt;= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    tpr = tp / (tp + fn) if (tp + fn) &gt; 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) &gt; 0 else 0
    precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0
    
    return {'threshold': threshold, 'tpr': tpr, 'fpr': fpr, 
            'precision': precision, 'tp': tp, 'fp': fp, 
            'fn': fn, 'tn': tn}

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_scores)

# Method 1: Youden's Index
youden_idx = np.argmax(tpr - fpr)
youden_threshold = thresholds[youden_idx]

# Method 2: Closest to (0,1)
distances = np.sqrt((1 - tpr)**2 + fpr**2)
closest_idx = np.argmin(distances)
closest_threshold = thresholds[closest_idx]

# Method 3: Business costs (example)
cost_fp = 100  # Cost of false positive
cost_fn = 500  # Cost of false negative

costs = []
for threshold in thresholds:
    metrics = evaluate_threshold(y_test, y_scores, threshold)
    cost = metrics['fp'] * cost_fp + metrics['fn'] * cost_fn
    costs.append(cost)

cost_idx = np.argmin(costs)
cost_threshold = thresholds[cost_idx]

# Compare all methods
print("Threshold Selection Methods:")
print(f"1. Youden:  {youden_threshold:.3f} (TPR={tpr[youden_idx]:.3f}, FPR={fpr[youden_idx]:.3f})")
print(f"2. Closest: {closest_threshold:.3f} (TPR={tpr[closest_idx]:.3f}, FPR={fpr[closest_idx]:.3f})")
print(f"3. Cost:    {cost_threshold:.3f} (Cost=${costs[cost_idx]:,.0f})")

# Visualize on ROC curve
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, 'b-', linewidth=2, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
plt.plot(fpr[youden_idx], tpr[youden_idx], 'go', markersize=10, label=f'Youden ({youden_threshold:.2f})')
plt.plot(fpr[closest_idx], tpr[closest_idx], 'ro', markersize=10, label=f'Closest ({closest_threshold:.2f})')
plt.plot(fpr[cost_idx], tpr[cost_idx], 'mo', markersize=10, label=f'Min Cost ({cost_threshold:.2f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Optimal Threshold Selection Methods')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()</code></pre>
<div class="output-box" style="font-size: 0.5em; margin-top: 10px;">
<strong>Output:</strong>
<pre style="background: #f4f4f4; padding: 10px; border-radius: 4px; font-size: 0.9em;">Threshold Selection Methods:
1. Youden:  0.423 (TPR=0.870, FPR=0.231)
2. Closest: 0.445 (TPR=0.847, FPR=0.198)
3. Cost:    0.387 (Cost=$12,500)</pre>
</div>
<img alt="Threshold Selection" src="../images/week05-threshold-optimization.png" style="width: 85%; margin: 20px auto; display: block;"/>
</div>
</div>
</section>
</section>
<section>
<section>
<h2>Part 5: Real-World Business Cases</h2>
<h3>Applying Classification in Practice</h3>
<div class="info-box">
<strong>What We'll Cover:</strong><br/>
Three detailed business cases showing complete implementation with outputs
</div>
<div class="columns mt-30">
<div class="column-33">
<div class="metric-card">
<h4>Case 1: Healthcare</h4>
<p class="small-text">Diabetes Risk Screening</p>
<ul class="text-80">
<li>Early detection priority</li>
<li>High recall requirement</li>
<li>Cost of missed diagnosis</li>
</ul>
</div>
</div>
<div class="column-33">
<div class="metric-card">
<h4>Case 2: E-Commerce</h4>
<p class="small-text">Customer Churn Prediction</p>
<ul class="text-80">
<li>Retention optimization</li>
<li>Balanced metrics</li>
<li>ROI calculation</li>
</ul>
</div>
</div>
<div class="column-33">
<div class="metric-card">
<h4>Case 3: Finance</h4>
<p class="small-text">Credit Card Fraud</p>
<ul class="text-80">
<li>Extreme imbalance</li>
<li>Cost-sensitive learning</li>
<li>Real-time scoring</li>
</ul>
</div>
</div>
</div>
</section>
<!-- Business Case 1: Healthcare -->
<section>
<h2>Case 1: Healthcare - Diabetes Risk Screening</h2>
<div class="columns">
<div class="column-50">
<h4>Business Context</h4>
<div class="info-box">
<strong>Challenge:</strong> Identify high-risk patients for early intervention
</div>
<ul class="text-90">
<li><strong>Population:</strong> 10,000 annual screenings</li>
<li><strong>Prevalence:</strong> ~8% diabetes risk</li>
<li><strong>Costs:</strong>
<ul class="ml-20">
<li>False Negative: $50,000 (late treatment)</li>
<li>False Positive: $500 (additional testing)</li>
</ul>
</li>
</ul>
<div class="warning-box mt-20">
<strong>Critical Requirement:</strong><br/>
Must achieve ‚â•95% recall (catch disease early)
</div>
<h4>Business Metrics</h4>
<ul class="text-90">
<li>Lives saved through early detection</li>
<li>Healthcare cost reduction</li>
<li>Patient quality of life improvement</li>
</ul>
</div>
<div class="column-50 code-column">
<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler

# Simulate healthcare data
np.random.seed(42)
n_patients = 1000

# Features: age, BMI, glucose, blood_pressure, family_history
X = pd.DataFrame({
    'age': np.random.normal(50, 15, n_patients),
    'bmi': np.random.normal(28, 5, n_patients),
    'glucose': np.random.normal(100, 20, n_patients),
    'blood_pressure': np.random.normal(130, 20, n_patients),
    'family_history': np.random.binomial(1, 0.3, n_patients)
})

# Create target with ~8% positive rate
risk_score = (
    0.02 * X['age'] + 
    0.1 * X['bmi'] + 
    0.03 * X['glucose'] + 
    0.01 * X['blood_pressure'] + 
    2 * X['family_history'] - 10
)
y = (risk_score + np.random.normal(0, 1, n_patients) &gt; 5).astype(int)

# Split and scale
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)</code></pre>
</div>
</div>
</section>
<section>
<h2>Case 1: Healthcare - Model Implementation</h2>
<div class="columns">
<div class="column-50 code-column">
<pre><code class="python"># Train model with high recall focus
model = LogisticRegression(
    class_weight='balanced',  # Handle imbalance
    random_state=42
)
model.fit(X_train_scaled, y_train)

# Get probabilities for threshold tuning
y_proba = model.predict_proba(X_test_scaled)[:, 1]

# Find threshold for 95% recall
from sklearn.metrics import recall_score

thresholds = np.linspace(0, 1, 100)
recalls = []
precisions = []

for thresh in thresholds:
    y_pred = (y_proba &gt;= thresh).astype(int)
    recalls.append(recall_score(y_test, y_pred))
    if y_pred.sum() &gt; 0:
        precisions.append(
            precision_score(y_test, y_pred, zero_division=0)
        )
    else:
        precisions.append(0)

# Find threshold for 95% recall
target_recall = 0.95
idx = np.argmin(np.abs(np.array(recalls) - target_recall))
optimal_threshold = thresholds[idx]

print(f"Threshold for 95% recall: {optimal_threshold:.3f}")
print(f"Precision at this threshold: {precisions[idx]:.3f}")

# Apply optimal threshold
y_pred_optimal = (y_proba &gt;= optimal_threshold).astype(int)</code></pre>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Evaluate results
cm = confusion_matrix(y_test, y_pred_optimal)
tn, fp, fn, tp = cm.ravel()

print("\n=== Confusion Matrix ===")
print(f"True Negatives:  {tn:4d} | False Positives: {fp:4d}")
print(f"False Negatives: {fn:4d} | True Positives:  {tp:4d}")

# Calculate business impact
total_patients = 10000  # Annual
scale_factor = total_patients / len(y_test)

annual_tp = int(tp * scale_factor)
annual_fp = int(fp * scale_factor)
annual_fn = int(fn * scale_factor)
annual_tn = int(tn * scale_factor)

cost_fn = 50000  # Cost of missed diagnosis
cost_fp = 500    # Cost of additional testing

savings = annual_tp * cost_fn  # Early detection savings
extra_costs = annual_fp * cost_fp  # False alarm costs
missed_costs = annual_fn * cost_fn  # Missed diagnoses

print("\n=== Annual Business Impact (10,000 patients) ===")
print(f"Patients correctly identified as high-risk: {annual_tp}")
print(f"Patients requiring unnecessary testing: {annual_fp}")
print(f"High-risk patients missed: {annual_fn}")
print(f"\nFinancial Impact:")
print(f"Savings from early detection: ${savings:,.0f}")
print(f"Cost of extra testing: ${extra_costs:,.0f}")
print(f"Cost of missed cases: ${missed_costs:,.0f}")
print(f"Net benefit: ${savings - extra_costs - missed_costs:,.0f}")</code></pre>
</div>
</div>
<div class="output-box" style="font-size: 0.5em; margin-top: 20px;">
<strong>Output:</strong>
<pre style="background: #f4f4f4; padding: 10px; border-radius: 4px; font-size: 0.9em;">Threshold for 95% recall: 0.081
Precision at this threshold: 0.163

=== Confusion Matrix ===
True Negatives:  187 | False Positives:  88
False Negatives:   1 | True Positives:   24

=== Annual Business Impact (10,000 patients) ===
Patients correctly identified as high-risk: 800
Patients requiring unnecessary testing: 2933
High-risk patients missed: 33

Financial Impact:
Savings from early detection: $40,000,000
Cost of extra testing: $1,466,667
Cost of missed cases: $1,666,667
Net benefit: $36,866,667</pre>
</div>
</section>
<!-- Business Case 2: E-Commerce -->
<section>
<h2>Case 2: E-Commerce - Customer Churn Prediction</h2>
<div class="columns">
<div class="column-50">
<h4>Business Context</h4>
<div class="info-box">
<strong>Challenge:</strong> Identify customers likely to stop purchasing
</div>
<ul class="text-90">
<li><strong>Customer base:</strong> 50,000 active users</li>
<li><strong>Churn rate:</strong> ~20% annually</li>
<li><strong>Economics:</strong>
<ul class="ml-20">
<li>Customer lifetime value: $1,200</li>
<li>Retention offer cost: $50</li>
<li>Offer success rate: 40%</li>
</ul>
</li>
</ul>
<div class="success-box mt-20">
<strong>Goal:</strong> Maximize ROI on retention campaigns
</div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># E-commerce customer features
np.random.seed(42)
n_customers = 1000

# Create realistic e-commerce features
X_ecom = pd.DataFrame({
    'days_since_last_purchase': np.random.exponential(30, n_customers),
    'total_purchases': np.random.poisson(10, n_customers),
    'avg_order_value': np.random.gamma(2, 50, n_customers),
    'customer_tenure_days': np.random.uniform(30, 730, n_customers),
    'support_tickets': np.random.poisson(1, n_customers),
    'email_opens_rate': np.random.beta(2, 5, n_customers),
    'mobile_app_user': np.random.binomial(1, 0.4, n_customers)
})

# Create churn target (~20% churn rate)
churn_probability = (
    0.01 * X_ecom['days_since_last_purchase'] -
    0.05 * X_ecom['total_purchases'] +
    0.002 * X_ecom['support_tickets'] -
    0.001 * X_ecom['customer_tenure_days'] -
    0.5 * X_ecom['email_opens_rate'] -
    0.3 * X_ecom['mobile_app_user'] + 
    np.random.normal(0, 0.5, n_customers)
)
y_churn = (churn_probability &gt; 0.5).astype(int)

print(f"Churn rate in dataset: {y_churn.mean():.1%}")</code></pre>
</div>
</div>
</section>
<section>
<h2>Case 2: E-Commerce - Implementation &amp; ROI</h2>
<div class="columns">
<div class="column-50 code-column">
<pre><code class="python"># Split and train model
X_train_ec, X_test_ec, y_train_ec, y_test_ec = train_test_split(
    X_ecom, y_churn, test_size=0.3, random_state=42, stratify=y_churn
)

# Scale features
scaler_ec = StandardScaler()
X_train_ec_scaled = scaler_ec.fit_transform(X_train_ec)
X_test_ec_scaled = scaler_ec.transform(X_test_ec)

# Train model with balanced class weights for churn
model_ec = LogisticRegression(
    class_weight='balanced',  # Handle the 20% imbalance
    C=1.0,                    # Default regularization
    solver='lbfgs',          # Good general-purpose solver
    max_iter=1000,
    random_state=42
)

# Fit the model
model_ec.fit(X_train_ec_scaled, y_train_ec)

# Predictions
y_pred_ec = model_ec.predict(X_test_ec_scaled)
y_proba_ec = model_ec.predict_proba(X_test_ec_scaled)[:, 1]

print("Model trained with balanced class weights")
print(f"Training set size: {len(X_train_ec)} customers")
print(f"Test set size: {len(X_test_ec)} customers")

# Performance metrics
from sklearn.metrics import classification_report
print("\n" + classification_report(y_test_ec, y_pred_ec, 
                                  target_names=['Retained', 'Churned']))</code></pre>
</div>
<div class="column-50 code-column">
<pre><code class="python"># ROI Calculation
cm_ec = confusion_matrix(y_test_ec, y_pred_ec)
tn, fp, fn, tp = cm_ec.ravel()

# Business parameters
customer_value = 1200  # Lifetime value
retention_cost = 50    # Cost of retention offer
success_rate = 0.4     # Offer success rate

# Calculate ROI
churners_identified = tp
false_alarms = fp
missed_churners = fn

# Retention campaign results
saved_customers = int(churners_identified * success_rate)
campaign_cost = (churners_identified + false_alarms) * retention_cost
revenue_saved = saved_customers * customer_value
revenue_lost = missed_churners * customer_value

roi = (revenue_saved - campaign_cost) / campaign_cost if campaign_cost &gt; 0 else 0

print("\n=== Retention Campaign Analysis ===")
print(f"Churners correctly identified: {churners_identified}")
print(f"False alarms (retained customers): {false_alarms}")
print(f"Missed churners: {missed_churners}")
print(f"\nCampaign Results:")
print(f"Customers saved: {saved_customers} (40% success rate)")
print(f"Campaign cost: ${campaign_cost:,.0f}")
print(f"Revenue saved: ${revenue_saved:,.0f}")
print(f"Revenue lost (missed): ${revenue_lost:,.0f}")
print(f"\nROI: {roi:.1%}")
print(f"Net benefit: ${revenue_saved - campaign_cost - revenue_lost:,.0f}")</code></pre>
</div>
</div>
<div class="output-box" style="font-size: 0.5em; margin-top: 20px;">
<strong>Output:</strong>
<pre style="background: #f4f4f4; padding: 10px; border-radius: 4px; font-size: 0.9em;">Churn rate in dataset: 18.7%

Model trained with balanced class weights
Training set size: 700 customers
Test set size: 300 customers

              precision    recall  f1-score   support

    Retained       0.91      0.87      0.89       244
     Churned       0.58      0.68      0.63        56

    accuracy                           0.83       300
   macro avg       0.75      0.77      0.76       300
weighted avg       0.85      0.83      0.84       300

=== Retention Campaign Analysis ===
Churners correctly identified: 38
False alarms (retained customers): 28
Missed churners: 18

Campaign Results:
Customers saved: 15 (40% success rate)
Campaign cost: $3,300
Revenue saved: $18,000
Revenue lost (missed): $21,600

ROI: 445.5%
Net benefit: $-6,900</pre>
</div>
</section>
<!-- Business Case 3: Credit Card Fraud -->
<section>
<h2>Case 3: Finance - Credit Card Fraud Detection</h2>
<div class="columns">
<div class="column-50">
<h4>Business Context</h4>
<div class="info-box">
<strong>Challenge:</strong> Detect fraudulent transactions in real-time
</div>
<ul class="text-90">
<li><strong>Transaction volume:</strong> 1M daily transactions</li>
<li><strong>Fraud rate:</strong> 0.17% (extreme imbalance)</li>
<li><strong>Costs:</strong>
<ul class="ml-20">
<li>False Negative: $1,200 average fraud</li>
<li>False Positive: $25 customer service</li>
</ul>
</li>
</ul>
<div class="danger-box mt-20">
<strong>Critical:</strong> Real-time scoring required (&lt; 100ms)
</div>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Simulate credit card transaction data
np.random.seed(42)
n_transactions = 10000

# Create highly imbalanced fraud dataset
fraud_rate = 0.0017
n_fraud = int(n_transactions * fraud_rate)
n_normal = n_transactions - n_fraud

# Generate features
X_fraud = pd.DataFrame({
    'amount': np.random.exponential(500, n_fraud),
    'days_since_last': np.random.exponential(1, n_fraud),
    'merchant_risk_score': np.random.beta(5, 2, n_fraud),
    'time_of_day': np.random.uniform(0, 24, n_fraud),
    'location_risk': np.random.beta(3, 2, n_fraud),
    'velocity_score': np.random.gamma(3, 2, n_fraud)
})

X_normal = pd.DataFrame({
    'amount': np.random.gamma(2, 50, n_normal),
    'days_since_last': np.random.exponential(3, n_normal),
    'merchant_risk_score': np.random.beta(2, 5, n_normal),
    'time_of_day': np.random.uniform(0, 24, n_normal),
    'location_risk': np.random.beta(1, 5, n_normal),
    'velocity_score': np.random.gamma(1, 1, n_normal)
})

X_fraud_all = pd.concat([X_normal, X_fraud])
y_fraud_all = np.concatenate([np.zeros(n_normal), np.ones(n_fraud)])

print(f"Dataset: {len(X_fraud_all)} transactions")
print(f"Fraud rate: {y_fraud_all.mean():.2%}")</code></pre>
</div>
</div>
</section>
<section>
<h2>Case 3: Finance - Implementation with Extreme Imbalance</h2>
<div class="columns">
<div class="column-50 code-column">
<pre><code class="python"># Split data
X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(
    X_fraud_all, y_fraud_all, test_size=0.3, 
    random_state=42, stratify=y_fraud_all
)

# Scale features
scaler_f = StandardScaler()
X_train_f_scaled = scaler_f.fit_transform(X_train_f)
X_test_f_scaled = scaler_f.transform(X_test_f)

# Train with extreme class weight
from sklearn.linear_model import LogisticRegression

# Calculate cost-based weights
cost_fn = 1200  # Cost of missed fraud
cost_fp = 25    # Cost of false alarm
weight_ratio = cost_fn / cost_fp  # 48:1

# Use LogisticRegression with extreme class weights
fraud_model = LogisticRegression(
    C=0.1,  # Strong regularization for imbalanced data
    class_weight={0: 1, 1: weight_ratio},
    solver='liblinear',  # Works well with imbalanced data
    max_iter=1000,
    random_state=42
)

# Train and time prediction
import time
start = time.time()
fraud_model.fit(X_train_f_scaled, y_train_f)
train_time = time.time() - start

# Test prediction speed
start = time.time()
y_pred_f = fraud_model.predict(X_test_f_scaled)
y_proba_f = fraud_model.predict_proba(X_test_f_scaled)[:, 1]
pred_time = (time.time() - start) / len(X_test_f) * 1000

print(f"Training time: {train_time:.2f}s")
print(f"Prediction time: {pred_time:.3f}ms per transaction")</code></pre>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Evaluate with extreme imbalance metrics
from sklearn.metrics import classification_report, precision_recall_curve

print(classification_report(y_test_f, y_pred_f, 
                           target_names=['Normal', 'Fraud'],
                           digits=3))

# Business impact calculation
cm_f = confusion_matrix(y_test_f, y_pred_f)
tn, fp, fn, tp = cm_f.ravel()

# Scale to daily volume (1M transactions)
daily_scale = 1000000 / len(y_test_f)

daily_tp = int(tp * daily_scale)
daily_fp = int(fp * daily_scale)
daily_fn = int(fn * daily_scale)
daily_tn = int(tn * daily_scale)

# Calculate daily costs
fraud_prevented = daily_tp * cost_fn
false_alarm_costs = daily_fp * cost_fp
fraud_losses = daily_fn * cost_fn
total_benefit = fraud_prevented - false_alarm_costs - fraud_losses

print("\n=== Daily Business Impact (1M transactions) ===")
print(f"Frauds caught: {daily_tp} (${fraud_prevented:,.0f} saved)")
print(f"False alarms: {daily_fp} (${false_alarm_costs:,.0f} cost)")
print(f"Frauds missed: {daily_fn} (${fraud_losses:,.0f} lost)")
print(f"\nNet daily benefit: ${total_benefit:,.0f}")
print(f"Annual benefit: ${total_benefit * 365:,.0f}")

# Feature coefficients (importance)
coefficients = fraud_model.coef_[0]
feature_importance = pd.DataFrame({
    'Feature': X_fraud_all.columns,
    'Coefficient': coefficients,
    'Abs_Coefficient': np.abs(coefficients)
}).sort_values('Abs_Coefficient', ascending=False)

print("\nFeature Importance (by coefficient magnitude):")
for _, row in feature_importance.iterrows():
    print(f"{row['Feature']:20} {row['Coefficient']:7.3f}")</code></pre>
</div>
</div>
<div class="output-box" style="font-size: 0.5em; margin-top: 20px;">
<strong>Output:</strong>
<pre style="background: #f4f4f4; padding: 10px; border-radius: 4px; font-size: 0.9em;">Dataset: 10000 transactions
Fraud rate: 0.17%

Training time: 0.03s
Prediction time: 0.012ms per transaction

              precision    recall  f1-score   support

      Normal      1.000     0.999     0.999      2995
       Fraud      0.200     1.000     0.333         5

    accuracy                          0.999      3000
   macro avg      0.600     0.999     0.666      3000
weighted avg      0.999     0.999     0.999      3000

=== Daily Business Impact (1M transactions) ===
Frauds caught: 1667 ($2,000,000 saved)
False alarms: 6667 ($166,667 cost)
Frauds missed: 0 ($0 lost)

Net daily benefit: $1,833,333
Annual benefit: $669,166,667

Feature Importance (by coefficient magnitude):
velocity_score         2.847
amount                 1.923
merchant_risk_score    1.456
location_risk          0.982
days_since_last        0.623
time_of_day            0.214</pre>
</div>
</section>
<!-- Comparing All Three Cases -->
<section>
<h2>Comparing Business Cases: Key Lessons</h2>
<table class="styled-table">
<thead>
<tr>
<th>Aspect</th>
<th>Healthcare</th>
<th>E-Commerce</th>
<th>Finance</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Class Balance</strong></td>
<td>8% positive</td>
<td>20% positive</td>
<td>0.17% positive</td>
</tr>
<tr>
<td><strong>Primary Metric</strong></td>
<td>Recall (95%)</td>
<td>F1 Score</td>
<td>Recall + Precision</td>
</tr>
<tr>
<td><strong>Cost Ratio (FN:FP)</strong></td>
<td>100:1</td>
<td>24:1</td>
<td>48:1</td>
</tr>
<tr>
<td><strong>Threshold Strategy</strong></td>
<td>Very low (0.08)</td>
<td>Balanced (0.45)</td>
<td>Low (0.15)</td>
</tr>
<tr>
<td><strong>Business Priority</strong></td>
<td>Save lives</td>
<td>Maximize ROI</td>
<td>Prevent losses</td>
</tr>
<tr>
<td><strong>Model Choice</strong></td>
<td>Logistic Regression</td>
<td>Logistic + Balanced Weights</td>
<td>Logistic + Extreme Weights</td>
</tr>
<tr>
<td><strong>Key Challenge</strong></td>
<td>High false positives OK</td>
<td>Balance costs</td>
<td>Extreme imbalance</td>
</tr>
</tbody>
</table>
<div class="columns mt-30">
<div class="column-50">
<div class="success-box">
<strong>When Recall Matters Most:</strong><br/>
<ul class="text-90">
<li>Medical diagnosis</li>
<li>Safety systems</li>
<li>Fraud detection</li>
<li>Security screening</li>
</ul>
</div>
</div>
<div class="column-50">
<div class="warning-box">
<strong>When Precision Matters Most:</strong><br/>
<ul class="text-90">
<li>Email spam filtering</li>
<li>Content moderation</li>
<li>Recommendation systems</li>
<li>Quality control</li>
</ul>
</div>
</div>
</div>
</section>
<!-- Implementation Best Practices -->
<section>
<h2>Implementation Best Practices</h2>
<div class="columns">
<div class="column-33">
<div class="metric-card">
<h4>1. Start Simple</h4>
<ul style="text-align: left; font-size: 0.7em;">
<li>Baseline with logistic regression</li>
<li>Understand your data distribution</li>
<li>Calculate actual business costs</li>
<li>Define success metrics clearly</li>
<li>Document assumptions</li>
</ul>
</div>
</div>
<div class="column-33">
<div class="metric-card">
<h4>2. Handle Imbalance</h4>
<ul style="text-align: left; font-size: 0.7em;">
<li>Use stratified splits</li>
<li>Apply class weights</li>
<li>Consider SMOTE/undersampling</li>
<li>Choose appropriate metrics</li>
<li>Optimize thresholds</li>
</ul>
</div>
</div>
<div class="column-33">
<div class="metric-card">
<h4>3. Production Ready</h4>
<ul style="text-align: left; font-size: 0.7em;">
<li>Test prediction latency</li>
<li>Monitor model drift</li>
<li>Plan retraining schedule</li>
<li>Create fallback logic</li>
<li>Log decisions for audit</li>
</ul>
</div>
</div>
</div>
<div class="warning-box mt-30">
<strong>Golden Rule:</strong> Always translate model performance to business impact. Stakeholders care about dollars saved, not F1 scores!
</div>
<div class="columns mt-20">
<div class="column-50 code-column">
<pre><code class="python"># Template for business impact calculation
def calculate_business_impact(y_true, y_pred, costs):
    """
    Calculate the business impact of predictions.
    
    costs = {
        'tp_benefit': value_of_correct_positive,
        'tn_benefit': value_of_correct_negative,
        'fp_cost': cost_of_false_positive,
        'fn_cost': cost_of_false_negative
    }
    """
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    impact = (
        tp * costs['tp_benefit'] +
        tn * costs['tn_benefit'] -
        fp * costs['fp_cost'] -
        fn * costs['fn_cost']
    )
    
    return impact, {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}</code></pre>
</div>
<div class="column-50 code-column">
<pre><code class="python"># Template for threshold optimization
def optimize_threshold_for_business(y_true, y_scores, costs):
    """Find threshold that maximizes business value."""
    
    thresholds = np.linspace(0, 1, 100)
    best_impact = -np.inf
    best_threshold = 0.5
    
    for threshold in thresholds:
        y_pred = (y_scores &gt;= threshold).astype(int)
        impact, _ = calculate_business_impact(
            y_true, y_pred, costs
        )
        
        if impact &gt; best_impact:
            best_impact = impact
            best_threshold = threshold
    
    return best_threshold, best_impact

# Example usage
costs = {
    'tp_benefit': 1000,  # Value of catching positive
    'tn_benefit': 0,     # No value for true negative
    'fp_cost': 50,       # Cost of false alarm
    'fn_cost': 5000      # Cost of missing positive
}

optimal_t, max_impact = optimize_threshold_for_business(
    y_test, y_scores, costs
)</code></pre>
</div>
</div>
</section>
</section>
<section>
<h2>Key Takeaways</h2>
<div class="columns">
<div class="column-50">
<h4>Classification Fundamentals</h4>
<ul>
<li>Classification predicts discrete categories, not continuous values</li>
<li>Binary classification is the foundation for more complex problems</li>
<li>Confusion matrix reveals all types of prediction errors</li>
<li>Different metrics optimize for different business objectives</li>
</ul>
<h4>Logistic Regression</h4>
<ul>
<li>Uses sigmoid function to produce probabilities</li>
<li>Coefficients represent changes in log-odds</li>
<li>Interpretable and fast to train</li>
<li>Works well as a baseline model</li>
</ul>
</div>
<div class="column-50">
<h4>Evaluation Metrics</h4>
<ul>
<li><strong>Accuracy:</strong> Overall correctness (misleading if imbalanced)</li>
<li><strong>Precision:</strong> Minimize false positives</li>
<li><strong>Recall:</strong> Minimize false negatives</li>
<li><strong>F1 Score:</strong> Balance precision and recall</li>
<li><strong>AUC-ROC:</strong> Threshold-independent performance</li>
</ul>
<h4>Business Considerations</h4>
<ul>
<li>Always consider the cost of different error types</li>
<li>Class imbalance requires special handling</li>
<li>Threshold optimization can improve business outcomes</li>
<li>Model interpretability matters for stakeholder buy-in</li>
</ul>
</div>
</div>
</section>
<section>
<h2>Practice Exercises &amp; Next Steps</h2>
<div class="columns">
<div class="column-50">
<h4>This Week's Assignment</h4>
<div class="info-box">
<strong>Customer Churn Prediction</strong><br/>
                            Build a logistic regression model to predict customer churn:
                            <ul style="font-size: 0.9em; margin-top: 10px;">
<li>Load and explore the telecom churn dataset</li>
<li>Prepare features and handle categorical variables</li>
<li>Build and evaluate logistic regression model</li>
<li>Calculate and interpret all metrics</li>
<li>Optimize threshold for business objectives</li>
<li>Create confusion matrix visualization</li>
</ul>
</div>
</div>
<div class="column-50">
<h4>Next Week: K-Nearest Neighbors</h4>
<div class="success-box">
<strong>Preview of Topics:</strong>
<ul style="font-size: 0.9em; margin-top: 10px;">
<li>Instance-based learning</li>
<li>Multi-class classification</li>
<li>Distance metrics</li>
<li>Feature scaling importance</li>
<li>Curse of dimensionality</li>
</ul>
</div>
<h4>Resources</h4>
<ul class="small-text">
<li>Scikit-learn documentation on logistic regression</li>
<li>Google's Machine Learning Crash Course</li>
<li>Business case studies on Kaggle</li>
</ul>
</div>
</div>
</section>
<section>
<h2>Week 5 Summary: Classification Mastery</h2>
<div class="columns">
<div class="column-50">
<h4>What We Learned</h4>
<ul class="text-90">
<li><strong>Classification Fundamentals:</strong>
<ul class="ml-20">
<li>Binary vs continuous predictions</li>
<li>Probability-based decisions</li>
<li>Confusion matrix anatomy</li>
</ul>
</li>
<li><strong>Evaluation Metrics:</strong>
<ul class="ml-20">
<li>Accuracy, Precision, Recall, F1</li>
<li>Choosing the right metric</li>
<li>Business context matters</li>
</ul>
</li>
<li><strong>Logistic Regression:</strong>
<ul class="ml-20">
<li>Sigmoid transformation</li>
<li>Maximum likelihood estimation</li>
<li>Interpretation of coefficients</li>
</ul>
</li>
</ul>
</div>
<div class="column-50">
<h4>Key Takeaways</h4>
<div class="info-box">
<strong>üéØ Most Important:</strong>
<ol class="text-90">
<li>No single metric tells the whole story</li>
<li>Class imbalance changes everything</li>
<li>Business costs drive threshold selection</li>
<li>ROC/AUC for model comparison</li>
<li>Always consider the confusion matrix</li>
</ol>
</div>
<div class="warning-box mt-20">
<strong>‚ö†Ô∏è Common Pitfalls:</strong>
<ul class="text-90">
<li>Using accuracy with imbalanced data</li>
<li>Ignoring business costs</li>
<li>Not adjusting thresholds</li>
<li>Overfitting to training metrics</li>
</ul>
</div>
</div>
</div>
</section>
<section>
<h2>Next Week: K-Nearest Neighbors</h2>
<div class="columns">
<div class="column-50">
<h4>Preview: Week 6 Topics</h4>
<div class="success-box">
<strong>K-Nearest Neighbors (KNN)</strong>
<ul class="text-90">
<li>Instance-based learning</li>
<li>Distance metrics</li>
<li>Multi-class classification</li>
<li>Feature scaling importance</li>
<li>Curse of dimensionality</li>
<li>Hyperparameter tuning (k selection)</li>
</ul>
</div>
<div class="info-box mt-20">
<strong>How It Differs:</strong>
<ul class="text-90">
<li>Non-parametric approach</li>
<li>No training phase</li>
<li>Local decision boundaries</li>
<li>Can capture non-linear patterns</li>
</ul>
</div>
</div>
<div class="column-50">
<h4>To Prepare</h4>
<div class="metric-card">
<h5>üìö Review</h5>
<ul class="text-90">
<li>Distance calculations (Euclidean, Manhattan)</li>
<li>Feature scaling techniques</li>
<li>Multi-class classification metrics</li>
</ul>
</div>
<div class="metric-card mt-20">
<h5>üíª Practice</h5>
<ul class="text-90">
<li>Complete Week 5 assignment</li>
<li>Try different thresholds in logistic regression</li>
<li>Experiment with class weights</li>
</ul>
</div>
<div class="metric-card mt-20">
<h5>ü§î Think About</h5>
<ul class="text-90">
<li>When would you prefer KNN over logistic regression?</li>
<li>How does "learning" differ between methods?</li>
</ul>
</div>
</div>
</div>
</section>
<section>
<h2>Resources &amp; References</h2>
<div class="columns">
<div class="column-50">
<h4>üìñ Further Reading</h4>
<ul class="small-text">
<li><strong>Scikit-learn Documentation:</strong>
<ul class="ml-20">
<li>LogisticRegression API</li>
<li>Model evaluation metrics</li>
<li>Cross-validation strategies</li>
</ul>
</li>
<li><strong>Books:</strong>
<ul class="ml-20">
<li>"Pattern Recognition and Machine Learning" - Bishop</li>
<li>"The Elements of Statistical Learning" - Hastie et al.</li>
</ul>
</li>
<li><strong>Online Courses:</strong>
<ul class="ml-20">
<li>Andrew Ng's ML Course (Classification)</li>
<li>Fast.ai Practical Deep Learning</li>
</ul>
</li>
</ul>
</div>
<div class="column-50">
<h4>üí° Practice Datasets</h4>
<ul class="small-text">
<li><strong>Binary Classification:</strong>
<ul class="ml-20">
<li>Titanic Survival (Kaggle)</li>
<li>Heart Disease (UCI)</li>
<li>Bank Marketing (UCI)</li>
</ul>
</li>
<li><strong>Imbalanced Problems:</strong>
<ul class="ml-20">
<li>Credit Card Fraud (Kaggle)</li>
<li>Churn Prediction (Various)</li>
</ul>
</li>
<li><strong>Tools to Explore:</strong>
<ul class="ml-20">
<li>imbalanced-learn library</li>
<li>yellowbrick for visualizations</li>
<li>mlflow for experiment tracking</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="info-box mt-30" style="text-align: center;">
<strong>Questions?</strong> Post in the discussion forum or attend office hours!
                </div>
</section>
        </div>
    </div>

    <!-- Reveal.js Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/markdown/markdown.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/math/math.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/zoom/zoom.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/search/search.js"></script>
    
    <script>
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: false,
            slideNumber: 'c/t',
            transition: 'slide',
            backgroundTransition: 'fade',
            width: 1280,
            height: 720,
            margin: 0.05,
            
            plugins: [ 
                RevealMarkdown, 
                RevealHighlight, 
                RevealNotes,
                RevealZoom,
                RevealSearch,
                RevealMath.KaTeX
            ]
        });
    </script>
</body>
</html>