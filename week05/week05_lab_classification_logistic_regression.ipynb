{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Lab: Classification & Logistic Regression for Credit Risk Assessment\n",
    "\n",
    "## Business Scenario\n",
    "\n",
    "You've been hired as a senior data scientist at **SecureBank**, a regional financial institution that processes thousands of loan applications monthly. The bank is facing several challenges:\n",
    "\n",
    "1. **Rising Default Rates**: Recent economic volatility has increased loan defaults\n",
    "2. **Manual Review Bottleneck**: Current credit assessment takes 5-7 days per application\n",
    "3. **Inconsistent Decisions**: Different loan officers make different decisions for similar profiles\n",
    "4. **Regulatory Compliance**: Need to demonstrate fair and explainable lending practices\n",
    "\n",
    "Your mission is to build an automated credit risk assessment system that can:\n",
    "- **Predict loan approval/denial** with high accuracy\n",
    "- **Handle class imbalance** (most applicants are approved)\n",
    "- **Optimize business outcomes** by balancing approval rates with default risk\n",
    "- **Provide explainable results** for regulatory compliance\n",
    "\n",
    "## Learning Objectives\n",
    "By completing this lab, you will:\n",
    "- Understand binary classification vs. regression\n",
    "- Work with imbalanced datasets using appropriate techniques\n",
    "- Master logistic regression and interpret coefficients\n",
    "- Evaluate models using classification metrics (precision, recall, F1, AUC)\n",
    "- Optimize decision thresholds for business objectives\n",
    "- Create and interpret confusion matrices\n",
    "- Analyze ROC and Precision-Recall curves\n",
    "- Calculate business costs and benefits of model decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Generation\n",
    "\n",
    "First, let's import necessary libraries and generate synthetic loan application data that reflects real-world patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, precision_recall_curve,\n",
    "    roc_auc_score, average_precision_score, accuracy_score, precision_score, \n",
    "    recall_score, f1_score, make_scorer\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Credit Risk Data\n",
    "\n",
    "We'll create realistic loan application data with the following features:\n",
    "- **Demographics**: Age, income, employment length\n",
    "- **Credit History**: Credit score, previous defaults, credit utilization\n",
    "- **Loan Details**: Requested amount, loan purpose, debt-to-income ratio\n",
    "- **Financial Profile**: Assets, existing debts, savings\n",
    "\n",
    "The approval decision will be based on these factors with realistic business rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loan_applications(n_samples=5000):\n",
    "    \"\"\"\n",
    "    Generate synthetic loan application data with realistic credit risk patterns.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Demographics\n",
    "    age = np.random.normal(40, 12, n_samples)\n",
    "    age = np.clip(age, 18, 75)\n",
    "    \n",
    "    # Income follows log-normal distribution (realistic for income)\n",
    "    log_income = np.random.normal(10.5, 0.6, n_samples)  # log of income\n",
    "    annual_income = np.exp(log_income)\n",
    "    annual_income = np.clip(annual_income, 20000, 500000)\n",
    "    \n",
    "    employment_length = np.random.exponential(5, n_samples)\n",
    "    employment_length = np.clip(employment_length, 0, 40)\n",
    "    \n",
    "    # Credit History\n",
    "    credit_score = np.random.normal(680, 80, n_samples)\n",
    "    credit_score = np.clip(credit_score, 300, 850)\n",
    "    \n",
    "    # Credit utilization (percentage of credit limit used)\n",
    "    credit_utilization = np.random.beta(2, 3, n_samples) * 100\n",
    "    \n",
    "    # Previous defaults (binary)\n",
    "    default_prob = 1 / (1 + np.exp((credit_score - 600) / 50))  # Sigmoid based on credit score\n",
    "    previous_defaults = np.random.binomial(1, default_prob, n_samples)\n",
    "    \n",
    "    # Loan Details\n",
    "    loan_amount = np.random.lognormal(10, 0.8, n_samples)\n",
    "    loan_amount = np.clip(loan_amount, 1000, 100000)\n",
    "    \n",
    "    # Debt-to-income ratio\n",
    "    existing_debt = np.random.exponential(annual_income * 0.3, n_samples)\n",
    "    debt_to_income = (existing_debt + loan_amount * 0.1) / annual_income\n",
    "    debt_to_income = np.clip(debt_to_income, 0, 2)\n",
    "    \n",
    "    # Loan purpose\n",
    "    loan_purposes = ['home', 'auto', 'business', 'personal', 'education', 'medical']\n",
    "    purpose_weights = [0.3, 0.25, 0.15, 0.15, 0.1, 0.05]\n",
    "    loan_purpose = np.random.choice(loan_purposes, n_samples, p=purpose_weights)\n",
    "    \n",
    "    # Assets and savings\n",
    "    liquid_assets = np.random.exponential(annual_income * 0.2, n_samples)\n",
    "    liquid_assets = np.clip(liquid_assets, 0, annual_income * 2)\n",
    "    \n",
    "    # Calculate approval probability based on realistic credit scoring\n",
    "    risk_score = (\n",
    "        0.4 * ((credit_score - 300) / 550)  # Credit score impact\n",
    "        + 0.2 * np.minimum(annual_income / 100000, 1)  # Income impact (capped)\n",
    "        + 0.15 * np.minimum(employment_length / 10, 1)  # Employment stability\n",
    "        + 0.1 * (1 - debt_to_income / 2)  # Debt-to-income (lower is better)\n",
    "        + 0.1 * np.minimum(liquid_assets / annual_income, 0.5) * 2  # Assets\n",
    "        + 0.05 * (1 - credit_utilization / 100)  # Credit utilization (lower is better)\n",
    "        - 0.3 * previous_defaults  # Previous defaults penalty\n",
    "    )\n",
    "    \n",
    "    # Add some noise and business-specific adjustments\n",
    "    purpose_adjustments = {\n",
    "        'home': 0.1, 'auto': 0.05, 'business': -0.05,\n",
    "        'personal': -0.1, 'education': 0.0, 'medical': 0.0\n",
    "    }\n",
    "    \n",
    "    for i, purpose in enumerate(loan_purpose):\n",
    "        risk_score[i] += purpose_adjustments[purpose]\n",
    "    \n",
    "    # Add random noise\n",
    "    risk_score += np.random.normal(0, 0.1, n_samples)\n",
    "    \n",
    "    # Convert to probability and make binary decision\n",
    "    approval_probability = 1 / (1 + np.exp(-5 * (risk_score - 0.4)))  # Sigmoid\n",
    "    \n",
    "    # Create imbalanced dataset (more approvals than denials, but not too extreme)\n",
    "    approved = np.random.binomial(1, approval_probability, n_samples)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'annual_income': annual_income,\n",
    "        'employment_length': employment_length,\n",
    "        'credit_score': credit_score,\n",
    "        'credit_utilization': credit_utilization,\n",
    "        'previous_defaults': previous_defaults,\n",
    "        'loan_amount': loan_amount,\n",
    "        'debt_to_income': debt_to_income,\n",
    "        'loan_purpose': loan_purpose,\n",
    "        'liquid_assets': liquid_assets,\n",
    "        'approved': approved\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_loan_applications(5000)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['approved'].value_counts(normalize=True))\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis\n",
    "\n",
    "Before building our classification model, let's understand the data and identify patterns that distinguish approved from denied applications.\n",
    "\n",
    "### Exercise 2.1: Basic Statistics and Class Balance\n",
    "**Task**: Analyze the dataset structure and examine class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display basic statistics and check data quality\n",
    "print(\"Dataset Summary Statistics:\")\n",
    "# YOUR CODE HERE: Display summary statistics for numeric columns\n",
    "______\n",
    "\n",
    "print(\"\\nMissing Values Check:\")\n",
    "# YOUR CODE HERE: Check for missing values\n",
    "______\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "approval_counts = df['approved'].value_counts()\n",
    "approval_pct = df['approved'].value_counts(normalize=True)\n",
    "print(f\"Denied (0): {approval_counts[0]:,} ({approval_pct[0]:.1%})\")\n",
    "print(f\"Approved (1): {approval_counts[1]:,} ({approval_pct[1]:.1%})\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = approval_counts[1] / approval_counts[0]\n",
    "print(f\"\\nImbalance Ratio (Approved:Denied): {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio > 2 or imbalance_ratio < 0.5:\n",
    "    print(\"⚠️  Class imbalance detected - will need special handling\")\n",
    "else:\n",
    "    print(\"✅ Classes are reasonably balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Visualize Feature Distributions by Class\n",
    "**Task**: Compare feature distributions between approved and denied applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations comparing approved vs denied applications\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Numeric features to plot\n",
    "numeric_features = ['credit_score', 'annual_income', 'debt_to_income', 'credit_utilization',\n",
    "                   'age', 'employment_length', 'loan_amount', 'liquid_assets', 'previous_defaults']\n",
    "\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    # Create overlapping histograms\n",
    "    denied = df[df['approved'] == 0][feature]\n",
    "    approved = df[df['approved'] == 1][feature]\n",
    "    \n",
    "    axes[i].hist(denied, bins=30, alpha=0.7, label='Denied', color='red', density=True)\n",
    "    axes[i].hist(approved, bins=30, alpha=0.7, label='Approved', color='green', density=True)\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Calculate and display mean differences for key features\n",
    "print(\"Feature Comparison (Approved vs Denied):\")\n",
    "print(\"=\"*50)\n",
    "for feature in ['credit_score', 'annual_income', 'debt_to_income', 'credit_utilization']:\n",
    "    denied_mean = df[df['approved'] == 0][feature].mean()\n",
    "    approved_mean = ______  # YOUR CODE HERE: Calculate mean for approved\n",
    "    difference = approved_mean - denied_mean\n",
    "    print(f\"{feature:18s}: Denied={denied_mean:8.0f}, Approved={approved_mean:8.0f}, Diff={difference:+8.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Categorical Feature Analysis\n",
    "**Task**: Analyze approval rates by loan purpose and other categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze approval rates by loan purpose\n",
    "purpose_analysis = df.groupby('loan_purpose')['approved'].agg(['count', 'mean', 'sum']).round(3)\n",
    "purpose_analysis.columns = ['Applications', 'Approval_Rate', 'Approved_Count']\n",
    "purpose_analysis = purpose_analysis.sort_values('Approval_Rate', ascending=False)\n",
    "\n",
    "print(\"Approval Rates by Loan Purpose:\")\n",
    "print(\"=\"*40)\n",
    "print(purpose_analysis)\n",
    "\n",
    "# Visualize approval rates by purpose\n",
    "plt.figure(figsize=(12, 6))\n",
    "purpose_analysis['Approval_Rate'].plot(kind='bar', color='steelblue')\n",
    "plt.title('Approval Rate by Loan Purpose')\n",
    "plt.xlabel('Loan Purpose')\n",
    "plt.ylabel('Approval Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Create a crosstab for previous defaults vs approval\n",
    "print(\"\\nApproval Rates by Previous Defaults:\")\n",
    "print(\"=\"*40)\n",
    "defaults_crosstab = ______  # YOUR CODE HERE: Create crosstab with margins\n",
    "print(defaults_crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Preprocessing\n",
    "\n",
    "Before training our classification model, we need to prepare the data properly.\n",
    "\n",
    "### Exercise 3.1: Handle Categorical Variables\n",
    "**Task**: Encode categorical variables for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# TODO: One-hot encode loan_purpose\n",
    "# YOUR CODE HERE: Use pd.get_dummies() to encode loan_purpose\n",
    "purpose_encoded = ______\n",
    "\n",
    "# Drop original categorical column and add encoded ones\n",
    "df_processed = df_processed.drop('loan_purpose', axis=1)\n",
    "df_processed = pd.concat([df_processed, purpose_encoded], axis=1)\n",
    "\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"After encoding: {df_processed.shape[1]}\")\n",
    "print(f\"\\nNew encoded features:\")\n",
    "print(purpose_encoded.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Create Train/Validation/Test Splits\n",
    "**Task**: Split data while maintaining class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_processed.drop('approved', axis=1)\n",
    "y = df_processed['approved']\n",
    "\n",
    "# TODO: Create stratified splits to maintain class balance\n",
    "# First split: separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=______  # YOUR CODE HERE: stratify by target\n",
    ")\n",
    "\n",
    "# Second split: separate train (60%) and validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=______  # YOUR CODE HERE\n",
    ")\n",
    "\n",
    "# Display split information\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check class distributions are preserved\n",
    "print(\"\\nClass distribution across splits:\")\n",
    "print(f\"Training:   {y_train.mean():.3f}\")\n",
    "print(f\"Validation: {y_val.mean():.3f}\")\n",
    "print(f\"Test:       {y_test.mean():.3f}\")\n",
    "print(f\"Original:   {y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Feature Scaling\n",
    "**Task**: Scale features for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform all sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = ______  # YOUR CODE HERE: Transform validation set\n",
    "X_test_scaled = ______  # YOUR CODE HERE: Transform test set\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"\\nExample scaling effect on 'annual_income':\")\n",
    "income_idx = list(X.columns).index('annual_income')\n",
    "print(f\"  Original: mean={X_train.iloc[:, income_idx].mean():.0f}, std={X_train.iloc[:, income_idx].std():.0f}\")\n",
    "print(f\"  Scaled:   mean={X_train_scaled[:, income_idx].mean():.3f}, std={X_train_scaled[:, income_idx].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Baseline Model - Logistic Regression\n",
    "\n",
    "Let's start with a basic logistic regression model.\n",
    "\n",
    "### Exercise 4.1: Train Basic Logistic Regression\n",
    "**Task**: Train and evaluate a baseline logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train baseline logistic regression\n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# YOUR CODE HERE: Fit the model on scaled training data\n",
    "______\n",
    "\n",
    "# Make predictions on all sets\n",
    "y_train_pred = baseline_model.predict(X_train_scaled)\n",
    "y_val_pred = ______  # YOUR CODE HERE\n",
    "y_test_pred = ______  # YOUR CODE HERE\n",
    "\n",
    "# Get prediction probabilities for ROC analysis\n",
    "y_train_proba = baseline_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_proba = ______  # YOUR CODE HERE: Get probabilities for validation set\n",
    "y_test_proba = ______  # YOUR CODE HERE: Get probabilities for test set\n",
    "\n",
    "print(\"Baseline logistic regression model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Evaluate with Classification Metrics\n",
    "**Task**: Calculate accuracy, precision, recall, F1-score, and AUC for all sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_model(y_true, y_pred, y_proba, set_name):\n",
    "    \"\"\"\n",
    "    Calculate and display comprehensive classification metrics.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = ______  # YOUR CODE HERE: Calculate recall\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    print(f\"{set_name} Set Metrics:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f} (of predicted approvals, what % were correct)\")\n",
    "    print(f\"  Recall:    {recall:.4f} (of actual approvals, what % were caught)\")\n",
    "    print(f\"  F1-Score:  {f1:.4f} (harmonic mean of precision and recall)\")\n",
    "    print(f\"  AUC-ROC:   {auc:.4f} (area under ROC curve)\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Evaluate baseline model\n",
    "print(\"BASELINE MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "train_metrics = evaluate_classification_model(y_train, y_train_pred, y_train_proba, \"Training\")\n",
    "print()\n",
    "val_metrics = evaluate_classification_model(______) # YOUR CODE HERE: Validation metrics\n",
    "print()\n",
    "test_metrics = evaluate_classification_model(______) # YOUR CODE HERE: Test metrics\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OVERFITTING ANALYSIS:\")\n",
    "train_acc, val_acc = train_metrics[0], val_metrics[0]\n",
    "print(f\"Training-Validation Accuracy Gap: {train_acc - val_acc:.4f}\")\n",
    "if abs(train_acc - val_acc) > 0.05:\n",
    "    print(\"⚠️  Potential overfitting detected\")\n",
    "else:\n",
    "    print(\"✅ Model appears well-generalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Confusion Matrix Analysis\n",
    "**Task**: Create and interpret confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, set_name, ax=None):\n",
    "    \"\"\"\n",
    "    Plot a formatted confusion matrix with business context.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Denied', 'Approved'],\n",
    "                yticklabels=['Denied', 'Approved'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(f'Confusion Matrix - {set_name} Set')\n",
    "    \n",
    "    # Add business context labels\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    ax.text(0.5, 0.1, f'True Negatives\\n(Correctly Denied)\\n{tn}', \n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=9)\n",
    "    ax.text(1.5, 0.1, f'False Positives\\n(Incorrectly Approved)\\n{fp}', \n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=9)\n",
    "    ax.text(0.5, 0.9, f'False Negatives\\n(Incorrectly Denied)\\n{fn}', \n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=9)\n",
    "    ax.text(1.5, 0.9, f'True Positives\\n(Correctly Approved)\\n{tp}', \n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=9)\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Create confusion matrices for all sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "cm_train = plot_confusion_matrix(y_train, y_train_pred, \"Training\", axes[0])\n",
    "cm_val = ______  # YOUR CODE HERE: Plot validation confusion matrix\n",
    "cm_test = ______  # YOUR CODE HERE: Plot test confusion matrix\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate business impact metrics\n",
    "tn, fp, fn, tp = cm_test.ravel()\n",
    "print(\"\\nBUSINESS IMPACT ANALYSIS (Test Set):\")\n",
    "print(\"=\"*40)\n",
    "print(f\"✅ Correct Approvals: {tp:,} (will generate revenue)\")\n",
    "print(f\"✅ Correct Denials: {tn:,} (avoided bad loans)\")\n",
    "print(f\"❌ Missed Opportunities: {fn:,} (denied good applicants)\")\n",
    "print(f\"💰 Potential Bad Loans: {fp:,} (approved risky applicants)\")\n",
    "print(f\"\\nError Rate: {(fp + fn) / len(y_test):.1%}\")\n",
    "print(f\"Approval Rate: {(tp + fp) / len(y_test):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Handle Class Imbalance\n",
    "\n",
    "Let's address class imbalance using different techniques.\n",
    "\n",
    "### Exercise 5.1: Class Weights\n",
    "**Task**: Train logistic regression with balanced class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=______,  # YOUR CODE HERE: unique classes\n",
    "    y=______         # YOUR CODE HERE: training target\n",
    ")\n",
    "class_weight_dict = dict(zip([0, 1], class_weights))\n",
    "\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "print(f\"This gives {class_weights[1]/class_weights[0]:.2f}x more weight to the minority class\")\n",
    "\n",
    "# Train balanced model\n",
    "balanced_model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "balanced_model.fit(______) # YOUR CODE HERE: Fit on scaled training data\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred_balanced = balanced_model.predict(X_val_scaled)\n",
    "y_val_proba_balanced = balanced_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "print(\"\\nBalanced model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Compare Model Performance\n",
    "**Task**: Compare baseline vs balanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models on validation set\n",
    "print(\"MODEL COMPARISON ON VALIDATION SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Baseline Model:\")\n",
    "evaluate_classification_model(y_val, y_val_pred, y_val_proba, \"Validation\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"Balanced Model:\")\n",
    "evaluate_classification_model(______) # YOUR CODE HERE: Evaluate balanced model\n",
    "\n",
    "# Compare confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "cm_baseline = confusion_matrix(y_val, y_val_pred)\n",
    "cm_balanced = confusion_matrix(______) # YOUR CODE HERE: Confusion matrix for balanced model\n",
    "\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Denied', 'Approved'], yticklabels=['Denied', 'Approved'])\n",
    "axes[0].set_title('Baseline Model')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(cm_balanced, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['Denied', 'Approved'], yticklabels=['Denied', 'Approved'])\n",
    "axes[1].set_title('Balanced Model')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate recall improvement\n",
    "baseline_recall = recall_score(y_val, y_val_pred)\n",
    "balanced_recall = ______  # YOUR CODE HERE: Calculate balanced model recall\n",
    "recall_improvement = balanced_recall - baseline_recall\n",
    "\n",
    "print(f\"\\nRecall Improvement: {recall_improvement:+.3f} ({recall_improvement/baseline_recall:+.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ROC and Precision-Recall Curves\n",
    "\n",
    "Let's create ROC and Precision-Recall curves to better understand model performance.\n",
    "\n",
    "### Exercise 6.1: ROC Curve Analysis\n",
    "**Task**: Plot and interpret ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create ROC curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ROC Curve\n",
    "fpr_baseline, tpr_baseline, _ = roc_curve(y_val, y_val_proba)\n",
    "fpr_balanced, tpr_balanced, _ = ______  # YOUR CODE HERE: ROC curve for balanced model\n",
    "\n",
    "# Plot ROC curves\n",
    "axes[0].plot(fpr_baseline, tpr_baseline, label=f'Baseline (AUC = {roc_auc_score(y_val, y_val_proba):.3f})', linewidth=2)\n",
    "axes[0].plot(fpr_balanced, tpr_balanced, label=f'Balanced (AUC = {roc_auc_score(y_val, y_val_proba_balanced):.3f})', linewidth=2)\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "axes[0].set_ylabel('True Positive Rate (Sensitivity/Recall)')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_baseline, recall_baseline, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "precision_balanced, recall_balanced, _ = ______  # YOUR CODE HERE: PR curve for balanced model\n",
    "\n",
    "# Plot PR curves\n",
    "baseline_ap = average_precision_score(y_val, y_val_proba)\n",
    "balanced_ap = ______  # YOUR CODE HERE: Average precision for balanced model\n",
    "\n",
    "axes[1].plot(recall_baseline, precision_baseline, label=f'Baseline (AP = {baseline_ap:.3f})', linewidth=2)\n",
    "axes[1].plot(recall_balanced, precision_balanced, label=f'Balanced (AP = {balanced_ap:.3f})', linewidth=2)\n",
    "axes[1].axhline(y=y_val.mean(), color='k', linestyle='--', label=f'Random (AP = {y_val.mean():.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"CURVE INTERPRETATION:\")\n",
    "print(\"=\"*30)\n",
    "print(\"ROC Curve:\")\n",
    "print(\"  • Shows trade-off between sensitivity and specificity\")\n",
    "print(\"  • Area Under Curve (AUC) ranges from 0.5 (random) to 1.0 (perfect)\")\n",
    "print(\"  • Good for balanced datasets\")\n",
    "print(\"\\nPrecision-Recall Curve:\")\n",
    "print(\"  • Shows trade-off between precision and recall\")\n",
    "print(\"  • More informative for imbalanced datasets\")\n",
    "print(\"  • Average Precision (AP) is area under PR curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Threshold Optimization\n",
    "\n",
    "Let's optimize the decision threshold based on business objectives.\n",
    "\n",
    "### Exercise 7.1: Business Cost Analysis\n",
    "**Task**: Define business costs and find optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define business costs\n",
    "COST_FALSE_POSITIVE = 50000  # Average loss from a bad loan\n",
    "COST_FALSE_NEGATIVE = 5000   # Opportunity cost of rejecting good applicant\n",
    "REVENUE_TRUE_POSITIVE = 15000  # Average profit from good loan\n",
    "COST_TRUE_NEGATIVE = 0       # No cost for correctly rejecting\n",
    "\n",
    "def calculate_business_value(y_true, y_pred, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculate total business value based on confusion matrix.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    total_value = (\n",
    "        tp * REVENUE_TRUE_POSITIVE +  # Revenue from good loans\n",
    "        tn * COST_TRUE_NEGATIVE +     # Cost of correct rejections (0)\n",
    "        fp * (-COST_FALSE_POSITIVE) + # Loss from bad loans\n",
    "        fn * (-COST_FALSE_NEGATIVE)   # Opportunity cost\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Business Value Breakdown:\")\n",
    "        print(f\"  Revenue (TP): {tp} × ${REVENUE_TRUE_POSITIVE:,} = ${tp * REVENUE_TRUE_POSITIVE:,}\")\n",
    "        print(f\"  Losses (FP):  {fp} × ${COST_FALSE_POSITIVE:,} = ${fp * COST_FALSE_POSITIVE:,}\")\n",
    "        print(f\"  Missed (FN):  {fn} × ${COST_FALSE_NEGATIVE:,} = ${fn * COST_FALSE_NEGATIVE:,}\")\n",
    "        print(f\"  Total Value: ${total_value:,}\")\n",
    "    \n",
    "    return total_value\n",
    "\n",
    "# Calculate current business value\n",
    "print(\"CURRENT BUSINESS VALUE ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "print(\"Baseline Model:\")\n",
    "baseline_value = calculate_business_value(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nBalanced Model:\")\n",
    "balanced_value = ______  # YOUR CODE HERE: Calculate balanced model value\n",
    "\n",
    "print(f\"\\nValue Difference: ${balanced_value - baseline_value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Optimize Decision Threshold\n",
    "**Task**: Find the threshold that maximizes business value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different thresholds to maximize business value\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "business_values = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Make predictions with custom threshold\n",
    "    y_pred_thresh = (y_val_proba_balanced >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    business_value = calculate_business_value(y_val, y_pred_thresh, verbose=False)\n",
    "    precision = precision_score(y_val, y_pred_thresh)\n",
    "    recall = ______  # YOUR CODE HERE: Calculate recall\n",
    "    f1 = f1_score(y_val, y_pred_thresh)\n",
    "    \n",
    "    business_values.append(business_value)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(business_values)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "max_business_value = business_values[optimal_idx]\n",
    "\n",
    "print(f\"OPTIMAL THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"Maximum Business Value: ${max_business_value:,}\")\n",
    "print(f\"Precision at optimal: {precisions[optimal_idx]:.3f}\")\n",
    "print(f\"Recall at optimal: {recalls[optimal_idx]:.3f}\")\n",
    "print(f\"F1-Score at optimal: {f1_scores[optimal_idx]:.3f}\")\n",
    "\n",
    "# Visualize threshold optimization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Business value vs threshold\n",
    "axes[0, 0].plot(thresholds, business_values, 'b-', linewidth=2)\n",
    "axes[0, 0].axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal: {optimal_threshold:.3f}')\n",
    "axes[0, 0].set_xlabel('Threshold')\n",
    "axes[0, 0].set_ylabel('Business Value ($)')\n",
    "axes[0, 0].set_title('Business Value vs Threshold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision vs threshold\n",
    "axes[0, 1].plot(thresholds, precisions, 'g-', linewidth=2)\n",
    "axes[0, 1].axvline(optimal_threshold, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Threshold')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].set_title('Precision vs Threshold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall vs threshold\n",
    "axes[1, 0].plot(thresholds, recalls, 'orange', linewidth=2)\n",
    "axes[1, 0].axvline(optimal_threshold, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Threshold')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].set_title('Recall vs Threshold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score vs threshold\n",
    "axes[1, 1].plot(thresholds, f1_scores, 'purple', linewidth=2)\n",
    "axes[1, 1].axvline(optimal_threshold, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Threshold')\n",
    "axes[1, 1].set_ylabel('F1-Score')\n",
    "axes[1, 1].set_title('F1-Score vs Threshold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Feature Importance and Model Interpretation\n",
    "\n",
    "Let's understand which features are most important for credit decisions.\n",
    "\n",
    "### Exercise 8.1: Logistic Regression Coefficients\n",
    "**Task**: Interpret model coefficients for business insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from logistic regression coefficients\n",
    "feature_names = X.columns\n",
    "coefficients = balanced_model.coef_[0]\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients),\n",
    "    'odds_ratio': np.exp(coefficients)  # Odds ratio interpretation\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"TOP 15 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Feature':<25} {'Coefficient':<12} {'Odds Ratio':<12} {'Impact'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    if row['coefficient'] > 0:\n",
    "        impact = \"↑ Increases approval odds\"\n",
    "    else:\n",
    "        impact = \"↓ Decreases approval odds\"\n",
    "    \n",
    "    print(f\"{row['feature']:<25} {row['coefficient']:>10.3f}   {row['odds_ratio']:>10.3f}   {impact}\")\n",
    "\n",
    "# TODO: Visualize top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_10 = feature_importance.head(10)\n",
    "colors = ['green' if coef > 0 else 'red' for coef in top_10['coefficient']]\n",
    "\n",
    "plt.barh(range(len(top_10)), top_10['coefficient'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_10)), top_10['feature'])\n",
    "plt.xlabel('Logistic Regression Coefficient')\n",
    "plt.title('Top 10 Feature Importance (Logistic Regression)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCOEFFICIENT INTERPRETATION:\")\n",
    "print(\"• Positive coefficient: Increases log-odds of approval\")\n",
    "print(\"• Negative coefficient: Decreases log-odds of approval\")\n",
    "print(\"• Odds Ratio > 1: Feature increases odds of approval\")\n",
    "print(\"• Odds Ratio < 1: Feature decreases odds of approval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2: Feature Importance with Random Forest\n",
    "**Task**: Compare with Random Forest feature importance for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train Random Forest for feature importance comparison\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(______) # YOUR CODE HERE: Fit on scaled training data\n",
    "\n",
    "# Get Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Compare top features from both models\n",
    "print(\"FEATURE IMPORTANCE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Feature':<25} {'Logistic':<10} {'RandomForest':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Merge the dataframes for comparison\n",
    "comparison = feature_importance[['feature', 'abs_coefficient']].merge(\n",
    "    rf_importance, on='feature', how='outer'\n",
    ").fillna(0)\n",
    "\n",
    "# Normalize for comparison\n",
    "comparison['logistic_norm'] = comparison['abs_coefficient'] / comparison['abs_coefficient'].max()\n",
    "comparison['rf_norm'] = comparison['importance'] / comparison['importance'].max()\n",
    "\n",
    "# Show top 10 by average importance\n",
    "comparison['avg_importance'] = (comparison['logistic_norm'] + comparison['rf_norm']) / 2\n",
    "top_features = comparison.sort_values('avg_importance', ascending=False).head(10)\n",
    "\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"{row['feature']:<25} {row['logistic_norm']:>8.3f}   {row['rf_norm']:>10.3f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(top_features))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], top_features['logistic_norm'], width, \n",
    "        label='Logistic Regression', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in x], top_features['rf_norm'], width, \n",
    "        label='Random Forest', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Normalized Importance')\n",
    "plt.title('Feature Importance Comparison')\n",
    "plt.xticks(x, top_features['feature'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Final Model Evaluation\n",
    "\n",
    "Let's evaluate our optimized model on the test set.\n",
    "\n",
    "### Exercise 9.1: Test Set Performance\n",
    "**Task**: Apply optimal threshold to test set and calculate final metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply optimal model to test set\n",
    "y_test_proba_final = balanced_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred_final = (y_test_proba_final >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calculate final performance metrics\n",
    "print(\"FINAL MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "final_metrics = evaluate_classification_model(\n",
    "    y_test, y_test_pred_final, y_test_proba_final, \"Test\"\n",
    ")\n",
    "\n",
    "# Calculate final business value\n",
    "print(\"\\nFINAL BUSINESS VALUE ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "final_business_value = calculate_business_value(y_test, y_test_pred_final)\n",
    "\n",
    "# Compare with baseline performance on test set\n",
    "y_test_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "baseline_test_value = calculate_business_value(y_test, y_test_pred_baseline, verbose=False)\n",
    "\n",
    "print(f\"\\nValue Improvement over Baseline: ${final_business_value - baseline_test_value:,}\")\n",
    "print(f\"Percentage Improvement: {((final_business_value - baseline_test_value) / abs(baseline_test_value)) * 100:+.1f}%\")\n",
    "\n",
    "# Create final confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_final = confusion_matrix(y_test, y_test_pred_final)\n",
    "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Denied', 'Approved'],\n",
    "            yticklabels=['Denied', 'Approved'])\n",
    "plt.title('Final Model - Test Set Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Add business context\n",
    "tn, fp, fn, tp = cm_final.ravel()\n",
    "plt.text(0.5, -0.15, f'Correctly Denied: {tn}\\nIncorrectly Approved: {fp}', \n",
    "         ha='center', va='top', transform=plt.gca().transAxes, fontsize=10)\n",
    "plt.text(1.5, -0.15, f'Incorrectly Denied: {fn}\\nCorrectly Approved: {tp}', \n",
    "         ha='center', va='top', transform=plt.gca().transAxes, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Business Recommendations and Deployment Considerations\n",
    "\n",
    "### Exercise 10.1: Generate Executive Summary\n",
    "**Task**: Create a comprehensive business report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive business report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECUREBANK CREDIT RISK ASSESSMENT - EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Model performance summary\n",
    "accuracy, precision, recall, f1, auc = final_metrics\n",
    "print(f\"\\n📊 MODEL PERFORMANCE METRICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Accuracy: {accuracy:.1%} (correctly classified applications)\")\n",
    "print(f\"• Precision: {precision:.1%} (of approved loans, % that are good)\")\n",
    "print(f\"• Recall: {recall:.1%} (of good applications, % we approved)\")\n",
    "print(f\"• AUC-ROC: {auc:.3f} (model discrimination ability)\")\n",
    "print(f\"• Optimal Threshold: {optimal_threshold:.3f} (custom business threshold)\")\n",
    "\n",
    "# Business impact\n",
    "print(f\"\\n💰 BUSINESS IMPACT\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Expected Value: ${final_business_value:,} (on test set)\")\n",
    "print(f\"• Improvement: ${final_business_value - baseline_test_value:,} over baseline\")\n",
    "print(f\"• Risk Reduction: {fp} potentially bad loans identified\")\n",
    "print(f\"• Revenue Protection: ${fp * COST_FALSE_POSITIVE:,} in potential losses avoided\")\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\n🔍 KEY RISK FACTORS\")\n",
    "print(\"-\" * 40)\n",
    "top_3_features = feature_importance.head(3)\n",
    "for idx, row in top_3_features.iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"• {row['feature']}: {direction} approval probability\")\n",
    "\n",
    "# Operational benefits\n",
    "print(f\"\\n⚡ OPERATIONAL BENEFITS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Automated Decision Making: Reduce review time from 5-7 days to minutes\")\n",
    "print(f\"• Consistent Criteria: Eliminate subjective decision variations\")\n",
    "print(f\"• Scalability: Handle increased application volume without proportional staff increase\")\n",
    "print(f\"• Compliance: Explainable AI features support regulatory requirements\")\n",
    "\n",
    "# Implementation recommendations\n",
    "print(f\"\\n🚀 IMPLEMENTATION RECOMMENDATIONS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"1. PHASE 1 - PILOT (Month 1-2)\")\n",
    "print(f\"   • Deploy model for 20% of applications\")\n",
    "print(f\"   • Human oversight for all model decisions\")\n",
    "print(f\"   • Monitor performance vs human decisions\")\n",
    "print(f\"\")\n",
    "print(f\"2. PHASE 2 - GRADUAL ROLLOUT (Month 3-4)\")\n",
    "print(f\"   • Increase to 50% of applications\")\n",
    "print(f\"   • Auto-approve obvious accepts (high confidence)\")\n",
    "print(f\"   • Human review for borderline cases\")\n",
    "print(f\"\")\n",
    "print(f\"3. PHASE 3 - FULL DEPLOYMENT (Month 5+)\")\n",
    "print(f\"   • 100% automated pre-screening\")\n",
    "print(f\"   • Human review only for edge cases\")\n",
    "print(f\"   • Continuous model monitoring and retraining\")\n",
    "\n",
    "# Monitoring and maintenance\n",
    "print(f\"\\n📈 MONITORING & MAINTENANCE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Model Performance: Track precision/recall monthly\")\n",
    "print(f\"• Data Drift: Monitor feature distributions for changes\")\n",
    "print(f\"• Business Metrics: Actual default rates vs predictions\")\n",
    "print(f\"• Retraining: Quarterly model updates with new data\")\n",
    "print(f\"• A/B Testing: Continuous threshold optimization\")\n",
    "\n",
    "# Risk mitigation\n",
    "print(f\"\\n⚠️  RISK MITIGATION\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Bias Monitoring: Regular fairness audits across demographic groups\")\n",
    "print(f\"• Explainability: Provide reasons for each decision\")\n",
    "print(f\"• Human Override: Maintain ability to override model decisions\")\n",
    "print(f\"• Backup Systems: Fallback to manual review if model fails\")\n",
    "print(f\"• Regulatory Compliance: Document all model changes and performance\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully completed a comprehensive classification analysis for credit risk assessment. \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Classification vs Regression**: Learned to predict categories rather than continuous values\n",
    "2. **Class Imbalance**: Addressed imbalanced datasets using class weights and custom thresholds\n",
    "3. **Multiple Metrics**: Used precision, recall, F1-score, and AUC for comprehensive evaluation\n",
    "4. **ROC and PR Curves**: Visualized model performance across different thresholds\n",
    "5. **Business Optimization**: Optimized decision threshold based on business costs and benefits\n",
    "6. **Feature Interpretation**: Understood which factors drive credit decisions\n",
    "7. **Model Comparison**: Compared different approaches and validation techniques\n",
    "\n",
    "### Skills Practiced:\n",
    "- Binary classification modeling\n",
    "- Handling imbalanced datasets\n",
    "- Logistic regression implementation and interpretation\n",
    "- Classification metrics calculation and interpretation\n",
    "- ROC and Precision-Recall curve analysis\n",
    "- Threshold optimization for business objectives\n",
    "- Feature importance analysis\n",
    "- Model deployment considerations\n",
    "- Business communication of technical results\n",
    "\n",
    "### Next Steps:\n",
    "In the next lab, we'll explore K-Nearest Neighbors for customer segmentation, learning about distance-based classification and the importance of feature scaling in instance-based learning algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}