<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 6: K-Nearest Neighbors & Distance Metrics</title>
    
    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/white.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/monokai.css">
    
    <!-- Common slide styles -->
    <link rel="stylesheet" href="../common-slides.css">
    
    <!-- Page-specific styles if needed -->
    <style>
        /* Week 6 specific styles */
        
        /* Math formulas */
        .math-formula {
            background: #f9f9f9;
            border: 1px solid #ddd;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
            text-align: center;
            font-size: 1.1em;
        }
        
        /* Distance metric comparison table */
        .distance-table {
            margin: 20px auto;
            border-collapse: collapse;
            font-size: 0.75em;
        }
        
        .distance-table th {
            background: linear-gradient(135deg, #1e3a5f 0%, #4a90e2 100%);
            color: white;
            font-weight: bold;
            padding: 10px;
            text-align: center;
        }
        
        .distance-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        
        .distance-table tr:nth-child(even) {
            background: #f9f9f9;
        }
        
        /* KNN visualization container */
        .knn-visual {
            text-align: center;
            margin: 20px 0;
        }
        
        .knn-visual img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        
        /* Metric cards for evaluation */
        .metric-card {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.05) 0%, rgba(118, 75, 162, 0.05) 100%);
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 15px;
            margin: 10px;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .metric-card h4 {
            color: #1e3a5f;
            margin: 0 0 10px 0;
            font-size: 0.9em;
        }
        
        .metric-card .formula {
            background: #f8f9fa;
            padding: 8px;
            border-radius: 4px;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.7em;
            margin: 10px 0;
        }
        
        /* Business case styling */
        .business-case {
            background: rgba(243, 156, 18, 0.05);
            border-left: 4px solid #f39c12;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }
        
        /* Highlight helper */
        .highlight {
            background-color: rgba(243, 156, 18, 0.3);
            padding: 2px 4px;
            border-radius: 2px;
        }
        
        /* Center helper */
        .center {
            text-align: center !important;
        }
        
        /* Small text helper */
        .small-text {
            font-size: 0.7em;
        }
        
        /* Code blocks in columns need special handling */
        .reveal .columns .column-50.code-column pre {
            max-height: 578px !important;
            font-size: 0.46em !important;
            margin: 0 10px !important;  /* Add horizontal margin for separation */
            width: calc(100% - 20px) !important;  /* Adjust width for margin */
            overflow-x: auto !important;
            overflow-y: auto !important;
            white-space: pre !important;
        }
        
        .reveal .columns .column-50.code-column pre code {
            max-height: 558px !important;
            overflow-x: visible !important;
            overflow-y: visible !important;
            white-space: pre !important;
        }
        
        /* Code blocks in standard columns - slightly larger font */
        .reveal .columns .column-50:not(.code-column) pre {
            max-height: 550px !important;
            font-size: 0.48em !important;
            margin: 0 10px !important;  /* Add horizontal margin for separation */
            width: calc(100% - 20px) !important;  /* Adjust width for margin */
            overflow-x: auto !important;
            overflow-y: auto !important;
        }
        
        /* Add margin for column-60 code blocks too */
        .reveal .columns .column-60.code-column pre {
            max-height: 578px !important;
            font-size: 0.46em !important;
            margin: 0 10px !important;
            width: calc(100% - 20px) !important;
            overflow-x: auto !important;
            overflow-y: auto !important;
            white-space: pre !important;
        }
        /* Copyright notice - appears on every slide */
        .reveal::after {
            content: "© Dr. T. Smith - Used by permission";
            position: fixed;
            bottom: 12px;
            left: 12px;
            font-size: 12px;
            color: #666;
            font-family: inherit;
            z-index: 31;
            opacity: 0.8;
        }
        
        /* Adjust to not overlap with slide numbers */
        .reveal .slide-number {
            right: 12px !important;
            left: auto !important;
        }

    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide and Overview Section -->
            <section>
                <section>
                    <h1>Week 6: K-Nearest Neighbors & Distance Metrics</h1>
                    <h3 class="center">Understanding Distance, Scale, and Classification Evaluation</h3>
                    
                    <div class="center" class="mt-50">
                        <p><strong>ISM 6251: Data Science Programming</strong></p>
                        <p>Fall 2025</p>
                    </div>
                    
                    <div class="info-box" class="mt-40">
                        <p class="center"><strong>Today's Focus:</strong> Distance metrics, KNN algorithm, feature scaling, and advanced classification evaluation</p>
                    </div>
                </section>

                <!-- Topic Overview Hierarchy -->
                <section>
                    <h2>Week 6: K-Nearest Neighbors & Distance Metrics - Topic Hierarchy</h2>
                    
                    <!-- Learning Path at the top -->
                    <div style="padding: 12px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 8px; margin-bottom: 15px;">
                        <p style="font-size: 1em; text-align: center; margin: 0; color: white; font-weight: bold;">
                            Learning Path: Distance Concepts → KNN Algorithm → Feature Scaling → Advanced Evaluation
                        </p>
                    </div>
                    
                    <!-- Two-column ASCII tree layout -->
                    <div class="hierarchy-container">
                        <!-- Left Column -->
                        <div class="hierarchy-column">
                            <pre class="hierarchy-pre">
<strong class="text-danger">K-NEAREST NEIGHBORS: INSTANCE-BASED LEARNING</strong>

<strong class="text-info">📚 Learning Objectives</strong>
├── Conceptual Understanding
│   ├── Distance in Feature Space
│   ├── Various Distance Metrics
│   ├── KNN Prediction Process
│   └── Importance of Scaling
└── Practical Implementation
    ├── KNN in scikit-learn
    ├── Choosing K Value
    ├── Feature Preprocessing
    └── Performance Evaluation

<strong style="color: #9b59b6;">📏 Distance Metrics Foundations</strong>
├── What is Distance in ML?
│   ├── Feature Space Concept
│   ├── Similarity Measurement
│   └── Geometric Interpretation
├── Common Distance Metrics
│   ├── Euclidean: L2 = √Σ(xi-yi)²
│   ├── Manhattan: L1 = Σ|xi-yi|
│   ├── Minkowski: Lp = (Σ|xi-yi|^p)^(1/p)
│   └── Cosine Similarity
├── Advanced Metrics
│   ├── Hamming Distance (categorical)
│   ├── Jaccard Similarity (sets)
│   └── Mahalanobis (covariance-weighted)
└── Python Implementation
    ├── Manual Calculations
    ├── scipy.spatial.distance
    └── sklearn.metrics.pairwise

<strong class="text-orange">🎯 K-Nearest Neighbors Algorithm</strong>
├── Algorithm Overview
│   ├── Instance-Based Learning
│   ├── Non-Parametric Method
│   ├── Lazy Learning Approach
│   └── Local Approximation
├── How KNN Works
│   ├── 1. Calculate Distances
│   ├── 2. Find K Nearest Neighbors
│   ├── 3. Vote/Average for Prediction
│   └── 4. Return Result
├── Mathematical Formulation
│   ├── Classification: Mode of K neighbors
│   ├── Regression: Mean of K neighbors
│   └── Weighted KNN: Distance-weighted votes
└── Visual Examples
    ├── K=1: Nearest neighbor only
    ├── K=3: Small neighborhood
    ├── K=7: Larger neighborhood
    └── Decision Boundaries
                            </pre>
                        </div>
                        
                        <!-- Right Column -->
                        <div class="hierarchy-column">
                            <pre class="hierarchy-pre">
<strong style="color: #16a085;">🎚️ Choosing K & Model Tuning</strong>
├── The Bias-Variance Tradeoff
│   ├── Small K → Low Bias, High Variance
│   ├── Large K → High Bias, Low Variance
│   └── Optimal K via Cross-Validation
├── Effect on Decision Boundaries
│   ├── K=1: Complex, Jagged Boundaries
│   ├── K=5: Smoother Boundaries
│   └── K=N: Linear Boundaries
└── Non-linear Relationships
    ├── No Assumptions Required
    ├── Captures Complex Patterns
    └── Local Decision Making

<strong style="color: #d35400;">⚖️ Feature Scaling</strong>
├── Why Scaling Matters
│   ├── <span class="highlight-warning">⚠️ Different Units Problem</span>
│   ├── Distance Domination
│   └── Equal Feature Importance
├── Standardization (Z-Score)
│   ├── Formula: z = (x - μ) / σ
│   ├── Mean = 0, Std = 1
│   ├── Preserves Distribution Shape
│   └── Good for Outliers
├── Min-Max Normalization
│   ├── Formula: x' = (x - min)/(max - min)
│   ├── Range: [0, 1]
│   ├── Preserves Zero Values
│   └── Sensitive to Outliers
└── Comparing Methods
    ├── Distribution Effects
    ├── Outlier Sensitivity
    └── Use Case Guidelines

<strong class="text-success">📊 Advanced Classification Evaluation</strong>
├── Multi-class Metrics
│   ├── Macro Average (equal weight)
│   ├── Micro Average (instance weight)
│   ├── Weighted Average (class weight)
│   └── Confusion Matrix (n×n)
├── ROC Curves
│   ├── TPR vs FPR Trade-off
│   ├── AUC Interpretation
│   ├── Multi-class Extensions
│   └── Threshold Selection
├── Precision-Recall Curves
│   ├── Better for Imbalanced Data
│   ├── Average Precision Score
│   └── F1 at Different Thresholds
└── Practical Implementation
    ├── sklearn.metrics Module
    ├── Cross-Validation Strategies
    └── Performance Visualization

<strong class="text-red">💼 Business Applications & Summary</strong>
├── Real-World Use Cases
│   ├── Recommendation Systems
│   ├── Image Recognition
│   ├── Anomaly Detection
│   └── Customer Segmentation
├── Advantages
│   ├── Simple & Intuitive
│   ├── No Training Phase
│   ├── Naturally Multi-class
│   └── Non-linear Boundaries
├── Limitations
│   ├── Curse of Dimensionality
│   ├── Computational Cost (O(n))
│   ├── Memory Intensive
│   └── Sensitive to Noise
└── Key Takeaways
    ├── Distance Metrics Matter
    ├── Always Scale Features
    ├── Choose K via CV
    └── Next: Decision Trees
                            </pre>
                        </div>
                    </div>
                </section>

                <!-- Learning Objectives -->
                <section>
                <h2>Learning Objectives</h2>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>Conceptual Understanding</h4>
                        <ul>
                            <li>Understand the concept of distance in feature space</li>
                            <li>Learn various distance metrics and their properties</li>
                            <li>Grasp how KNN makes predictions</li>
                            <li>Understand the importance of feature scaling</li>
                        </ul>
                    </div>
                    <div class="column-50">
                        <h4>Practical Skills</h4>
                        <ul>
                            <li>Implement KNN for classification and regression</li>
                            <li>Apply standardization and normalization</li>
                            <li>Evaluate classifiers using PR and ROC curves</li>
                            <li>Handle multi-class classification problems</li>
                        </ul>
                    </div>
                </div>
                
                <div class="business-case" class="mt-30">
                    <h5>Business Context</h5>
                    <p>KNN is widely used in recommendation systems, customer segmentation, and anomaly detection</p>
                </div>
                </section>
            </section>

            <!-- Section 1: The Concept of Distance -->
            <section>
                <section>
                    <h1>Section 1: The Concept of Distance</h1>
                    <h3 class="center">Measuring Similarity in Feature Space</h3>
                </section>

                <!-- What is Distance? -->
                <section>
                <h2>What is Distance in Machine Learning?</h2>
                
                <div class="info-box">
                    <p><strong>Distance</strong> quantifies how similar or dissimilar two data points are in feature space</p>
                </div>
                
                <div class="columns" class="mt-30">
                    <div class="column-60">
                        <h4>Key Concepts</h4>
                        <ul>
                            <li><strong>Feature Space:</strong> Multi-dimensional space where each dimension represents a feature</li>
                            <li><strong>Data Points:</strong> Observations represented as coordinates in feature space</li>
                            <li><strong>Distance Metric:</strong> Mathematical function to measure separation between points</li>
                            <li><strong>Similarity:</strong> Inverse relationship with distance (smaller distance = more similar)</li>
                        </ul>
                    </div>
                    <div class="column-40">
                        <h4>Business Applications</h4>
                        <div class="success-box">
                            <h5>Real-World Use Cases</h5>
                            <ul>
                                <li>Customer similarity for recommendations</li>
                                <li>Product matching in e-commerce</li>
                                <li>Fraud detection via anomaly distance</li>
                                <li>Document similarity in search engines</li>
                            </ul>
                        </div>
                    </div>
                </div>
                </section>

                <!-- Visual Distance Calculation -->
                <section>
                <h2>Visualizing Distance Calculations</h2>
                <h3>Euclidean vs Manhattan Distance</h3>
                
                <div class="knn-visual">
                    <img src="../images/week06-distance-calculation.svg" alt="Distance calculation visualization" class="img-85">
                </div>
                
                <div class="columns" class="mt-20">
                    <div class="column-50">
                        <div class="info-box">
                            <h5>Euclidean Distance</h5>
                            <ul class="small-text">
                                <li>Direct "as the crow flies" path</li>
                                <li>Uses Pythagorean theorem</li>
                                <li>Most intuitive for continuous space</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column-50">
                        <div class="warning-box">
                            <h5>Manhattan Distance</h5>
                            <ul class="small-text">
                                <li>Path along grid lines only</li>
                                <li>Sum of absolute differences</li>
                                <li>Better for discrete/grid-like features</li>
                            </ul>
                        </div>
                    </div>
                </div>
                </section>

                <!-- Distance Metrics -->
                <section>
                <h2>Common Distance Metrics</h2>
                <h3>Mathematical Foundations</h3>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>1. Euclidean Distance</h4>
                        <div class="math-formula">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mi>d</mi><mo>(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo>)</mo>
                                <mo>=</mo>
                                <msqrt>
                                    <munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover>
                                    <msup><mrow><mo>(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><msub><mi>q</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup>
                                </msqrt>
                            </math>
                        </div>
                        <ul class="small-text">
                            <li>Most common distance metric</li>
                            <li>"Straight line" distance</li>
                            <li>Sensitive to scale differences</li>
                        </ul>
                    </div>
                    <div class="column-50">
                        <h4>2. Manhattan Distance</h4>
                        <div class="math-formula">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mi>d</mi><mo>(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo>)</mo>
                                <mo>=</mo>
                                <munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover>
                                <mo>|</mo><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><msub><mi>q</mi><mi>i</mi></msub><mo>|</mo>
                            </math>
                        </div>
                        <ul class="small-text">
                            <li>Also called "City Block" distance</li>
                            <li>Sum of absolute differences</li>
                            <li>Robust to outliers</li>
                        </ul>
                    </div>
                </div>
                </section>

                <!-- More Distance Metrics -->
                <section>
                <h2>Advanced Distance Metrics</h2>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>3. Minkowski Distance</h4>
                        <div class="math-formula">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mi>d</mi><mo>(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo>)</mo>
                                <mo>=</mo>
                                <msup>
                                    <mrow>
                                        <mo>(</mo>
                                        <munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover>
                                        <msup><mrow><mo>|</mo><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><msub><mi>q</mi><mi>i</mi></msub><mo>|</mo></mrow><mi>r</mi></msup>
                                        <mo>)</mo>
                                    </mrow>
                                    <mfrac><mn>1</mn><mi>r</mi></mfrac>
                                </msup>
                            </math>
                        </div>
                        <p class="small-text">Generalization: r=1 (Manhattan), r=2 (Euclidean)</p>
                    </div>
                    <div class="column-50">
                        <h4>4. Cosine Similarity</h4>
                        <div class="math-formula">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mi>similarity</mi><mo>=</mo>
                                <mfrac>
                                    <mrow><mi>p</mi><mo>·</mo><mi>q</mi></mrow>
                                    <mrow><mo>||</mo><mi>p</mi><mo>||</mo><mo>||</mo><mi>q</mi><mo>||</mo></mrow>
                                </mfrac>
                            </math>
                        </div>
                        <p class="small-text">Measures angle between vectors (used in text analysis)</p>
                    </div>
                </div>
                
                <div class="warning-box" class="mt-20">
                    <h5>Choosing the Right Metric</h5>
                    <ul>
                        <li><strong>Euclidean:</strong> When actual distance matters (geographic data)</li>
                        <li><strong>Manhattan:</strong> Grid-like paths or when features are not comparable</li>
                        <li><strong>Cosine:</strong> Text analysis, when magnitude doesn't matter</li>
                    </ul>
                </div>
                </section>

                <!-- Distance Calculation Code Example -->
                <section>
                <h2>Computing Distances in Python</h2>
                <h3>Manual and Library Implementations</h3>
                
                <div class="columns">
                    <div class="column-50 code-column">
                        <h4>Manual Distance Calculations</h4>
                        <pre><code class="python">import numpy as np

# Sample points
point_a = np.array([1, 2])
point_b = np.array([4, 6])

# Euclidean Distance
euclidean = np.sqrt(np.sum((point_a - point_b)**2))
print(f"Euclidean: {euclidean:.2f}")
# Output: Euclidean: 5.00

# Manhattan Distance
manhattan = np.sum(np.abs(point_a - point_b))
print(f"Manhattan: {manhattan:.0f}")
# Output: Manhattan: 7

# Minkowski (p=3)
p = 3
minkowski = np.sum(np.abs(point_a - point_b)**p)**(1/p)
print(f"Minkowski (p=3): {minkowski:.2f}")
# Output: Minkowski (p=3): 4.50

# Cosine Similarity
cos_sim = np.dot(point_a, point_b) / (
    np.linalg.norm(point_a) * np.linalg.norm(point_b)
)
cos_dist = 1 - cos_sim  # Convert to distance
print(f"Cosine distance: {cos_dist:.3f}")
# Output: Cosine distance: 0.020</code></pre>
                    </div>
                    <div class="column-50 code-column">
                        <h4>Using Scikit-learn</h4>
                        <pre><code class="python">from scipy.spatial import distance
from sklearn.metrics.pairwise import (
    euclidean_distances,
    manhattan_distances,
    cosine_distances
)

# Multiple points
X = np.array([[1, 2], [4, 6], [3, 3]])

# Pairwise Euclidean distances
euc_matrix = euclidean_distances(X)
print("Euclidean distance matrix:")
print(euc_matrix.round(2))
# [[0.   5.   2.24]
#  [5.   0.   3.16]
#  [2.24 3.16 0.  ]]

# Using scipy for single pairs
point1, point2 = X[0], X[1]

# Various metrics
print(f"Euclidean: {distance.euclidean(point1, point2):.2f}")
print(f"Manhattan: {distance.cityblock(point1, point2):.0f}")
print(f"Chebyshev: {distance.chebyshev(point1, point2):.0f}")

# Custom distance function
def weighted_euclidean(x, y, weights):
    return np.sqrt(np.sum(weights * (x - y)**2))

weights = [2, 1]  # Weight feature 1 more
w_dist = weighted_euclidean(point1, point2, weights)
print(f"Weighted Euclidean: {w_dist:.2f}")</code></pre>
                    </div>
                </div>
                </section>
            </section>

            <!-- Section 2: K-Nearest Neighbors -->
            <section>
                <section>
                    <h1>Section 2: K-Nearest Neighbors Algorithm</h1>
                    <h3 class="center">Instance-Based Learning for Classification and Regression</h3>
                </section>

                <!-- KNN Introduction -->
                <section>
                <h2>K-Nearest Neighbors (KNN)</h2>
                <h3>Algorithm Overview</h3>
                
                <div class="info-box">
                    <p><strong>Core Principle:</strong> "Birds of a feather flock together" - Similar data points tend to have similar labels</p>
                </div>
                
                <div class="columns" class="mt-30">
                    <div class="column-60">
                        <h4>How KNN Works</h4>
                        <ol class="small-text">
                            <li>Store all training data (no explicit training phase)</li>
                            <li>For a new point, calculate distance to all training points</li>
                            <li>Select K nearest neighbors</li>
                            <li><strong>Classification:</strong> Vote among K neighbors (majority class)</li>
                            <li><strong>Regression:</strong> Average of K neighbors' values</li>
                        </ol>
                    </div>
                    <div class="column-40">
                        <h4>Key Characteristics</h4>
                        <div class="success-box">
                            <ul class="small-text">
                                <li><strong>Lazy Learning:</strong> No model training</li>
                                <li><strong>Non-parametric:</strong> No assumptions about data distribution</li>
                                <li><strong>Local:</strong> Predictions based on local neighborhood</li>
                                <li><strong>Versatile:</strong> Works for both classification and regression</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="danger-box" class="mt-20">
                    <p><strong>Note on Regression:</strong> While KNN can be used for regression, we'll focus on classification as regression evaluation is simpler (MSE, R²). Once you understand KNN classification, regression is straightforward - just average instead of vote!</p>
                </div>
                </section>

                <!-- KNN Classification Visualization -->
                <section>
                <h2>How KNN Makes Predictions</h2>
                <h3>Visual Example with Different K Values</h3>
                
                <div class="knn-visual">
                    <img src="../images/week06-knn-classification.svg" alt="KNN classification with different K values" class="img-90">
                </div>
                
                <div class="info-box" class="mt-20">
                    <p><strong>Key Observations:</strong></p>
                    <ul class="small-text">
                        <li><strong>K=1:</strong> Decision based on single nearest neighbor (can be noisy)</li>
                        <li><strong>K=3:</strong> Majority vote among 3 neighbors (more stable)</li>
                        <li><strong>K=7:</strong> Larger neighborhood considered (smoother decisions)</li>
                        <li>Green circle shows the distance to K-th neighbor</li>
                    </ul>
                </div>
                </section>

                <!-- KNN Algorithm Details -->
                <section>
                <h2>KNN Algorithm: Mathematical Formulation</h2>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>For Classification</h4>
                        <div class="math-formula">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mover><mi>y</mi><mo>^</mo></mover><mo>=</mo>
                                <munder><mi>mode</mi><mrow><mi>i</mi><mo>∈</mo><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></munder>
                                <mo>{</mo><msub><mi>y</mi><mi>i</mi></msub><mo>}</mo>
                            </math>
                        </div>
                        <p class="small-text">Where N<sub>k</sub>(x) are the k nearest neighbors of x</p>
                        
                        <h5 class="mt-20">Weighted KNN</h5>
                        <p class="small-text">Weight votes by inverse distance:</p>
                        <div class="math-formula" class="text-90">
                            <math xmlns="http://www.w3.org/1998/Math/MathML">
                                <msub><mi>w</mi><mi>i</mi></msub><mo>=</mo>
                                <mfrac><mn>1</mn><mrow><mi>d</mi><mo>(</mo><mi>x</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow></mfrac>
                            </math>
                        </div>
                    </div>
                    <div class="column-50">
                        <h4>For Regression</h4>
                        <div class="math-formula">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mover><mi>y</mi><mo>^</mo></mover><mo>=</mo>
                                <mfrac><mn>1</mn><mi>k</mi></mfrac>
                                <munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></munder>
                                <msub><mi>y</mi><mi>i</mi></msub>
                            </math>
                        </div>
                        <p class="small-text">Simple average of k nearest neighbors' values</p>
                        
                        <h5 class="mt-20">Distance-Weighted Average</h5>
                        <div class="math-formula" class="text-90">
                            <math xmlns="http://www.w3.org/1998/Math/MathML">
                                <mover><mi>y</mi><mo>^</mo></mover><mo>=</mo>
                                <mfrac>
                                    <mrow><mo>∑</mo><msub><mi>w</mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub></mrow>
                                    <mrow><mo>∑</mo><msub><mi>w</mi><mi>i</mi></msub></mrow>
                                </mfrac>
                            </math>
                        </div>
                    </div>
                </div>
                </section>

                <!-- Choosing K -->
                <section>
                <h2>Choosing the Value of K</h2>
                <h3>The Bias-Variance Tradeoff</h3>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>Small K (e.g., K=1)</h4>
                        <div class="info-box">
                            <ul>
                                <li><strong>Low bias:</strong> Can capture complex patterns</li>
                                <li><strong>High variance:</strong> Sensitive to noise</li>
                                <li>Prone to overfitting</li>
                                <li>Decision boundary is rough</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column-50">
                        <h4>Large K (e.g., K=n)</h4>
                        <div class="warning-box">
                            <ul>
                                <li><strong>High bias:</strong> Oversimplified model</li>
                                <li><strong>Low variance:</strong> Stable predictions</li>
                                <li>Prone to underfitting</li>
                                <li>Decision boundary is smooth</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="success-box" class="mt-30">
                    <h4>Best Practices for Choosing K</h4>
                    <ul>
                        <li><strong>Odd numbers for binary classification:</strong> Avoids ties (e.g., K=3, 5, 7)</li>
                        <li><strong>Cross-validation:</strong> Use CV to find optimal K</li>
                        <li><strong>Rule of thumb:</strong> K = √n (where n is number of training samples)</li>
                        <li><strong>Domain knowledge:</strong> Consider the problem context</li>
                    </ul>
                </div>
                </section>

                <!-- Effect of K on Decision Boundary -->
                <section>
                <h2>Effect of K on Complex, Overlapping Data</h2>
                <h3>Visualizing Decision Boundaries and Confidence</h3>
                
                <div class="knn-visual">
                    <img src="../images/week06-k-effect.svg" alt="Effect of K on decision boundary with complex data" class="full-width">
                </div>
                
                <div class="info-box" class="mt-15">
                    <p><strong>Key Insights from Complex Data:</strong></p>
                    <ul class="small-text">
                        <li><strong>K=1:</strong> Perfect training accuracy but poor generalization (overfitting to noise)</li>
                        <li><strong>K=3-7:</strong> Captures intricate patterns while maintaining some smoothness</li>
                        <li><strong>K=15:</strong> Smoother boundaries but may miss important local patterns</li>
                        <li>Bottom row shows prediction confidence - notice how larger K creates more confident (but potentially wrong) predictions</li>
                    </ul>
                </div>
                </section>

                <!-- K vs Accuracy Curve -->
                <section>
                <h2>Finding Optimal K: The Bias-Variance Tradeoff</h2>
                <h3>Training vs Cross-Validation Performance</h3>
                
                <div class="knn-visual">
                    <img src="../images/week06-k-accuracy-curve.svg" alt="K vs Accuracy curve" class="img-75">
                </div>
                
                <div class="columns" class="mt-20">
                    <div class="column-50">
                        <div class="danger-box">
                            <h5>Overfitting (Small K)</h5>
                            <ul class="small-text">
                                <li>High training accuracy</li>
                                <li>Poor cross-validation score</li>
                                <li>Large gap = overfitting</li>
                                <li>Model memorizes noise</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column-50">
                        <div class="success-box">
                            <h5>Optimal Region (K ≈ 5-15)</h5>
                            <ul class="small-text">
                                <li>Best CV performance</li>
                                <li>Smaller train-test gap</li>
                                <li>Good generalization</li>
                                <li>Balances bias and variance</li>
                            </ul>
                        </div>
                    </div>
                </div>
                </section>

                <!-- KNN Non-linear Relationships -->
                <section>
                <h2>KNN Captures Non-linear Relationships</h2>
                <h3>No Assumptions About Data Distribution</h3>
                
                <div class="knn-visual">
                    <img src="../images/week06-knn-nonlinear.svg" alt="KNN capturing non-linear boundaries" class="img-85">
                </div>
                
                <div class="columns" class="mt-20">
                    <div class="column-50">
                        <div class="success-box">
                            <h5>Advantages Over Linear Models</h5>
                            <ul class="small-text">
                                <li>No linearity assumption</li>
                                <li>Adapts to local data structure</li>
                                <li>Can model complex boundaries</li>
                                <li>Works for any shape distribution</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column-50">
                        <div class="danger-box">
                            <h5>Trade-offs</h5>
                            <ul class="small-text">
                                <li>K=1 perfectly fits training data (overfits)</li>
                                <li>Small K: captures complexity but noisy</li>
                                <li>Large K: smoother but may miss patterns</li>
                                <li>Computational cost increases with data</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="info-box" class="mt-20">
                    <p class="center"><strong>Key Insight:</strong> Unlike linear models (logistic regression, linear SVM), KNN makes no assumptions about the functional form of the decision boundary. It learns directly from the local structure of the data.</p>
                </div>
                </section>
            </section>

            <!-- Section 3: Feature Scaling -->
            <section>
                <section>
                    <h1>Section 3: The Importance of Scale</h1>
                    <h3 class="center">Standardization and Normalization</h3>
                </section>

                <!-- Why Scale Matters -->
                <section>
                <h2>Why Feature Scaling Matters</h2>
                <h3>The Problem of Different Scales</h3>
                
                <div class="danger-box">
                    <p><strong>Problem:</strong> Features with larger scales dominate distance calculations</p>
                </div>
                
                <div class="columns" class="mt-30">
                    <div class="column-50">
                        <h4>Example: Customer Data</h4>
                        <table>
                            <tr>
                                <th>Feature</th>
                                <th>Range</th>
                                <th>Scale</th>
                            </tr>
                            <tr>
                                <td>Age (years)</td>
                                <td>18-80</td>
                                <td>~10¹</td>
                            </tr>
                            <tr>
                                <td>Income ($)</td>
                                <td>20,000-200,000</td>
                                <td>~10⁵</td>
                            </tr>
                            <tr>
                                <td>Purchase frequency</td>
                                <td>1-12</td>
                                <td>~10¹</td>
                            </tr>
                        </table>
                        <p class="small-text mt-10">Income will dominate distance calculations!</p>
                    </div>
                    <div class="column-50">
                        <h4>Impact on KNN</h4>
                        <div class="warning-box">
                            <p class="small-text"><strong>Without scaling:</strong></p>
                            <ul class="small-text">
                                <li>Distance mainly determined by income</li>
                                <li>Age and frequency become irrelevant</li>
                                <li>Poor model performance</li>
                                <li>Biased neighbor selection</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="info-box" class="mt-20">
                    <p><strong>Solution:</strong> Transform features to comparable scales before computing distances</p>
                </div>
                </section>

                <!-- Scaling Code Example -->
                <section>
                <h2>Feature Scaling in Practice</h2>
                <h3>Comparing Unscaled vs Scaled KNN Performance</h3>
                
                <pre style="max-height: 550px; overflow-y: auto;"><code class="python">import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification

# Create dataset with different scales
X, y = make_classification(n_samples=500, n_features=3, n_informative=3, 
                         n_redundant=0, random_state=42)

# Artificially scale features differently
X[:, 0] *= 1000    # Feature 1: range 0-1000s
X[:, 1] *= 0.01    # Feature 2: range 0.01-0.1  
X[:, 2] *= 50      # Feature 3: range 0-100s

print("Feature ranges (before scaling):")
for i in range(3):
    print(f"Feature {i+1}: [{X[:, i].min():.2f}, {X[:, i].max():.2f}]")

# Test KNN without scaling
knn_unscaled = KNeighborsClassifier(n_neighbors=5)
scores_unscaled = cross_val_score(knn_unscaled, X, y, cv=5)
print(f"\nWithout scaling - Accuracy: {scores_unscaled.mean():.3f} (+/- {scores_unscaled.std():.3f})")

# Test with different scaling methods
scalers = {
    'StandardScaler': StandardScaler(),
    'MinMaxScaler': MinMaxScaler(),
    'RobustScaler': RobustScaler()
}

print("\nWith scaling:")
for name, scaler in scalers.items():
    X_scaled = scaler.fit_transform(X)
    knn_scaled = KNeighborsClassifier(n_neighbors=5)
    scores = cross_val_score(knn_scaled, X_scaled, y, cv=5)
    print(f"{name:15} - Accuracy: {scores.mean():.3f} (+/- {scores.std():.3f})")
    
# Show feature ranges after StandardScaler
scaler = StandardScaler()
X_standard = scaler.fit_transform(X)
print("\nFeature statistics after StandardScaler:")
for i in range(3):
    print(f"Feature {i+1}: mean={X_standard[:, i].mean():.3f}, std={X_standard[:, i].std():.3f}")

# Demonstrating the importance of fitting on training data only
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# CORRECT way
scaler = StandardScaler()
scaler.fit(X_train)  # Fit only on training data
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Use same parameters for test

print("\n✓ Correct scaling - Test set statistics:")
print(f"Mean: {X_test_scaled.mean(axis=0).round(3)}")
print(f"Std:  {X_test_scaled.std(axis=0).round(3)}")
print("(Note: Test set mean ≠ 0, std ≠ 1 - this is expected!)")</code></pre>
                </section>

                <!-- Standardization -->
                <section>
                <h2>Standardization (Z-Score Normalization)</h2>
                <h3>Transforming to Standard Normal Distribution</h3>
                
                <div class="columns">
                    <div class="column-60">
                        <h4>Mathematical Formula</h4>
                        <div class="math-formula">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mi>z</mi><mo>=</mo>
                                <mfrac>
                                    <mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow>
                                    <mi>σ</mi>
                                </mfrac>
                            </math>
                        </div>
                        <p>Where:</p>
                        <ul class="small-text">
                            <li><strong>x:</strong> Original value</li>
                            <li><strong>μ:</strong> Mean of the feature</li>
                            <li><strong>σ:</strong> Standard deviation of the feature</li>
                            <li><strong>z:</strong> Standardized value</li>
                        </ul>
                    </div>
                    <div class="column-40">
                        <h4>Properties</h4>
                        <div class="success-box">
                            <ul class="small-text">
                                <li>Mean = 0</li>
                                <li>Standard deviation = 1</li>
                                <li>Preserves shape of distribution</li>
                                <li>No bounded range</li>
                                <li>Handles outliers well</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="columns" class="mt-30">
                    <div class="column-50">
                        <h4>Python Implementation</h4>
                        <pre><code class="python">from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Manual calculation
mean = X_train.mean(axis=0)
std = X_train.std(axis=0)
X_scaled = (X - mean) / std</code></pre>
                    </div>
                    <div class="column-50">
                        <h4>When to Use</h4>
                        <div class="info-box">
                            <ul class="small-text">
                                <li>Features are normally distributed</li>
                                <li>Outliers are meaningful</li>
                                <li>Algorithm assumes normal distribution</li>
                                <li>Distance-based algorithms (KNN, SVM)</li>
                            </ul>
                        </div>
                    </div>
                </div>
                </section>

                <!-- Visualization of Standardization -->
                <section>
                <h2>Standardization: Visual Example</h2>
                <h3>Histogram Transformation with Z-Scores</h3>
                
                <div class="knn-visual">
                    <img src="../images/week06-standardization-histogram.svg" alt="Standardization histogram example" style="width: 90%; margin: 0 auto;">
                </div>
                
                <div class="columns" class="mt-20">
                    <div class="column-50">
                        <h4>Before Standardization</h4>
                        <ul class="small-text">
                            <li>Original scale (e.g., income: $20k-$200k)</li>
                            <li>Mean ≠ 0</li>
                            <li>Standard deviation ≠ 1</li>
                            <li>Different features have different ranges</li>
                        </ul>
                    </div>
                    <div class="column-50">
                        <h4>After Standardization</h4>
                        <ul class="small-text">
                            <li>Z-score scale (typically -3 to +3)</li>
                            <li>Mean = 0 (centered)</li>
                            <li>Standard deviation = 1</li>
                            <li>All features on same scale</li>
                        </ul>
                    </div>
                </div>
                
                <div class="info-box" class="mt-20">
                    <p><strong>Interpretation:</strong> Z-score tells us how many standard deviations away from the mean a value is</p>
                    <ul class="small-text">
                        <li>z = 0: At the mean</li>
                        <li>z = 1: One standard deviation above mean</li>
                        <li>z = -2: Two standard deviations below mean</li>
                    </ul>
                </div>
                </section>

                <!-- Normalization -->
                <section>
                <h2>Min-Max Normalization</h2>
                <h3>Scaling to a Fixed Range</h3>
                
                <div class="columns">
                    <div class="column-60">
                        <h4>Mathematical Formula</h4>
                        <div class="math-formula">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <msub><mi>x</mi><mi>norm</mi></msub><mo>=</mo>
                                <mfrac>
                                    <mrow><mi>x</mi><mo>−</mo><msub><mi>x</mi><mi>min</mi></msub></mrow>
                                    <mrow><msub><mi>x</mi><mi>max</mi></msub><mo>−</mo><msub><mi>x</mi><mi>min</mi></msub></mrow>
                                </mfrac>
                            </math>
                        </div>
                        <p>Scales values to [0, 1] range</p>
                        
                        <h5 class="mt-20">General Range [a, b]</h5>
                        <div class="math-formula" class="text-90">
                            <math xmlns="http://www.w3.org/1998/Math/MathML">
                                <msub><mi>x</mi><mi>scaled</mi></msub><mo>=</mo>
                                <mi>a</mi><mo>+</mo>
                                <mfrac>
                                    <mrow><mo>(</mo><mi>x</mi><mo>−</mo><msub><mi>x</mi><mi>min</mi></msub><mo>)</mo><mo>(</mo><mi>b</mi><mo>−</mo><mi>a</mi><mo>)</mo></mrow>
                                    <mrow><msub><mi>x</mi><mi>max</mi></msub><mo>−</mo><msub><mi>x</mi><mi>min</mi></msub></mrow>
                                </mfrac>
                            </math>
                        </div>
                    </div>
                    <div class="column-40">
                        <h4>Properties</h4>
                        <div class="success-box">
                            <ul class="small-text">
                                <li>Bounded range [0, 1]</li>
                                <li>Preserves zero values</li>
                                <li>Preserves relationships</li>
                                <li>Sensitive to outliers</li>
                                <li>Simple interpretation</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="columns" class="mt-30">
                    <div class="column-50">
                        <h4>Python Implementation</h4>
                        <pre><code class="python">from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_norm = scaler.fit_transform(X_train)
X_test_norm = scaler.transform(X_test)

# Manual calculation
X_min = X_train.min(axis=0)
X_max = X_train.max(axis=0)
X_norm = (X - X_min) / (X_max - X_min)</code></pre>
                    </div>
                    <div class="column-50">
                        <h4>When to Use</h4>
                        <div class="info-box">
                            <ul class="small-text">
                                <li>Features have known bounds</li>
                                <li>Data is uniformly distributed</li>
                                <li>Need values in specific range</li>
                                <li>Neural networks (often prefer [0,1])</li>
                            </ul>
                        </div>
                    </div>
                </div>
                </section>

                <!-- Comparison of Scaling Methods -->
                <section>
                <h2>Comparing Scaling Methods</h2>
                
                <table class="distance-table">
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>Standardization</th>
                            <th>Min-Max Normalization</th>
                            <th>Robust Scaling</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Formula</strong></td>
                            <td>(x - μ) / σ</td>
                            <td>(x - min) / (max - min)</td>
                            <td>(x - median) / IQR</td>
                        </tr>
                        <tr>
                            <td><strong>Range</strong></td>
                            <td>Unbounded (~-3 to 3)</td>
                            <td>[0, 1]</td>
                            <td>Unbounded</td>
                        </tr>
                        <tr>
                            <td><strong>Outlier Sensitivity</strong></td>
                            <td>Moderate</td>
                            <td>High</td>
                            <td>Low</td>
                        </tr>
                        <tr>
                            <td><strong>Preserves Distribution</strong></td>
                            <td>Yes</td>
                            <td>Yes</td>
                            <td>Yes</td>
                        </tr>
                        <tr>
                            <td><strong>Best For</strong></td>
                            <td>Normal distributions</td>
                            <td>Bounded data</td>
                            <td>Data with outliers</td>
                        </tr>
                        <tr>
                            <td><strong>Use Cases</strong></td>
                            <td>KNN, SVM, PCA</td>
                            <td>Neural Networks</td>
                            <td>Robust models</td>
                        </tr>
                    </tbody>
                </table>
                
                <div class="danger-box" class="mt-20">
                    <h5>Critical Rule</h5>
                    <p>Always fit the scaler on training data only, then transform both training and test data</p>
                    <pre><code class="python"># CORRECT
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# WRONG - causes data leakage
scaler.fit(X_all)  # Don't fit on test data!</code></pre>
                </div>
                </section>
            </section>

            <!-- Section 4: Classification Evaluation -->
            <section>
                <section>
                    <h1>Section 4: Advanced Classification Evaluation</h1>
                    <h3 class="center">PR Curves, ROC Curves, and Multi-class Metrics</h3>
                </section>

                <!-- Confusion Matrix Review -->
                <section>
                <h2>Multi-class Classification Metrics</h2>
                <h3>Beyond Binary Classification</h3>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>Multi-class Confusion Matrix</h4>
                        <table class="distance-table" class="text-70">
                            <tr>
                                <th></th>
                                <th colspan="3">Predicted</th>
                            </tr>
                            <tr>
                                <th>Actual</th>
                                <th>Class A</th>
                                <th>Class B</th>
                                <th>Class C</th>
                            </tr>
                            <tr>
                                <td><strong>Class A</strong></td>
                                <td class="bg-success-light">45</td>
                                <td class="bg-danger-light">3</td>
                                <td class="bg-danger-light">2</td>
                            </tr>
                            <tr>
                                <td><strong>Class B</strong></td>
                                <td class="bg-danger-light">5</td>
                                <td class="bg-success-light">38</td>
                                <td class="bg-danger-light">7</td>
                            </tr>
                            <tr>
                                <td><strong>Class C</strong></td>
                                <td class="bg-danger-light">1</td>
                                <td class="bg-danger-light">4</td>
                                <td class="bg-success-light">45</td>
                            </tr>
                        </table>
                    </div>
                    <div class="column-50">
                        <h4>Per-Class Metrics</h4>
                        <div class="info-box">
                            <p class="small-text"><strong>For each class:</strong></p>
                            <ul class="small-text">
                                <li><strong>Precision:</strong> TP / (TP + FP)</li>
                                <li><strong>Recall:</strong> TP / (TP + FN)</li>
                                <li><strong>F1-Score:</strong> 2 × (Precision × Recall) / (Precision + Recall)</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="success-box" class="mt-20">
                    <h4>Averaging Strategies</h4>
                    <ul class="small-text">
                        <li><strong>Micro-average:</strong> Calculate metrics globally (treats all instances equally)</li>
                        <li><strong>Macro-average:</strong> Calculate metrics for each class, then average (treats all classes equally)</li>
                        <li><strong>Weighted-average:</strong> Weight by class support (number of instances)</li>
                    </ul>
                </div>
                </section>

                <!-- Classification Metrics Code -->
                <section>
                <h2>Implementing Classification Evaluation</h2>
                <h3>ROC Curves, PR Curves, and Multi-class Metrics</h3>
                
                <pre style="max-height: 580px; overflow-y: auto;"><code class="python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    roc_curve, auc, precision_recall_curve, average_precision_score,
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    roc_auc_score, precision_recall_fscore_support
)
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier

# Binary Classification Example
# Assume we have y_test (true labels) and y_proba (predicted probabilities)

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# PR Curve  
precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)
pr_auc = average_precision_score(y_test, y_proba)

# Plotting both curves
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# ROC Curve
axes[0].plot(fpr, tpr, 'b-', label=f'ROC (AUC = {roc_auc:.3f})')
axes[0].plot([0, 1], [0, 1], 'r--', label='Random')
axes[0].set_xlabel('False Positive Rate')
axes[0].set_ylabel('True Positive Rate')
axes[0].set_title('ROC Curve')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# PR Curve
axes[1].plot(recall, precision, 'g-', label=f'PR (AUC = {pr_auc:.3f})')
axes[1].axhline(y=y_test.mean(), color='r', linestyle='--', label='Baseline')
axes[1].set_xlabel('Recall')
axes[1].set_ylabel('Precision')
axes[1].set_title('Precision-Recall Curve')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(ax=axes[2], cmap='Blues')
axes[2].set_title('Confusion Matrix')

plt.tight_layout()
plt.show()

# Multi-class Classification (3+ classes)
if len(np.unique(y_test)) > 2:
    # Binarize labels for multi-class ROC
    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
    n_classes = y_test_bin.shape[1]
    
    # Compute ROC curve for each class
    fpr_multi = dict()
    tpr_multi = dict()
    roc_auc_multi = dict()
    
    for i in range(n_classes):
        fpr_multi[i], tpr_multi[i], _ = roc_curve(y_test_bin[:, i], y_proba_multi[:, i])
        roc_auc_multi[i] = auc(fpr_multi[i], tpr_multi[i])
    
    # Plot all ROC curves
    plt.figure(figsize=(8, 6))
    colors = ['blue', 'red', 'green', 'orange', 'purple']
    for i, color in zip(range(n_classes), colors):
        plt.plot(fpr_multi[i], tpr_multi[i], color=color, 
                label=f'Class {i} (AUC = {roc_auc_multi[i]:.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Multi-class ROC Curves')
    plt.legend(loc='lower right')
    plt.show()
    
    # Classification report with per-class metrics
    print("\nDetailed Classification Report:")
    print(classification_report(y_test, y_pred, target_names=[f'Class {i}' for i in range(n_classes)]))
    
    # Per-class precision, recall, f1
    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)
    for i in range(n_classes):
        print(f"\nClass {i}:")
        print(f"  Precision: {precision[i]:.3f}")
        print(f"  Recall:    {recall[i]:.3f}")
        print(f"  F1-Score:  {f1[i]:.3f}")
        print(f"  Support:   {support[i]}")</code></pre>
                </section>

                <!-- ROC Curves -->
                <section>
                <h2>ROC Curves</h2>
                <h3>Receiver Operating Characteristic</h3>
                
                <div class="columns">
                    <div class="column-60">
                        <h4>What is an ROC Curve?</h4>
                        <ul class="small-text">
                            <li>Plots True Positive Rate vs False Positive Rate</li>
                            <li>Shows performance across all classification thresholds</li>
                            <li>Area Under Curve (AUC) summarizes performance</li>
                            <li>Diagonal line represents random classifier</li>
                        </ul>
                        
                        <div class="math-formula" class="mt-20">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mi>TPR</mi><mo>=</mo><mfrac><mi>TP</mi><mrow><mi>TP</mi><mo>+</mo><mi>FN</mi></mrow></mfrac>
                                <mspace width="2em"/>
                                <mi>FPR</mi><mo>=</mo><mfrac><mi>FP</mi><mrow><mi>FP</mi><mo>+</mo><mi>TN</mi></mrow></mfrac>
                            </math>
                        </div>
                    </div>
                    <div class="column-40">
                        <h4>Interpreting AUC</h4>
                        <div class="info-box">
                            <ul class="small-text">
                                <li><strong>AUC = 1.0:</strong> Perfect classifier</li>
                                <li><strong>AUC = 0.9-1.0:</strong> Excellent</li>
                                <li><strong>AUC = 0.8-0.9:</strong> Good</li>
                                <li><strong>AUC = 0.7-0.8:</strong> Fair</li>
                                <li><strong>AUC = 0.5:</strong> Random guess</li>
                                <li><strong>AUC < 0.5:</strong> Worse than random</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="knn-visual">
                    <img src="../images/week06-roc-curve.svg" alt="ROC Curve example" style="width: 60%;">
                </div>
                </section>

                <!-- PR Curves -->
                <section>
                <h2>Precision-Recall Curves</h2>
                <h3>Better for Imbalanced Datasets</h3>
                
                <div class="columns">
                    <div class="column-60">
                        <h4>What is a PR Curve?</h4>
                        <ul class="small-text">
                            <li>Plots Precision vs Recall</li>
                            <li>More informative for imbalanced datasets</li>
                            <li>Area Under PR Curve (AUPRC) summarizes performance</li>
                            <li>Baseline is positive class prevalence (not 0.5)</li>
                        </ul>
                        
                        <div class="math-formula" class="mt-20">
                            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                                <mi>Precision</mi><mo>=</mo><mfrac><mi>TP</mi><mrow><mi>TP</mi><mo>+</mo><mi>FP</mi></mrow></mfrac>
                                <mspace width="2em"/>
                                <mi>Recall</mi><mo>=</mo><mfrac><mi>TP</mi><mrow><mi>TP</mi><mo>+</mo><mi>FN</mi></mrow></mfrac>
                            </math>
                        </div>
                    </div>
                    <div class="column-40">
                        <h4>When to Use Each</h4>
                        <div class="warning-box">
                            <p class="small-text"><strong>Use ROC when:</strong></p>
                            <ul class="small-text">
                                <li>Classes are balanced</li>
                                <li>False positives and false negatives equally important</li>
                            </ul>
                            <p class="small-text mt-10"><strong>Use PR when:</strong></p>
                            <ul class="small-text">
                                <li>Classes are imbalanced</li>
                                <li>Positive class is rare</li>
                                <li>Focus on positive class performance</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="knn-visual">
                    <img src="../images/week06-pr-curve.svg" alt="PR Curve example" style="width: 60%;">
                </div>
                </section>

                <!-- Comparing ROC and PR -->
                <section>
                <h2>ROC vs PR Curves: Practical Example</h2>
                <h3>Fraud Detection Scenario</h3>
                
                <div class="danger-box">
                    <p><strong>Business Context:</strong> Credit card fraud detection - 0.1% of transactions are fraudulent</p>
                </div>
                
                <div class="columns" class="mt-30">
                    <div class="column-50">
                        <h4>ROC Curve Analysis</h4>
                        <img src="../images/week06-roc-fraud.svg" alt="ROC for fraud detection" class="full-width">
                        <ul class="small-text">
                            <li>AUC = 0.95 (looks excellent!)</li>
                            <li>But doesn't show class imbalance impact</li>
                            <li>May be misleading for business decisions</li>
                        </ul>
                    </div>
                    <div class="column-50">
                        <h4>PR Curve Analysis</h4>
                        <img src="../images/week06-pr-fraud.svg" alt="PR for fraud detection" class="full-width">
                        <ul class="small-text">
                            <li>AUPRC = 0.65 (more realistic)</li>
                            <li>Shows precision drops quickly</li>
                            <li>Better for setting operating threshold</li>
                        </ul>
                    </div>
                </div>
                
                <div class="info-box" class="mt-20">
                    <p><strong>Business Insight:</strong> Even with 95% AUC, at 80% recall (catching 80% of fraud), precision might be only 20% (4 false alarms for every real fraud)</p>
                </div>
                </section>
            </section>

            <!-- Section 5: Implementation and Applications -->
            <section>
                <!-- Implementation Code -->
                <section>
                <h2>Python Implementation</h2>
                <h3>Complete KNN Pipeline with Evaluation</h3>
                
                <pre class="max-height-650 scrollable"><code class="python">import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, precision_recall_curve,
                            average_precision_score)
import matplotlib.pyplot as plt

# Load and split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Find optimal K using cross-validation
k_values = range(1, 31, 2)  # Odd numbers from 1 to 29
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='f1_weighted')
    cv_scores.append(scores.mean())

optimal_k = k_values[np.argmax(cv_scores)]
print(f"Optimal K: {optimal_k}")

# Train final model
knn_final = KNeighborsClassifier(n_neighbors=optimal_k, metric='euclidean')
knn_final.fit(X_train_scaled, y_train)

# Predictions
y_pred = knn_final.predict(X_test_scaled)
y_proba = knn_final.predict_proba(X_test_scaled)[:, 1]  # For binary classification

# Evaluation
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# PR Curve
precision, recall, _ = precision_recall_curve(y_test, y_proba)
pr_auc = average_precision_score(y_test, y_proba)

# Plotting
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# ROC Curve
ax1.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
ax1.plot([0, 1], [0, 1], 'k--', label='Random')
ax1.set_xlabel('False Positive Rate')
ax1.set_ylabel('True Positive Rate')
ax1.set_title('ROC Curve')
ax1.legend()

# PR Curve
ax2.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.2f})')
ax2.set_xlabel('Recall')
ax2.set_ylabel('Precision')
ax2.set_title('Precision-Recall Curve')
ax2.legend()

plt.tight_layout()
plt.show()</code></pre>
                </section>

                <!-- Business Applications -->
                <section>
                <h2>Business Applications of KNN</h2>
                <h3>Real-World Use Cases</h3>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>1. Recommendation Systems</h4>
                        <div class="info-box">
                            <p class="small-text"><strong>Collaborative Filtering</strong></p>
                            <ul class="small-text">
                                <li>Find K most similar users</li>
                                <li>Recommend items they liked</li>
                                <li>Netflix, Amazon, Spotify</li>
                            </ul>
                            <pre class="text-90"><code class="python"># User-based collaborative filtering
similar_users = knn.kneighbors(user_profile)
recommendations = aggregate_preferences(similar_users)</code></pre>
                        </div>
                    </div>
                    <div class="column-50">
                        <h4>2. Customer Segmentation</h4>
                        <div class="success-box">
                            <p class="small-text"><strong>Marketing Campaigns</strong></p>
                            <ul class="small-text">
                                <li>Identify similar customer groups</li>
                                <li>Target marketing efforts</li>
                                <li>Predict customer lifetime value</li>
                            </ul>
                            <pre class="text-90"><code class="python"># Customer similarity
new_customer_segment = knn.predict(new_customer_features)
campaign_type = segment_campaigns[new_customer_segment]</code></pre>
                        </div>
                    </div>
                </div>
                
                <div class="columns" class="mt-20">
                    <div class="column-50">
                        <h4>3. Anomaly Detection</h4>
                        <div class="warning-box">
                            <p class="small-text"><strong>Fraud Detection</strong></p>
                            <ul class="small-text">
                                <li>Identify unusual transactions</li>
                                <li>Distance to neighbors indicates anomaly</li>
                                <li>Real-time fraud prevention</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column-50">
                        <h4>4. Image Recognition</h4>
                        <div class="danger-box">
                            <p class="small-text"><strong>Visual Search</strong></p>
                            <ul class="small-text">
                                <li>Find similar products from images</li>
                                <li>Face recognition systems</li>
                                <li>Medical image diagnosis</li>
                            </ul>
                        </div>
                    </div>
                </div>
                </section>

                <!-- Advantages and Disadvantages -->
                <section>
                <h2>KNN: Advantages and Limitations</h2>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>✓ Advantages</h4>
                        <div class="success-box">
                            <ul>
                                <li><strong>Simple and intuitive:</strong> Easy to understand and explain</li>
                                <li><strong>No training phase:</strong> Instant model updates</li>
                                <li><strong>Non-parametric:</strong> No assumptions about data</li>
                                <li><strong>Versatile:</strong> Works for classification and regression</li>
                                <li><strong>Multi-class natural:</strong> Handles multiple classes easily</li>
                                <li><strong>Local patterns:</strong> Captures local structure well</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column-50">
                        <h4>✗ Limitations</h4>
                        <div class="danger-box">
                            <ul>
                                <li><strong>Computationally expensive:</strong> O(n) for each prediction</li>
                                <li><strong>Memory intensive:</strong> Stores all training data</li>
                                <li><strong>Curse of dimensionality:</strong> Poor with high dimensions</li>
                                <li><strong>Sensitive to scale:</strong> Requires feature scaling</li>
                                <li><strong>Sensitive to noise:</strong> Outliers affect predictions</li>
                                <li><strong>Imbalanced data:</strong> Majority class bias</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="info-box" class="mt-30">
                    <h4>Optimization Techniques</h4>
                    <ul class="small-text">
                        <li><strong>KD-Trees / Ball Trees:</strong> Speed up neighbor search for low dimensions</li>
                        <li><strong>Approximate methods:</strong> LSH (Locality Sensitive Hashing) for high dimensions</li>
                        <li><strong>Feature selection:</strong> Reduce dimensionality to improve performance</li>
                        <li><strong>Distance weighting:</strong> Give closer neighbors more influence</li>
                    </ul>
                </div>
                </section>
            </section>

            <!-- Summary and Next Week -->
            <section>
                <!-- Summary -->
                <section>
                <h2>Summary: Key Takeaways</h2>
                
                <div class="columns">
                    <div class="column-50">
                        <h4>Distance Metrics</h4>
                        <ul>
                            <li>Euclidean for continuous features</li>
                            <li>Manhattan for grid-like data</li>
                            <li>Cosine for text/high-dimensional data</li>
                            <li>Choice affects model performance</li>
                        </ul>
                        
                        <h4 class="mt-20">Feature Scaling</h4>
                        <ul>
                            <li>Essential for distance-based algorithms</li>
                            <li>Standardization for normal distributions</li>
                            <li>Normalization for bounded ranges</li>
                            <li>Always fit on training data only</li>
                        </ul>
                    </div>
                    <div class="column-50">
                        <h4>KNN Algorithm</h4>
                        <ul>
                            <li>Simple but powerful lazy learner</li>
                            <li>K selection via cross-validation</li>
                            <li>Works for classification and regression</li>
                            <li>Sensitive to curse of dimensionality</li>
                        </ul>
                        
                        <h4 class="mt-20">Evaluation Metrics</h4>
                        <ul>
                            <li>Multi-class confusion matrices</li>
                            <li>ROC curves for balanced data</li>
                            <li>PR curves for imbalanced data</li>
                            <li>Choose metrics based on business needs</li>
                        </ul>
                    </div>
                </div>
                
                <div class="warning-box" class="mt-30">
                    <p class="center"><strong>Remember:</strong> KNN's performance heavily depends on proper feature scaling and choosing the right distance metric for your data!</p>
                </div>
                </section>

                <!-- Practice & Next Steps -->
                <section>
                <h2>Practice Exercises & Next Steps</h2>
                <div class="columns">
                    <div class="column-50">
                        <h4>This Week's Assignment</h4>
                        <div class="info-box">
                            <strong>Product Recommendation with KNN</strong><br>
                            Build a KNN-based recommendation system:
                            <ul style="font-size: 0.9em; margin-top: 10px;">
                                <li>Implement user-based collaborative filtering</li>
                                <li>Compare different distance metrics</li>
                                <li>Experiment with K values (3, 5, 10, 20)</li>
                                <li>Handle sparse user-item matrices</li>
                                <li>Evaluate with precision@k and recall@k</li>
                                <li>Visualize recommendation quality</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column-50">
                        <h4>Next Week: Support Vector Machines</h4>
                        <div class="success-box">
                            <strong>Preview of Topics:</strong>
                            <ul style="font-size: 0.9em; margin-top: 10px;">
                                <li>Maximum margin classification</li>
                                <li>The kernel trick for non-linear boundaries</li>
                                <li>Handling class imbalance with class weights</li>
                                <li>SVM for regression (SVR)</li>
                                <li>Comparison with KNN and other classifiers</li>
                            </ul>
                        </div>
                        
                        <h4>Resources</h4>
                        <ul class="small-text">
                            <li>"Pattern Recognition" - Duda, Hart & Stork</li>
                            <li>Scikit-learn KNN documentation</li>
                            <li>"The Elements of Statistical Learning" - Ch. 13</li>
                        </ul>
                    </div>
                </div>
                </section>
            </section>
        </div>
    </div>

    <!-- Reveal.js Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/markdown/markdown.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/math/math.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/zoom/zoom.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/search/search.js"></script>
    
    <script>
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: false,
            slideNumber: 'c/t',
            transition: 'slide',
            backgroundTransition: 'fade',
            width: 1280,
            height: 720,
            margin: 0.05,
            
            plugins: [ 
                RevealMarkdown, 
                RevealHighlight, 
                RevealNotes,
                RevealZoom,
                RevealSearch,
                RevealMath.KaTeX
            ]
        });
    </script>
</body>
</html>