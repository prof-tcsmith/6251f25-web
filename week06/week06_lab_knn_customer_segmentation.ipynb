{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Lab: K-Nearest Neighbors for Customer Segmentation\n",
    "\n",
    "## Business Scenario\n",
    "\n",
    "You've been hired as a lead data scientist at **TrendMart**, a rapidly growing e-commerce company with over 100,000 active customers. The marketing team is struggling with several key challenges:\n",
    "\n",
    "1. **Generic Marketing**: Current campaigns treat all customers the same, leading to low engagement rates\n",
    "2. **Resource Waste**: Marketing budget is spread thin across ineffective broad campaigns\n",
    "3. **Customer Churn**: Unable to identify at-risk customers before they leave\n",
    "4. **Product Recommendations**: Poor personalization leads to low conversion rates\n",
    "\n",
    "The marketing director wants to implement **targeted customer segmentation** to:\n",
    "- **Identify customer types** based on purchasing behavior and demographics\n",
    "- **Personalize marketing campaigns** for each customer segment\n",
    "- **Optimize marketing spend** by focusing on high-value customer groups\n",
    "- **Improve customer retention** through targeted interventions\n",
    "\n",
    "Your task is to build a K-Nearest Neighbors classification system that can:\n",
    "- Classify customers into meaningful segments\n",
    "- Handle mixed data types (numeric and categorical)\n",
    "- Provide interpretable results for marketing strategy\n",
    "- Scale to handle new customers in real-time\n",
    "\n",
    "## Learning Objectives\n",
    "By completing this lab, you will:\n",
    "- Understand the K-Nearest Neighbors algorithm and its applications\n",
    "- Learn different distance metrics (Euclidean, Manhattan, Minkowski)\n",
    "- Master the critical importance of feature scaling in KNN\n",
    "- Optimize the k parameter through validation techniques\n",
    "- Visualize decision boundaries and algorithm behavior\n",
    "- Handle multi-class classification problems\n",
    "- Compare KNN performance across different scenarios\n",
    "- Create actionable business insights from model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Generation\n",
    "\n",
    "First, let's import necessary libraries and generate synthetic customer data that reflects real e-commerce patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, validation_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, \n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.spatial.distance import euclidean, manhattan, minkowski\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Available distance metrics: Euclidean, Manhattan, Minkowski\")\n",
    "print(f\"KNN is an instance-based, lazy learning algorithm\")\n",
    "print(f\"Key consideration: Feature scaling is CRITICAL for KNN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Customer Data\n",
    "\n",
    "We'll create realistic customer data with features that naturally form clusters:\n",
    "- **Demographics**: Age, income, location\n",
    "- **Behavior**: Purchase frequency, average order value, browsing time\n",
    "- **Preferences**: Category preferences, brand loyalty, price sensitivity\n",
    "- **Engagement**: Email opens, social media activity, review participation\n",
    "\n",
    "We'll create 4 distinct customer segments:\n",
    "1. **Budget Shoppers**: Price-sensitive, low spending\n",
    "2. **Premium Buyers**: High income, luxury preferences\n",
    "3. **Frequent Shoppers**: Regular purchases, moderate spending\n",
    "4. **Occasional Buyers**: Infrequent but targeted purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_customer_data(n_samples=2000):\n",
    "    \"\"\"\n",
    "    Generate synthetic customer data with distinct segments.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create customer segments with different characteristics\n",
    "    segment_names = ['Budget_Shoppers', 'Premium_Buyers', 'Frequent_Shoppers', 'Occasional_Buyers']\n",
    "    n_per_segment = n_samples // 4\n",
    "    \n",
    "    all_customers = []\n",
    "    \n",
    "    for i, segment in enumerate(segment_names):\n",
    "        if segment == 'Budget_Shoppers':\n",
    "            # Budget-conscious customers\n",
    "            age = np.random.normal(35, 8, n_per_segment)\n",
    "            income = np.random.normal(35000, 8000, n_per_segment)\n",
    "            avg_order_value = np.random.normal(25, 8, n_per_segment)\n",
    "            purchase_frequency = np.random.normal(2, 1, n_per_segment)  # purchases per month\n",
    "            browsing_time = np.random.normal(15, 5, n_per_segment)  # minutes per session\n",
    "            price_sensitivity = np.random.normal(8, 1, n_per_segment)  # scale 1-10\n",
    "            brand_loyalty = np.random.normal(4, 1.5, n_per_segment)  # scale 1-10\n",
    "            email_engagement = np.random.normal(3, 1, n_per_segment)  # opens per month\n",
    "            social_activity = np.random.normal(2, 1, n_per_segment)  # interactions per month\n",
    "            \n",
    "        elif segment == 'Premium_Buyers':\n",
    "            # High-income, luxury-focused customers\n",
    "            age = np.random.normal(45, 10, n_per_segment)\n",
    "            income = np.random.normal(85000, 15000, n_per_segment)\n",
    "            avg_order_value = np.random.normal(150, 40, n_per_segment)\n",
    "            purchase_frequency = np.random.normal(4, 1.5, n_per_segment)\n",
    "            browsing_time = np.random.normal(25, 8, n_per_segment)\n",
    "            price_sensitivity = np.random.normal(3, 1, n_per_segment)\n",
    "            brand_loyalty = np.random.normal(8, 1, n_per_segment)\n",
    "            email_engagement = np.random.normal(8, 2, n_per_segment)\n",
    "            social_activity = np.random.normal(5, 2, n_per_segment)\n",
    "            \n",
    "        elif segment == 'Frequent_Shoppers':\n",
    "            # Regular, moderate-spending customers\n",
    "            age = np.random.normal(30, 8, n_per_segment)\n",
    "            income = np.random.normal(55000, 12000, n_per_segment)\n",
    "            avg_order_value = np.random.normal(75, 20, n_per_segment)\n",
    "            purchase_frequency = np.random.normal(8, 2, n_per_segment)\n",
    "            browsing_time = np.random.normal(30, 10, n_per_segment)\n",
    "            price_sensitivity = np.random.normal(6, 1.5, n_per_segment)\n",
    "            brand_loyalty = np.random.normal(6, 1.5, n_per_segment)\n",
    "            email_engagement = np.random.normal(12, 3, n_per_segment)\n",
    "            social_activity = np.random.normal(8, 3, n_per_segment)\n",
    "            \n",
    "        else:  # Occasional_Buyers\n",
    "            # Infrequent but targeted purchases\n",
    "            age = np.random.normal(40, 12, n_per_segment)\n",
    "            income = np.random.normal(60000, 15000, n_per_segment)\n",
    "            avg_order_value = np.random.normal(120, 30, n_per_segment)\n",
    "            purchase_frequency = np.random.normal(1, 0.5, n_per_segment)\n",
    "            browsing_time = np.random.normal(45, 15, n_per_segment)\n",
    "            price_sensitivity = np.random.normal(5, 2, n_per_segment)\n",
    "            brand_loyalty = np.random.normal(7, 2, n_per_segment)\n",
    "            email_engagement = np.random.normal(6, 2, n_per_segment)\n",
    "            social_activity = np.random.normal(3, 2, n_per_segment)\n",
    "        \n",
    "        # Clip values to realistic ranges\n",
    "        age = np.clip(age, 18, 70)\n",
    "        income = np.clip(income, 20000, 200000)\n",
    "        avg_order_value = np.clip(avg_order_value, 10, 500)\n",
    "        purchase_frequency = np.clip(purchase_frequency, 0.5, 15)\n",
    "        browsing_time = np.clip(browsing_time, 5, 120)\n",
    "        price_sensitivity = np.clip(price_sensitivity, 1, 10)\n",
    "        brand_loyalty = np.clip(brand_loyalty, 1, 10)\n",
    "        email_engagement = np.clip(email_engagement, 0, 20)\n",
    "        social_activity = np.clip(social_activity, 0, 15)\n",
    "        \n",
    "        # Create categorical features\n",
    "        if segment == 'Budget_Shoppers':\n",
    "            preferred_category = np.random.choice(['Essentials', 'Home', 'Books'], n_per_segment, p=[0.5, 0.3, 0.2])\n",
    "            location_type = np.random.choice(['Urban', 'Suburban', 'Rural'], n_per_segment, p=[0.3, 0.5, 0.2])\n",
    "        elif segment == 'Premium_Buyers':\n",
    "            preferred_category = np.random.choice(['Electronics', 'Fashion', 'Luxury'], n_per_segment, p=[0.3, 0.4, 0.3])\n",
    "            location_type = np.random.choice(['Urban', 'Suburban', 'Rural'], n_per_segment, p=[0.6, 0.3, 0.1])\n",
    "        elif segment == 'Frequent_Shoppers':\n",
    "            preferred_category = np.random.choice(['Fashion', 'Electronics', 'Health'], n_per_segment, p=[0.4, 0.3, 0.3])\n",
    "            location_type = np.random.choice(['Urban', 'Suburban', 'Rural'], n_per_segment, p=[0.5, 0.4, 0.1])\n",
    "        else:  # Occasional_Buyers\n",
    "            preferred_category = np.random.choice(['Electronics', 'Luxury', 'Sports'], n_per_segment, p=[0.4, 0.3, 0.3])\n",
    "            location_type = np.random.choice(['Urban', 'Suburban', 'Rural'], n_per_segment, p=[0.4, 0.5, 0.1])\n",
    "        \n",
    "        # Create segment dataframe\n",
    "        segment_data = pd.DataFrame({\n",
    "            'age': age,\n",
    "            'annual_income': income,\n",
    "            'avg_order_value': avg_order_value,\n",
    "            'purchase_frequency': purchase_frequency,\n",
    "            'browsing_time': browsing_time,\n",
    "            'price_sensitivity': price_sensitivity,\n",
    "            'brand_loyalty': brand_loyalty,\n",
    "            'email_engagement': email_engagement,\n",
    "            'social_activity': social_activity,\n",
    "            'preferred_category': preferred_category,\n",
    "            'location_type': location_type,\n",
    "            'customer_segment': segment\n",
    "        })\n",
    "        \n",
    "        all_customers.append(segment_data)\n",
    "    \n",
    "    # Combine all segments\n",
    "    df = pd.concat(all_customers, ignore_index=True)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_customer_data(2000)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nCustomer segment distribution:\")\n",
    "print(df['customer_segment'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis\n",
    "\n",
    "Before building our KNN model, let's understand the data structure and segment characteristics.\n",
    "\n",
    "### Exercise 2.1: Basic Data Exploration\n",
    "**Task**: Explore the dataset structure and segment distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display basic dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "# YOUR CODE HERE: Display dataset info and summary statistics\n",
    "______\n",
    "\n",
    "print(\"\\nMissing Values Check:\")\n",
    "# YOUR CODE HERE: Check for missing values\n",
    "______\n",
    "\n",
    "print(\"\\nNumeric Feature Statistics by Segment:\")\n",
    "numeric_features = ['age', 'annual_income', 'avg_order_value', 'purchase_frequency', \n",
    "                   'browsing_time', 'price_sensitivity', 'brand_loyalty', \n",
    "                   'email_engagement', 'social_activity']\n",
    "\n",
    "# Display mean values by segment for key features\n",
    "segment_stats = df.groupby('customer_segment')[numeric_features[:4]].mean().round(1)\n",
    "print(segment_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Visualize Segment Characteristics\n",
    "**Task**: Create visualizations to understand how segments differ across key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive segment analysis visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Key features to visualize\n",
    "key_features = ['annual_income', 'avg_order_value', 'purchase_frequency', \n",
    "                'browsing_time', 'price_sensitivity', 'brand_loyalty']\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    # Create box plots for each feature by segment\n",
    "    df.boxplot(column=feature, by='customer_segment', ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Customer Segment')\n",
    "    axes[i].set_xlabel('Customer Segment')\n",
    "    axes[i].set_ylabel(feature)\n",
    "    # Rotate x-axis labels for better readability\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Create a scatter plot matrix for key features\n",
    "print(\"\\nFeature Relationships (Scatter Plot Matrix):\")\n",
    "# Create scatter plot for income vs order value, colored by segment\n",
    "plt.figure(figsize=(12, 8))\n",
    "for segment in df['customer_segment'].unique():\n",
    "    segment_data = df[df['customer_segment'] == segment]\n",
    "    plt.scatter(segment_data['annual_income'], segment_data['avg_order_value'], \n",
    "               label=segment, alpha=0.6, s=50)\n",
    "\n",
    "plt.xlabel('Annual Income ($)')\n",
    "plt.ylabel('Average Order Value ($)')\n",
    "plt.title('Customer Segments: Income vs Order Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# TODO: Analyze categorical feature distributions\n",
    "print(\"\\nCategorical Feature Analysis:\")\n",
    "# YOUR CODE HERE: Create crosstab for preferred_category vs customer_segment\n",
    "category_analysis = ______\n",
    "print(\"Preferred Category by Segment:\")\n",
    "print(category_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Feature Scaling Analysis\n",
    "**Task**: Examine why feature scaling is critical for KNN by comparing feature ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the importance of feature scaling for KNN\n",
    "print(\"FEATURE SCALING ANALYSIS - Why it's CRITICAL for KNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display feature ranges\n",
    "feature_ranges = df[numeric_features].agg(['min', 'max', 'mean', 'std']).round(2)\n",
    "feature_ranges.loc['range'] = feature_ranges.loc['max'] - feature_ranges.loc['min']\n",
    "\n",
    "print(\"Raw Feature Statistics:\")\n",
    "print(feature_ranges)\n",
    "\n",
    "# TODO: Calculate distance between two sample customers without scaling\n",
    "customer1 = df.iloc[0][numeric_features].values\n",
    "customer2 = df.iloc[1][numeric_features].values\n",
    "\n",
    "print(f\"\\nDistance Calculation Example (Unscaled):\")\n",
    "print(f\"Customer 1: {customer1}\")\n",
    "print(f\"Customer 2: {customer2}\")\n",
    "print(f\"Feature differences: {np.abs(customer1 - customer2)}\")\n",
    "\n",
    "# Calculate Euclidean distance\n",
    "unscaled_distance = euclidean(customer1, customer2)\n",
    "print(f\"Unscaled Euclidean distance: {unscaled_distance:.2f}\")\n",
    "\n",
    "# Show which features dominate\n",
    "squared_diffs = (customer1 - customer2) ** 2\n",
    "print(f\"\\nSquared differences by feature:\")\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    contribution = squared_diffs[i] / sum(squared_diffs) * 100\n",
    "    print(f\"  {feature:20s}: {squared_diffs[i]:8.1f} ({contribution:5.1f}% of total)\")\n",
    "\n",
    "print(f\"\\n⚠️  PROBLEM: Features with larger scales (like annual_income) dominate the distance!\")\n",
    "print(f\"   This means KNN will primarily classify based on income, ignoring other features.\")\n",
    "print(f\"   SOLUTION: Feature scaling makes all features contribute equally to distances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Preprocessing\n",
    "\n",
    "Proper preprocessing is crucial for KNN success.\n",
    "\n",
    "### Exercise 3.1: Handle Categorical Variables\n",
    "**Task**: Encode categorical variables for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for KNN\n",
    "df_processed = df.copy()\n",
    "\n",
    "# TODO: One-hot encode categorical variables\n",
    "categorical_features = ['preferred_category', 'location_type']\n",
    "\n",
    "print(\"Encoding categorical variables...\")\n",
    "for feature in categorical_features:\n",
    "    print(f\"  {feature}: {df[feature].nunique()} unique values\")\n",
    "    # YOUR CODE HERE: Create dummy variables for categorical features\n",
    "    encoded_features = ______\n",
    "    df_processed = pd.concat([df_processed, encoded_features], axis=1)\n",
    "\n",
    "# Drop original categorical columns\n",
    "df_processed = df_processed.drop(categorical_features, axis=1)\n",
    "\n",
    "print(f\"\\nOriginal features: {df.shape[1]}\")\n",
    "print(f\"After encoding: {df_processed.shape[1]}\")\n",
    "print(f\"New feature columns:\")\n",
    "new_columns = set(df_processed.columns) - set(df.columns)\n",
    "for col in sorted(new_columns):\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Create Train/Test Split\n",
    "**Task**: Split data while maintaining segment proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_processed.drop('customer_segment', axis=1)\n",
    "y = df_processed['customer_segment']\n",
    "\n",
    "# TODO: Create stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=______, random_state=42, stratify=______  # YOUR CODE HERE: 20% test, stratify by target\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(\"\\nSegment distribution (should be similar):\")\n",
    "print(\"Training:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nTest:\")\n",
    "print(y_test.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Feature Scaling Comparison\n",
    "**Task**: Compare different scaling methods and their impact on KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare different scaling methods\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler()\n",
    "}\n",
    "\n",
    "scaled_data = {}\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    # YOUR CODE HERE: Fit scaler on training data and transform both sets\n",
    "    X_train_scaled = ______\n",
    "    X_test_scaled = ______\n",
    "    \n",
    "    scaled_data[scaler_name] = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "# Compare scaling effects\n",
    "print(\"SCALING COMPARISON:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show effect on first few features\n",
    "sample_features = ['age', 'annual_income', 'avg_order_value']\n",
    "sample_indices = [list(X.columns).index(feat) for feat in sample_features]\n",
    "\n",
    "print(f\"Original data (first 3 samples, selected features):\")\n",
    "print(X_train.iloc[:3][sample_features])\n",
    "\n",
    "for scaler_name, data in scaled_data.items():\n",
    "    print(f\"\\n{scaler_name} scaled data:\")\n",
    "    sample_scaled = data['X_train'][:3, sample_indices]\n",
    "    sample_df = pd.DataFrame(sample_scaled, columns=sample_features)\n",
    "    print(sample_df)\n",
    "\n",
    "# TODO: Calculate distance comparison\n",
    "print(f\"\\nDistance comparison between first two customers:\")\n",
    "customer1_orig = X_train.iloc[0].values\n",
    "customer2_orig = X_train.iloc[1].values\n",
    "orig_distance = euclidean(customer1_orig, customer2_orig)\n",
    "print(f\"  Unscaled: {orig_distance:.2f}\")\n",
    "\n",
    "for scaler_name, data in scaled_data.items():\n",
    "    customer1_scaled = data['X_train'][0]\n",
    "    customer2_scaled = data['X_train'][1]\n",
    "    scaled_distance = euclidean(customer1_scaled, customer2_scaled)\n",
    "    print(f\"  {scaler_name}: {scaled_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Understanding Distance Metrics\n",
    "\n",
    "KNN uses distance metrics to find nearest neighbors. Let's explore different metrics.\n",
    "\n",
    "### Exercise 4.1: Distance Metrics Comparison\n",
    "**Task**: Compare Euclidean, Manhattan, and Minkowski distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler data for distance metric comparison\n",
    "X_train_scaled = scaled_data['StandardScaler']['X_train']\n",
    "X_test_scaled = scaled_data['StandardScaler']['X_test']\n",
    "\n",
    "# TODO: Calculate different distances between sample customers\n",
    "customer1 = X_train_scaled[0]\n",
    "customer2 = X_train_scaled[1]\n",
    "customer3 = X_train_scaled[2]\n",
    "\n",
    "print(\"DISTANCE METRICS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Comparing distances between Customer 1 and others...\\n\")\n",
    "\n",
    "customers = [('Customer 2', customer2), ('Customer 3', customer3)]\n",
    "\n",
    "for name, customer in customers:\n",
    "    print(f\"Customer 1 vs {name}:\")\n",
    "    \n",
    "    # Calculate different distance metrics\n",
    "    euclidean_dist = euclidean(customer1, customer)\n",
    "    manhattan_dist = ______  # YOUR CODE HERE: Calculate Manhattan distance\n",
    "    minkowski_dist_p3 = minkowski(customer1, customer, p=3)\n",
    "    \n",
    "    print(f\"  Euclidean (L2):   {euclidean_dist:.4f}\")\n",
    "    print(f\"  Manhattan (L1):   {manhattan_dist:.4f}\")\n",
    "    print(f\"  Minkowski (p=3):  {minkowski_dist_p3:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize distance metric effects\n",
    "print(\"Distance Metric Characteristics:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"• Euclidean (L2): Standard straight-line distance\")\n",
    "print(\"  - Sensitive to outliers\")\n",
    "print(\"  - Works well with continuous features\")\n",
    "print(\"\\n• Manhattan (L1): Sum of absolute differences\")\n",
    "print(\"  - More robust to outliers\")\n",
    "print(\"  - Good for high-dimensional data\")\n",
    "print(\"\\n• Minkowski (Lp): Generalization of both\")\n",
    "print(\"  - p=1: Manhattan, p=2: Euclidean\")\n",
    "print(\"  - Higher p values reduce outlier sensitivity\")\n",
    "\n",
    "# TODO: Test KNN with different distance metrics\n",
    "print(\"\\nKNN Performance with Different Distance Metrics:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "distance_metrics = {\n",
    "    'euclidean': 'Euclidean',\n",
    "    'manhattan': 'Manhattan',\n",
    "    'minkowski': 'Minkowski (p=3)'\n",
    "}\n",
    "\n",
    "for metric, name in distance_metrics.items():\n",
    "    if metric == 'minkowski':\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, metric=metric, p=3)\n",
    "    else:\n",
    "        knn = ______  # YOUR CODE HERE: Create KNN with appropriate metric\n",
    "    \n",
    "    # Fit and evaluate\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    accuracy = knn.score(X_test_scaled, y_test)\n",
    "    print(f\"{name:20s}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: K Parameter Optimization\n",
    "\n",
    "The choice of k (number of neighbors) is crucial for KNN performance.\n",
    "\n",
    "### Exercise 5.1: K Value Analysis\n",
    "**Task**: Find the optimal k value using validation curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different k values\n",
    "k_values = list(range(1, 31, 2))  # Test odd values from 1 to 29\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"K VALUE OPTIMIZATION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Testing k values: {k_values[:5]}...{k_values[-5:]}\")\n",
    "\n",
    "for k in k_values:\n",
    "    # YOUR CODE HERE: Create and train KNN with different k values\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn.fit(______) # Fit on scaled training data\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc = knn.score(X_train_scaled, y_train)\n",
    "    test_acc = ______  # YOUR CODE HERE: Calculate test accuracy\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Find optimal k\n",
    "optimal_k_idx = np.argmax(test_accuracies)\n",
    "optimal_k = k_values[optimal_k_idx]\n",
    "best_test_acc = test_accuracies[optimal_k_idx]\n",
    "\n",
    "print(f\"\\nOptimal k: {optimal_k}\")\n",
    "print(f\"Best test accuracy: {best_test_acc:.4f}\")\n",
    "\n",
    "# Plot k vs accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_values, train_accuracies, 'o-', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(k_values, test_accuracies, 'o-', label='Test Accuracy', linewidth=2)\n",
    "plt.axvline(optimal_k, color='red', linestyle='--', alpha=0.7, label=f'Optimal k = {optimal_k}')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Performance vs K Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Analyze overfitting vs underfitting\n",
    "print(\"\\nBias-Variance Analysis:\")\n",
    "print(f\"k=1:  Train={train_accuracies[0]:.4f}, Test={test_accuracies[0]:.4f} (High Variance/Overfitting)\")\n",
    "print(f\"k={optimal_k}:  Train={train_accuracies[optimal_k_idx]:.4f}, Test={test_accuracies[optimal_k_idx]:.4f} (Optimal)\")\n",
    "print(f\"k={k_values[-1]}: Train={train_accuracies[-1]:.4f}, Test={test_accuracies[-1]:.4f} (High Bias/Underfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Cross-Validation for Robust K Selection\n",
    "**Task**: Use cross-validation to select k more robustly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use cross-validation for k selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_k_values = [1, 3, 5, 7, 9, 11, 15, 19, 25]\n",
    "cv_scores_mean = []\n",
    "cv_scores_std = []\n",
    "\n",
    "print(\"CROSS-VALIDATION K SELECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use stratified k-fold to maintain class balance\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for k in cv_k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # YOUR CODE HERE: Perform cross-validation\n",
    "    cv_scores = cross_val_score(knn, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    mean_score = cv_scores.mean()\n",
    "    std_score = cv_scores.std()\n",
    "    \n",
    "    cv_scores_mean.append(mean_score)\n",
    "    cv_scores_std.append(std_score)\n",
    "    \n",
    "    print(f\"k={k:2d}: {mean_score:.4f} (+/- {std_score:.4f})\")\n",
    "\n",
    "# Find best k from cross-validation\n",
    "best_cv_k_idx = np.argmax(cv_scores_mean)\n",
    "best_cv_k = cv_k_values[best_cv_k_idx]\n",
    "best_cv_score = cv_scores_mean[best_cv_k_idx]\n",
    "\n",
    "print(f\"\\nBest k from cross-validation: {best_cv_k}\")\n",
    "print(f\"Cross-validation accuracy: {best_cv_score:.4f}\")\n",
    "\n",
    "# Plot cross-validation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(cv_k_values, cv_scores_mean, yerr=cv_scores_std, \n",
    "             marker='o', linewidth=2, capsize=5)\n",
    "plt.axvline(best_cv_k, color='red', linestyle='--', alpha=0.7, \n",
    "           label=f'Best k = {best_cv_k}')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Cross-Validation K Selection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Training and Evaluation\n",
    "\n",
    "Now let's train our final KNN model with optimal parameters.\n",
    "\n",
    "### Exercise 6.1: Train Final KNN Model\n",
    "**Task**: Train KNN with optimal k and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train final KNN model with optimal parameters\n",
    "final_knn = KNeighborsClassifier(\n",
    "    n_neighbors=best_cv_k,\n",
    "    metric='euclidean',\n",
    "    weights='uniform'  # Can also try 'distance' for distance-weighted voting\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE: Fit the model\n",
    "______\n",
    "\n",
    "# Make predictions\n",
    "y_pred = final_knn.predict(X_test_scaled)\n",
    "y_pred_proba = final_knn.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(\"FINAL KNN MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  k = {best_cv_k}\")\n",
    "print(f\"  Distance metric: Euclidean\")\n",
    "print(f\"  Feature scaling: StandardScaler\")\n",
    "print(f\"  Features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Per-class performance\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=sorted(df['customer_segment'].unique())))\n",
    "\n",
    "# TODO: Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=sorted(df['customer_segment'].unique()),\n",
    "            yticklabels=sorted(df['customer_segment'].unique()))\n",
    "plt.title('Customer Segment Classification - Confusion Matrix')\n",
    "plt.xlabel('Predicted Segment')\n",
    "plt.ylabel('Actual Segment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-segment accuracy\n",
    "print(\"\\nPer-Segment Accuracy:\")\n",
    "for i, segment in enumerate(sorted(df['customer_segment'].unique())):\n",
    "    segment_accuracy = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"  {segment:20s}: {segment_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Compare with Different Configurations\n",
    "**Task**: Compare uniform vs distance-weighted voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare different KNN configurations\n",
    "configurations = {\n",
    "    'Uniform Weights': {'weights': 'uniform'},\n",
    "    'Distance Weights': {'weights': 'distance'},\n",
    "    'Manhattan Distance': {'weights': 'uniform', 'metric': 'manhattan'},\n",
    "    'Large K (k=15)': {'weights': 'uniform', 'n_neighbors': 15}\n",
    "}\n",
    "\n",
    "print(\"KNN CONFIGURATION COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config_name, params in configurations.items():\n",
    "    # Set default parameters\n",
    "    knn_params = {\n",
    "        'n_neighbors': best_cv_k,\n",
    "        'metric': 'euclidean',\n",
    "        'weights': 'uniform'\n",
    "    }\n",
    "    # Update with specific configuration\n",
    "    knn_params.update(params)\n",
    "    \n",
    "    # Train model\n",
    "    knn_config = KNeighborsClassifier(**knn_params)\n",
    "    knn_config.fit(______) # YOUR CODE HERE: Fit on scaled training data\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = knn_config.score(X_train_scaled, y_train)\n",
    "    test_acc = ______  # YOUR CODE HERE: Calculate test accuracy\n",
    "    \n",
    "    results[config_name] = {'train': train_acc, 'test': test_acc}\n",
    "    print(f\"{config_name:20s}: Train={train_acc:.4f}, Test={test_acc:.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "config_names = list(results.keys())\n",
    "train_scores = [results[name]['train'] for name in config_names]\n",
    "test_scores = [results[name]['test'] for name in config_names]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(config_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], train_scores, width, label='Training', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in x], test_scores, width, label='Test', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Configuration Comparison')\n",
    "plt.xticks(x, config_names, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Interpretation and Business Insights\n",
    "\n",
    "### Exercise 7.1: Analyze Misclassified Customers\n",
    "**Task**: Understand which customers are being misclassified and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze misclassified samples\n",
    "# Get test set with predictions\n",
    "test_results = X_test.copy()\n",
    "test_results['actual_segment'] = y_test\n",
    "test_results['predicted_segment'] = y_pred\n",
    "test_results['correct'] = (y_test == y_pred)\n",
    "\n",
    "# Find misclassified customers\n",
    "misclassified = test_results[~test_results['correct']]\n",
    "print(f\"MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total test samples: {len(test_results)}\")\n",
    "print(f\"Misclassified: {len(misclassified)} ({len(misclassified)/len(test_results):.1%})\")\n",
    "\n",
    "# Analyze misclassification patterns\n",
    "print(f\"\\nMisclassification Patterns:\")\n",
    "misclass_patterns = misclassified.groupby(['actual_segment', 'predicted_segment']).size()\n",
    "print(misclass_patterns)\n",
    "\n",
    "# TODO: Find borderline cases\n",
    "print(f\"\\nBorderline Cases Analysis:\")\n",
    "# Get prediction probabilities for misclassified samples\n",
    "misclassified_indices = test_results[~test_results['correct']].index\n",
    "test_indices = test_results.index\n",
    "misclass_prob_indices = [list(test_indices).index(idx) for idx in misclassified_indices]\n",
    "\n",
    "# Show examples of borderline classifications\n",
    "print(f\"Examples of uncertain predictions (low confidence):\")\n",
    "for i in range(min(5, len(misclass_prob_indices))):\n",
    "    prob_idx = misclass_prob_indices[i]\n",
    "    actual = y_test.iloc[prob_idx]\n",
    "    predicted = y_pred[prob_idx]\n",
    "    max_prob = np.max(y_pred_proba[prob_idx])\n",
    "    \n",
    "    print(f\"  Customer {i+1}: {actual} → {predicted} (confidence: {max_prob:.3f})\")\n",
    "\n",
    "# Visualize prediction confidence\n",
    "confidence_scores = np.max(y_pred_proba, axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(confidence_scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(confidence_scores.mean(), color='red', linestyle='--', \n",
    "           label=f'Mean confidence: {confidence_scores.mean():.3f}')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Distribution of Prediction Confidence Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"  Mean confidence: {confidence_scores.mean():.3f}\")\n",
    "print(f\"  Min confidence: {confidence_scores.min():.3f}\")\n",
    "print(f\"  Customers with confidence < 0.5: {(confidence_scores < 0.5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Feature Importance Analysis\n",
    "**Task**: Understand which features are most important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze feature importance by examining feature differences between segments\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate feature statistics by segment\n",
    "feature_analysis = []\n",
    "segments = sorted(df['customer_segment'].unique())\n",
    "\n",
    "for feature in numeric_features:\n",
    "    # Calculate variance between segments (F-statistic like measure)\n",
    "    segment_means = []\n",
    "    for segment in segments:\n",
    "        segment_data = df[df['customer_segment'] == segment][feature]\n",
    "        segment_means.append(segment_data.mean())\n",
    "    \n",
    "    # Calculate between-group variance\n",
    "    overall_mean = df[feature].mean()\n",
    "    between_var = np.var(segment_means)\n",
    "    within_var = np.mean([df[df['customer_segment'] == seg][feature].var() for seg in segments])\n",
    "    \n",
    "    # F-ratio as importance measure\n",
    "    f_ratio = between_var / within_var if within_var > 0 else 0\n",
    "    \n",
    "    feature_analysis.append({\n",
    "        'feature': feature,\n",
    "        'f_ratio': f_ratio,\n",
    "        'between_var': between_var,\n",
    "        'within_var': within_var\n",
    "    })\n",
    "\n",
    "# Sort by importance\n",
    "feature_df = pd.DataFrame(feature_analysis).sort_values('f_ratio', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (F-ratio - higher is more discriminative):\")\n",
    "print(\"-\" * 60)\n",
    "for idx, row in feature_df.iterrows():\n",
    "    print(f\"{row['feature']:20s}: {row['f_ratio']:8.2f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_df)), feature_df['f_ratio'], alpha=0.7)\n",
    "plt.yticks(range(len(feature_df)), feature_df['feature'])\n",
    "plt.xlabel('F-ratio (Discriminative Power)')\n",
    "plt.title('Feature Importance for Customer Segmentation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Show segment characteristics for top features\n",
    "print(f\"\\nTop 3 Most Discriminative Features:\")\n",
    "top_features = feature_df.head(3)['feature'].tolist()\n",
    "segment_profiles = df.groupby('customer_segment')[top_features].mean().round(1)\n",
    "print(segment_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Business Applications and Recommendations\n",
    "\n",
    "### Exercise 8.1: Customer Segment Profiling\n",
    "**Task**: Create detailed profiles for each customer segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comprehensive segment profiles\n",
    "print(\"CUSTOMER SEGMENT PROFILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate comprehensive statistics by segment\n",
    "segment_profiles = {}\n",
    "\n",
    "for segment in segments:\n",
    "    segment_data = df[df['customer_segment'] == segment]\n",
    "    \n",
    "    profile = {\n",
    "        'size': len(segment_data),\n",
    "        'percentage': len(segment_data) / len(df) * 100,\n",
    "        'avg_age': segment_data['age'].mean(),\n",
    "        'avg_income': segment_data['annual_income'].mean(),\n",
    "        'avg_order_value': segment_data['avg_order_value'].mean(),\n",
    "        'purchase_frequency': segment_data['purchase_frequency'].mean(),\n",
    "        'browsing_time': segment_data['browsing_time'].mean(),\n",
    "        'price_sensitivity': segment_data['price_sensitivity'].mean(),\n",
    "        'brand_loyalty': segment_data['brand_loyalty'].mean(),\n",
    "        'email_engagement': segment_data['email_engagement'].mean(),\n",
    "        'top_category': segment_data['preferred_category'].mode().iloc[0],\n",
    "        'top_location': segment_data['location_type'].mode().iloc[0]\n",
    "    }\n",
    "    \n",
    "    segment_profiles[segment] = profile\n",
    "\n",
    "# Display detailed profiles\n",
    "for segment, profile in segment_profiles.items():\n",
    "    print(f\"\\n{segment.upper().replace('_', ' ')}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Size: {profile['size']:,} customers ({profile['percentage']:.1f}%)\")\n",
    "    print(f\"Demographics: {profile['avg_age']:.0f} years old, ${profile['avg_income']:,.0f} income\")\n",
    "    print(f\"Shopping: ${profile['avg_order_value']:.0f} avg order, {profile['purchase_frequency']:.1f} purchases/month\")\n",
    "    print(f\"Behavior: {profile['browsing_time']:.0f}min browsing, prefers {profile['top_category']}\")\n",
    "    print(f\"Engagement: {profile['email_engagement']:.1f} email opens/month\")\n",
    "    print(f\"Preferences: Price sensitivity {profile['price_sensitivity']:.1f}/10, Brand loyalty {profile['brand_loyalty']:.1f}/10\")\n",
    "    print(f\"Location: Primarily {profile['top_location']}\")\n",
    "\n",
    "# TODO: Calculate business value by segment\n",
    "print(f\"\\n\\nBUSINESS VALUE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for segment, profile in segment_profiles.items():\n",
    "    # Calculate annual value per customer\n",
    "    monthly_value = profile['avg_order_value'] * profile['purchase_frequency']\n",
    "    annual_value = monthly_value * 12\n",
    "    total_segment_value = annual_value * profile['size']\n",
    "    \n",
    "    print(f\"\\n{segment.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Annual value per customer: ${annual_value:,.0f}\")\n",
    "    print(f\"  Total segment value: ${total_segment_value:,.0f}\")\n",
    "    print(f\"  Marketing receptivity: {profile['email_engagement']/20*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2: Marketing Strategy Recommendations\n",
    "**Task**: Generate actionable marketing recommendations based on segment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive business recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRENDMART CUSTOMER SEGMENTATION - MARKETING STRATEGY RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"• Classification Accuracy: {accuracy:.1%}\")\n",
    "print(f\"• Model Type: K-Nearest Neighbors (k={best_cv_k})\")\n",
    "print(f\"• Features Used: {X_train.shape[1]} (scaled and encoded)\")\n",
    "print(f\"• Ready for real-time customer classification\")\n",
    "\n",
    "print(f\"\\n🎯 SEGMENT-SPECIFIC MARKETING STRATEGIES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Strategy recommendations by segment\n",
    "strategies = {\n",
    "    'Budget_Shoppers': {\n",
    "        'focus': 'Price-driven campaigns',\n",
    "        'tactics': ['Discount promotions', 'Free shipping offers', 'Bundle deals', 'Clearance alerts'],\n",
    "        'channels': ['Email newsletters', 'SMS promotions'],\n",
    "        'kpis': ['Conversion rate', 'Cart abandonment reduction']\n",
    "    },\n",
    "    'Premium_Buyers': {\n",
    "        'focus': 'Quality and exclusivity',\n",
    "        'tactics': ['Premium product showcases', 'Early access to new arrivals', 'Luxury brand partnerships', 'VIP customer service'],\n",
    "        'channels': ['Personalized emails', 'Social media ads', 'Influencer partnerships'],\n",
    "        'kpis': ['Average order value', 'Customer lifetime value']\n",
    "    },\n",
    "    'Frequent_Shoppers': {\n",
    "        'focus': 'Engagement and loyalty',\n",
    "        'tactics': ['Loyalty point programs', 'Personalized recommendations', 'Regular engagement campaigns', 'Cross-selling'],\n",
    "        'channels': ['App notifications', 'Personalized emails', 'Social media'],\n",
    "        'kpis': ['Purchase frequency', 'Customer retention rate']\n",
    "    },\n",
    "    'Occasional_Buyers': {\n",
    "        'focus': 'Targeted activation',\n",
    "        'tactics': ['Need-based targeting', 'Seasonal campaigns', 'Wishlist reminders', 'Time-limited offers'],\n",
    "        'channels': ['Retargeting ads', 'Quarterly email campaigns'],\n",
    "        'kpis': ['Purchase activation rate', 'Time between purchases']\n",
    "    }\n",
    "}\n",
    "\n",
    "for segment, strategy in strategies.items():\n",
    "    segment_name = segment.replace('_', ' ').title()\n",
    "    profile = segment_profiles[segment]\n",
    "    \n",
    "    print(f\"\\n🔹 {segment_name}\")\n",
    "    print(f\"   Size: {profile['size']:,} customers ({profile['percentage']:.1f}% of base)\")\n",
    "    print(f\"   Focus: {strategy['focus']}\")\n",
    "    print(f\"   Key Tactics: {', '.join(strategy['tactics'][:3])}\")\n",
    "    print(f\"   Channels: {', '.join(strategy['channels'])}\")\n",
    "    print(f\"   Success Metrics: {', '.join(strategy['kpis'])}\")\n",
    "\n",
    "print(f\"\\n💡 IMPLEMENTATION ROADMAP\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"1. PHASE 1 - MODEL DEPLOYMENT (Week 1-2)\")\n",
    "print(f\"   • Deploy KNN model to production environment\")\n",
    "print(f\"   • Set up real-time customer classification pipeline\")\n",
    "print(f\"   • Create segment dashboards for marketing team\")\n",
    "print(f\"\")\n",
    "print(f\"2. PHASE 2 - CAMPAIGN SETUP (Week 3-4)\")\n",
    "print(f\"   • Design segment-specific email templates\")\n",
    "print(f\"   • Configure automated campaign triggers\")\n",
    "print(f\"   • Set up A/B testing framework\")\n",
    "print(f\"\")\n",
    "print(f\"3. PHASE 3 - LAUNCH & OPTIMIZE (Week 5+)\")\n",
    "print(f\"   • Launch targeted campaigns for each segment\")\n",
    "print(f\"   • Monitor performance and adjust tactics\")\n",
    "print(f\"   • Continuously retrain model with new customer data\")\n",
    "\n",
    "print(f\"\\n📈 EXPECTED BUSINESS IMPACT\")\n",
    "print(\"-\" * 50)\n",
    "total_customers = sum(profile['size'] for profile in segment_profiles.values())\n",
    "total_annual_value = sum(\n",
    "    profile['size'] * profile['avg_order_value'] * profile['purchase_frequency'] * 12\n",
    "    for profile in segment_profiles.values()\n",
    ")\n",
    "\n",
    "print(f\"• Customer Base: {total_customers:,} customers\")\n",
    "print(f\"• Total Annual GMV: ${total_annual_value:,.0f}\")\n",
    "print(f\"• Expected Improvements with Targeted Marketing:\")\n",
    "print(f\"  - Email engagement: +25-40% through personalization\")\n",
    "print(f\"  - Conversion rates: +15-30% through segment-specific offers\")\n",
    "print(f\"  - Customer retention: +10-20% through loyalty programs\")\n",
    "print(f\"  - Marketing ROI: +50-100% through better targeting\")\n",
    "\n",
    "print(f\"\\n⚠️  MONITORING & MAINTENANCE\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"• Model Retraining: Monthly with new customer data\")\n",
    "print(f\"• Performance Monitoring: Track classification accuracy weekly\")\n",
    "print(f\"• Segment Drift Detection: Monitor changing customer behaviors\")\n",
    "print(f\"• Business Metrics: Weekly campaign performance reviews\")\n",
    "print(f\"• Feature Updates: Quarterly addition of new behavioral data\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Model Deployment Considerations\n",
    "\n",
    "### Exercise 9.1: Create Production Pipeline\n",
    "**Task**: Design a production-ready pipeline for real-time customer classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a production pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['age', 'annual_income', 'avg_order_value', 'purchase_frequency',\n",
    "                   'browsing_time', 'price_sensitivity', 'brand_loyalty', \n",
    "                   'email_engagement', 'social_activity']\n",
    "\n",
    "categorical_features = ['preferred_category', 'location_type']\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', 'passthrough', categorical_features)  # Will be handled separately\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create full pipeline\n",
    "production_pipeline = Pipeline([\n",
    "    ('preprocessor', StandardScaler()),  # Simplified for demo\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=best_cv_k, metric='euclidean'))\n",
    "])\n",
    "\n",
    "# Train on the processed data (already encoded)\n",
    "production_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Test pipeline performance\n",
    "pipeline_score = production_pipeline.score(X_test, y_test)\n",
    "print(f\"PRODUCTION PIPELINE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Pipeline Accuracy: {pipeline_score:.4f}\")\n",
    "print(f\"Ready for deployment: {'✅' if pipeline_score > 0.8 else '⚠️'}\")\n",
    "\n",
    "# Demonstrate real-time prediction\n",
    "print(f\"\\nReal-time Prediction Example:\")\n",
    "sample_customer = X_test.iloc[0:1]  # Take first test customer\n",
    "predicted_segment = production_pipeline.predict(sample_customer)[0]\n",
    "prediction_proba = production_pipeline.predict_proba(sample_customer)[0]\n",
    "confidence = max(prediction_proba)\n",
    "\n",
    "print(f\"Customer Features: {dict(sample_customer.iloc[0])[:3]}...\")  # Show first 3 features\n",
    "print(f\"Predicted Segment: {predicted_segment}\")\n",
    "print(f\"Confidence: {confidence:.3f}\")\n",
    "print(f\"Actual Segment: {y_test.iloc[0]}\")\n",
    "print(f\"Prediction: {'✅ Correct' if predicted_segment == y_test.iloc[0] else '❌ Incorrect'}\")\n",
    "\n",
    "# TODO: Show deployment architecture\n",
    "print(f\"\\nPRODUCTION ARCHITECTURE RECOMMENDATIONS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"1. Model Serving:\")\n",
    "print(f\"   • REST API with Flask/FastAPI\")\n",
    "print(f\"   • Response time: <100ms for real-time classification\")\n",
    "print(f\"   • Input: Customer features (JSON)\")\n",
    "print(f\"   • Output: Segment + confidence score\")\n",
    "print(f\"\")\n",
    "print(f\"2. Data Pipeline:\")\n",
    "print(f\"   • Feature engineering from raw customer data\")\n",
    "print(f\"   • Real-time feature computation\")\n",
    "print(f\"   • Feature validation and error handling\")\n",
    "print(f\"\")\n",
    "print(f\"3. Monitoring:\")\n",
    "print(f\"   • Model performance metrics\")\n",
    "print(f\"   • Data drift detection\")\n",
    "print(f\"   • Prediction confidence monitoring\")\n",
    "print(f\"   • Business impact tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully completed a comprehensive K-Nearest Neighbors analysis for customer segmentation.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **KNN Fundamentals**: Learned how KNN classifies based on nearest neighbor voting\n",
    "2. **Feature Scaling Critical**: Demonstrated why scaling is essential for distance-based algorithms\n",
    "3. **Distance Metrics**: Compared Euclidean, Manhattan, and Minkowski distances\n",
    "4. **K Optimization**: Used validation curves and cross-validation for optimal k selection\n",
    "5. **Multi-class Classification**: Successfully classified customers into 4 distinct segments\n",
    "6. **Business Application**: Translated technical results into actionable marketing strategies\n",
    "7. **Production Considerations**: Designed deployment architecture for real-time classification\n",
    "\n",
    "### Skills Practiced:\n",
    "- K-Nearest Neighbors algorithm implementation and optimization\n",
    "- Feature scaling and preprocessing for distance-based methods\n",
    "- Distance metric comparison and selection\n",
    "- Hyperparameter tuning with cross-validation\n",
    "- Multi-class classification evaluation\n",
    "- Business insight generation from model results\n",
    "- Production pipeline design for real-time deployment\n",
    "\n",
    "### Business Impact:\n",
    "The KNN customer segmentation model enables TrendMart to:\n",
    "- **Personalize Marketing**: Target campaigns to specific customer types\n",
    "- **Optimize Spend**: Focus marketing budget on high-value segments\n",
    "- **Improve Retention**: Identify at-risk customers and intervene appropriately\n",
    "- **Scale Operations**: Automatically classify new customers in real-time\n",
    "\n",
    "### Next Steps:\n",
    "In the next lab, we'll explore Decision Trees for product recommendation systems, learning about interpretable models that can provide clear business rules and handle mixed data types naturally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}